
#####################################################################################

# experiment 1

# folder xx4
# at exactly 100 epochs!!!

$ python eval.py --model_dir saved_models/xx4 --model checkpoint_epoch_100.pt
Loading model from saved_models/xx4/checkpoint_epoch_100.pt
Number of heads:  3
d_v and d_k:  120.0
Fine-tune all embeddings.
Using weights [1.0, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]
Vocab size 55950 loaded from file
Loading data from dataset/tacred/test.json with batch size 50...
100%|██████████| 15509/15509 [00:01<00:00, 12406.89it/s]
311 batches created for dataset/tacred/test.json

Running with the following configs:
        data_dir : dataset/tacred
        vocab_dir : dataset/vocab
        emb_dim : 300
        ner_dim : 30
        pos_dim : 30
        hidden_dim : 360
        hidden_self : 130
        query_size_attn : 360
        num_layers : 2
        num_layers_encoder : 1
        dropout : 0.4
        scaled_dropout : 0.1
        temper_value : 0.5
        word_dropout : 0.06
        lstm_dropout : 0.5
        topn : 10000000000.0
        lower : False
        weight_no_rel : 1.0
        weight_rest : 0.85
        self_att : True
        self_att_and_rnn : False
        use_lemmas : False
        preload_lemmas : False
        obj_sub_pos : True
        use_batch_norm : True
        diagonal_positional_attention : True
        relative_positions : True
        new_residual : True
        n_head : 3
        attn : True
        attn_dim : 200
        pe_dim : 30
        lr : 0.1
        lr_decay : 0.9
        decay_epoch : 15
        optim : sgd
        num_epoch : 100
        batch_size : 50
        max_grad_norm : 5.0
        log_step : 400
        log : logs.txt
        save_epoch : 1
        save_dir : ./saved_models
        id : xx4
        info :
        seed : 1234
        cuda : True
        cpu : False
        num_class : 42
        vocab_size : 55950
        model_save_dir : ./saved_models/xx4


Per-relation statistics:
org:alternate_names                  P:  75.45%  R:  79.34%  F1:  77.35%  #: 213
org:city_of_headquarters             P:  68.89%  R:  75.61%  F1:  72.09%  #: 82
org:country_of_headquarters          P:  61.04%  R:  43.52%  F1:  50.81%  #: 108
org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2
org:founded                          P:  80.00%  R:  86.49%  F1:  83.12%  #: 37
org:founded_by                       P:  73.53%  R:  36.76%  F1:  49.02%  #: 68
org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18
org:members                          P:  40.00%  R:   6.45%  F1:  11.11%  #: 31
org:number_of_employees/members      P:  66.67%  R:  73.68%  F1:  70.00%  #: 19
org:parents                          P:  34.78%  R:  12.90%  F1:  18.82%  #: 62
org:political/religious_affiliation  P:  21.88%  R:  70.00%  F1:  33.33%  #: 10
org:shareholders                     P: 100.00%  R:  30.77%  F1:  47.06%  #: 13
org:stateorprovince_of_headquarters  P:  70.18%  R:  78.43%  F1:  74.07%  #: 51
org:subsidiaries                     P:  42.86%  R:  27.27%  F1:  33.33%  #: 44
org:top_members/employees            P:  67.29%  R:  83.82%  F1:  74.65%  #: 346
org:website                          P:  55.56%  R:  96.15%  F1:  70.42%  #: 26
per:age                              P:  81.12%  R:  94.50%  F1:  87.30%  #: 200
per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11
per:cause_of_death                   P:  67.44%  R:  55.77%  F1:  61.05%  #: 52
per:charges                          P:  62.59%  R:  89.32%  F1:  73.60%  #: 103
per:children                         P:  41.03%  R:  43.24%  F1:  42.11%  #: 37
per:cities_of_residence              P:  52.97%  R:  61.38%  F1:  56.86%  #: 189
per:city_of_birth                    P:  50.00%  R:  20.00%  F1:  28.57%  #: 5
per:city_of_death                    P: 100.00%  R:  28.57%  F1:  44.44%  #: 28
per:countries_of_residence           P:  49.62%  R:  43.92%  F1:  46.59%  #: 148
per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 5
per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9
per:date_of_birth                    P:  70.00%  R:  77.78%  F1:  73.68%  #: 9
per:date_of_death                    P:  62.16%  R:  42.59%  F1:  50.55%  #: 54
per:employee_of                      P:  63.12%  R:  67.42%  F1:  65.20%  #: 264
per:origin                           P:  62.22%  R:  63.64%  F1:  62.92%  #: 132
per:other_family                     P:  65.22%  R:  50.00%  F1:  56.60%  #: 60
per:parents                          P:  65.48%  R:  62.50%  F1:  63.95%  #: 88
per:religion                         P:  43.94%  R:  61.70%  F1:  51.33%  #: 47
per:schools_attended                 P:  70.83%  R:  56.67%  F1:  62.96%  #: 30
per:siblings                         P:  65.52%  R:  69.09%  F1:  67.26%  #: 55
per:spouse                           P:  55.84%  R:  65.15%  F1:  60.14%  #: 66
per:stateorprovince_of_birth         P:  50.00%  R:  75.00%  F1:  60.00%  #: 8
per:stateorprovince_of_death         P:  55.56%  R:  35.71%  F1:  43.48%  #: 14
per:stateorprovinces_of_residence    P:  70.59%  R:  59.26%  F1:  64.43%  #: 81
per:title                            P:  73.94%  R:  87.40%  F1:  80.11%  #: 500

Final Score:
Calculating Precision: correct_by_relation= 2253.0 / guessed_by_relation= 3437.0
Calculating Recall: correct_by_relation= 2253.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 65.551%
   Recall (micro): 67.759%
       F1 (micro): 66.637%
Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.
(cuda2)


########################################################################################################

# experiment 2
# folder: xx5
# epoch 100


$ python eval.py --model_dir saved_models/xx5 --model checkpoint_epoch_100.pt
Loading model from saved_models/xx5/checkpoint_epoch_100.pt
Number of heads:  3
d_v and d_k:  120.0
Fine-tune all embeddings.
Using weights [1.0, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]
Vocab size 55950 loaded from file
Loading data from dataset/tacred/test.json with batch size 50...
100%|██████████| 15509/15509 [00:01<00:00, 12456.25it/s]
311 batches created for dataset/tacred/test.json

Running with the following configs:
        data_dir : dataset/tacred
        vocab_dir : dataset/vocab
        emb_dim : 300
        ner_dim : 30
        pos_dim : 30
        hidden_dim : 360
        hidden_self : 130
        query_size_attn : 360
        num_layers : 2
        num_layers_encoder : 1
        dropout : 0.4
        scaled_dropout : 0.1
        temper_value : 0.5
        word_dropout : 0.06
        lstm_dropout : 0.5
        topn : 10000000000.0
        lower : False
        weight_no_rel : 1.0
        weight_rest : 0.85
        self_att : True
        self_att_and_rnn : False
        use_lemmas : False
        preload_lemmas : False
        obj_sub_pos : True
        use_batch_norm : True
        diagonal_positional_attention : True
        relative_positions : True
        new_residual : True
        n_head : 3
        attn : True
        attn_dim : 200
        pe_dim : 30
        lr : 0.1
        lr_decay : 0.9
        decay_epoch : 15
        optim : sgd
        num_epoch : 100
        batch_size : 50
        max_grad_norm : 5.0
        log_step : 400
        log : logs.txt
        save_epoch : 10
        save_dir : ./saved_models
        id : xx5
        info :
        seed : 1234
        cuda : True
        cpu : False
        num_class : 42
        vocab_size : 55950
        model_save_dir : ./saved_models/xx5


Per-relation statistics:
org:alternate_names                  P:  75.23%  R:  78.40%  F1:  76.78%  #: 213
org:city_of_headquarters             P:  70.59%  R:  73.17%  F1:  71.86%  #: 82
org:country_of_headquarters          P:  59.74%  R:  42.59%  F1:  49.73%  #: 108
org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2
org:founded                          P:  84.21%  R:  86.49%  F1:  85.33%  #: 37
org:founded_by                       P:  72.73%  R:  35.29%  F1:  47.52%  #: 68
org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18
org:members                          P:  25.00%  R:   3.23%  F1:   5.71%  #: 31
org:number_of_employees/members      P:  60.87%  R:  73.68%  F1:  66.67%  #: 19
org:parents                          P:  39.13%  R:  14.52%  F1:  21.18%  #: 62
org:political/religious_affiliation  P:  21.88%  R:  70.00%  F1:  33.33%  #: 10
org:shareholders                     P:  80.00%  R:  30.77%  F1:  44.44%  #: 13
org:stateorprovince_of_headquarters  P:  70.91%  R:  76.47%  F1:  73.58%  #: 51
org:subsidiaries                     P:  45.83%  R:  25.00%  F1:  32.35%  #: 44
org:top_members/employees            P:  67.93%  R:  82.66%  F1:  74.58%  #: 346
org:website                          P:  55.81%  R:  92.31%  F1:  69.57%  #: 26
per:age                              P:  83.04%  R:  93.00%  F1:  87.74%  #: 200
per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11
per:cause_of_death                   P:  62.86%  R:  42.31%  F1:  50.57%  #: 52
per:charges                          P:  68.89%  R:  90.29%  F1:  78.15%  #: 103
per:children                         P:  40.00%  R:  37.84%  F1:  38.89%  #: 37
per:cities_of_residence              P:  55.71%  R:  61.90%  F1:  58.65%  #: 189
per:city_of_birth                    P:  50.00%  R:  20.00%  F1:  28.57%  #: 5
per:city_of_death                    P: 100.00%  R:  25.00%  F1:  40.00%  #: 28
per:countries_of_residence           P:  48.53%  R:  44.59%  F1:  46.48%  #: 148
per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 5
per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9
per:date_of_birth                    P:  70.00%  R:  77.78%  F1:  73.68%  #: 9
per:date_of_death                    P:  60.53%  R:  42.59%  F1:  50.00%  #: 54
per:employee_of                      P:  64.75%  R:  68.18%  F1:  66.42%  #: 264
per:origin                           P:  57.93%  R:  63.64%  F1:  60.65%  #: 132
per:other_family                     P:  65.22%  R:  50.00%  F1:  56.60%  #: 60
per:parents                          P:  60.67%  R:  61.36%  F1:  61.02%  #: 88
per:religion                         P:  43.84%  R:  68.09%  F1:  53.33%  #: 47
per:schools_attended                 P:  62.07%  R:  60.00%  F1:  61.02%  #: 30
per:siblings                         P:  65.52%  R:  69.09%  F1:  67.26%  #: 55
per:spouse                           P:  52.38%  R:  66.67%  F1:  58.67%  #: 66
per:stateorprovince_of_birth         P:  62.50%  R:  62.50%  F1:  62.50%  #: 8
per:stateorprovince_of_death         P:  55.56%  R:  35.71%  F1:  43.48%  #: 14
per:stateorprovinces_of_residence    P:  66.67%  R:  61.73%  F1:  64.10%  #: 81
per:title                            P:  75.75%  R:  85.60%  F1:  80.38%  #: 500

Final Score:
Calculating Precision: correct_by_relation= 2228.0 / guessed_by_relation= 3380.0
Calculating Recall: correct_by_relation= 2228.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 65.917%
   Recall (micro): 67.008%
       F1 (micro): 66.458%

       # this is FINAL FINAL result with dpa


Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.

# xx5 folder
# experiment 3 was exactly the same


# same as above but with lemmas:

# no dpa no lemmas:

P 65.85%
R 65.77
F 65.814
#########################################################################################



# folder: xx6
# with dpa, lr 0.2, new data


# epoch 70

 python eval.py --model_dir saved_models/xx6 --model checkpoint_epoch_70.pt
Loading model from saved_models/xx6/checkpoint_epoch_70.pt
Number of heads:  3
d_v and d_k:  120.0
Fine-tune all embeddings.
Using weights [1.0, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]
Vocab size 55950 loaded from file
Loading data from dataset/tacred/test.json with batch size 50...
100%|██████████| 15509/15509 [00:01<00:00, 10882.12it/s]
311 batches created for dataset/tacred/test.json

Running with the following configs:
        data_dir : dataset/tacred
        vocab_dir : dataset/vocab
        emb_dim : 300
        ner_dim : 30
        pos_dim : 30
        hidden_dim : 360
        hidden_self : 130
        query_size_attn : 360
        num_layers : 2
        num_layers_encoder : 1
        dropout : 0.4
        scaled_dropout : 0.1
        temper_value : 0.5
        word_dropout : 0.06
        lstm_dropout : 0.5
        topn : 10000000000.0
        lower : False
        weight_no_rel : 1.0
        weight_rest : 0.85
        self_att : True
        self_att_and_rnn : False
        use_lemmas : False
        preload_lemmas : False
        obj_sub_pos : True
        use_batch_norm : True
        diagonal_positional_attention : True
        relative_positions : True
        new_residual : True
        n_head : 3
        attn : True
        attn_dim : 200
        pe_dim : 30
        lr : 0.2
        lr_decay : 0.9
        decay_epoch : 15
        optim : sgd
        num_epoch : 100
        batch_size : 50
        max_grad_norm : 5.0
        log_step : 400
        log : logs.txt
        save_epoch : 10
        save_dir : ./saved_models
        id : xx6
        info :
        seed : 1234
        cuda : True
        cpu : False
        num_class : 42
        vocab_size : 55950
        model_save_dir : ./saved_models/xx6


Per-relation statistics:
org:alternate_names                  P:  77.21%  R:  77.93%  F1:  77.57%  #: 213
org:city_of_headquarters             P:  69.41%  R:  71.95%  F1:  70.66%  #: 82
org:country_of_headquarters          P:  71.70%  R:  35.19%  F1:  47.20%  #: 108
org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2
org:founded                          P:  86.49%  R:  86.49%  F1:  86.49%  #: 37
org:founded_by                       P:  70.73%  R:  42.65%  F1:  53.21%  #: 68
org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18
org:members                          P:   0.00%  R:   0.00%  F1:   0.00%  #: 31
org:number_of_employees/members      P:  60.87%  R:  73.68%  F1:  66.67%  #: 19
org:parents                          P:  42.86%  R:  14.52%  F1:  21.69%  #: 62
org:political/religious_affiliation  P:  23.08%  R:  90.00%  F1:  36.73%  #: 10
org:shareholders                     P:  80.00%  R:  30.77%  F1:  44.44%  #: 13
org:stateorprovince_of_headquarters  P:  73.91%  R:  66.67%  F1:  70.10%  #: 51
org:subsidiaries                     P:  40.74%  R:  25.00%  F1:  30.99%  #: 44
org:top_members/employees            P:  69.77%  R:  78.03%  F1:  73.67%  #: 346
org:website                          P:  51.06%  R:  92.31%  F1:  65.75%  #: 26
per:age                              P:  81.98%  R:  91.00%  F1:  86.26%  #: 200
per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11
per:cause_of_death                   P:  68.29%  R:  53.85%  F1:  60.22%  #: 52
per:charges                          P:  67.91%  R:  88.35%  F1:  76.79%  #: 103
per:children                         P:  40.00%  R:  48.65%  F1:  43.90%  #: 37
per:cities_of_residence              P:  55.41%  R:  65.08%  F1:  59.85%  #: 189
per:city_of_birth                    P:  50.00%  R:  20.00%  F1:  28.57%  #: 5
per:city_of_death                    P: 100.00%  R:  39.29%  F1:  56.41%  #: 28
per:countries_of_residence           P:  48.08%  R:  50.68%  F1:  49.34%  #: 148
per:country_of_birth                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 5
per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9
per:date_of_birth                    P:  70.00%  R:  77.78%  F1:  73.68%  #: 9
per:date_of_death                    P:  74.19%  R:  42.59%  F1:  54.12%  #: 54
per:employee_of                      P:  64.96%  R:  67.42%  F1:  66.17%  #: 264
per:origin                           P:  61.11%  R:  58.33%  F1:  59.69%  #: 132
per:other_family                     P:  65.22%  R:  50.00%  F1:  56.60%  #: 60
per:parents                          P:  61.45%  R:  57.95%  F1:  59.65%  #: 88
per:religion                         P:  42.86%  R:  70.21%  F1:  53.23%  #: 47
per:schools_attended                 P:  62.07%  R:  60.00%  F1:  61.02%  #: 30
per:siblings                         P:  65.52%  R:  69.09%  F1:  67.26%  #: 55
per:spouse                           P:  64.38%  R:  71.21%  F1:  67.63%  #: 66
per:stateorprovince_of_birth         P:  57.14%  R:  50.00%  F1:  53.33%  #: 8
per:stateorprovince_of_death         P:  62.50%  R:  35.71%  F1:  45.45%  #: 14
per:stateorprovinces_of_residence    P:  68.12%  R:  58.02%  F1:  62.67%  #: 81
per:title                            P:  75.44%  R:  85.40%  F1:  80.11%  #: 500

Final Score:
Calculating Precision: correct_by_relation= 2213.0 / guessed_by_relation= 3318.0
Calculating Recall: correct_by_relation= 2213.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 66.697%
   Recall (micro): 66.556%
       F1 (micro): 66.627%
Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.


############################################################################################
xx6 epoch 150

$ python eval.py --model_dir saved_models/xx6 --model checkpoint_epoch_150.pt
Loading model from saved_models/xx6/checkpoint_epoch_150.pt
Number of heads:  3
d_v and d_k:  120.0
Fine-tune all embeddings.
Using weights [1.0, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85,                                   0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.                                  85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85                                  , 0.85, 0.85, 0.85, 0.85]
Vocab size 55950 loaded from file
Loading data from dataset/tacred/test.json with batch size 50...
100%|██████████| 15509/15509 [00:01<00:00, 12120.33it/s]
311 batches created for dataset/tacred/test.json

Running with the following configs:
        data_dir : dataset/tacred
        vocab_dir : dataset/vocab
        emb_dim : 300
        ner_dim : 30
        pos_dim : 30
        hidden_dim : 360
        hidden_self : 130
        query_size_attn : 360
        num_layers : 2
        num_layers_encoder : 1
        dropout : 0.4
        scaled_dropout : 0.1
        temper_value : 0.5
        word_dropout : 0.06
        lstm_dropout : 0.5
        topn : 10000000000.0
        lower : False
        weight_no_rel : 1.0
        weight_rest : 0.85
        self_att : True
        self_att_and_rnn : False
        use_lemmas : False
        preload_lemmas : False
        obj_sub_pos : True
        use_batch_norm : True
        diagonal_positional_attention : True
        relative_positions : True
        new_residual : True
        n_head : 3
        attn : True
        attn_dim : 200
        pe_dim : 30
        lr : 0.1
        lr_decay : 0.9
        decay_epoch : 15
        optim : sgd
        num_epoch : 500
        batch_size : 50
        max_grad_norm : 5.0
        log_step : 400
        log : logs.txt
        save_epoch : 10
        save_dir : ./saved_models
        id : xx6
        info :
        seed : 1234
        cuda : True
        cpu : False
        num_class : 42
        vocab_size : 55950
        model_save_dir : ./saved_models/xx6


Per-relation statistics:
org:alternate_names                  P:  75.45%  R:  79.34%  F1:  77.35%  #: 213
org:city_of_headquarters             P:  71.76%  R:  74.39%  F1:  73.05%  #: 82
org:country_of_headquarters          P:  62.86%  R:  40.74%  F1:  49.44%  #: 108
org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2
org:founded                          P:  84.21%  R:  86.49%  F1:  85.33%  #: 37
org:founded_by                       P:  70.59%  R:  35.29%  F1:  47.06%  #: 68
org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18
org:members                          P:  25.00%  R:   3.23%  F1:   5.71%  #: 31
org:number_of_employees/members      P:  60.87%  R:  73.68%  F1:  66.67%  #: 19
org:parents                          P:  39.13%  R:  14.52%  F1:  21.18%  #: 62
org:political/religious_affiliation  P:  22.58%  R:  70.00%  F1:  34.15%  #: 10
org:shareholders                     P:  80.00%  R:  30.77%  F1:  44.44%  #: 13
org:stateorprovince_of_headquarters  P:  71.43%  R:  78.43%  F1:  74.77%  #: 51
org:subsidiaries                     P:  50.00%  R:  27.27%  F1:  35.29%  #: 44
org:top_members/employees            P:  67.70%  R:  82.37%  F1:  74.32%  #: 346
org:website                          P:  55.81%  R:  92.31%  F1:  69.57%  #: 26
per:age                              P:  83.86%  R:  93.50%  F1:  88.42%  #: 200
per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11
per:cause_of_death                   P:  61.11%  R:  42.31%  F1:  50.00%  #: 52
per:charges                          P:  68.84%  R:  92.23%  F1:  78.84%  #: 103
per:children                         P:  35.90%  R:  37.84%  F1:  36.84%  #: 37
per:cities_of_residence              P:  53.64%  R:  62.43%  F1:  57.70%  #: 189
per:city_of_birth                    P:  50.00%  R:  20.00%  F1:  28.57%  #: 5
per:city_of_death                    P: 100.00%  R:  21.43%  F1:  35.29%  #: 28
per:countries_of_residence           P:  50.75%  R:  45.95%  F1:  48.23%  #: 148
per:country_of_birth                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 5
per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9
per:date_of_birth                    P:  63.64%  R:  77.78%  F1:  70.00%  #: 9
per:date_of_death                    P:  59.46%  R:  40.74%  F1:  48.35%  #: 54
per:employee_of                      P:  63.14%  R:  70.08%  F1:  66.43%  #: 264
per:origin                           P:  60.15%  R:  60.61%  F1:  60.38%  #: 132
per:other_family                     P:  66.67%  R:  50.00%  F1:  57.14%  #: 60
per:parents                          P:  58.70%  R:  61.36%  F1:  60.00%  #: 88
per:religion                         P:  43.48%  R:  63.83%  F1:  51.72%  #: 47
per:schools_attended                 P:  62.07%  R:  60.00%  F1:  61.02%  #: 30
per:siblings                         P:  63.33%  R:  69.09%  F1:  66.09%  #: 55
per:spouse                           P:  55.00%  R:  66.67%  F1:  60.27%  #: 66
per:stateorprovince_of_birth         P:  62.50%  R:  62.50%  F1:  62.50%  #: 8
per:stateorprovince_of_death         P:  55.56%  R:  35.71%  F1:  43.48%  #: 14
per:stateorprovinces_of_residence    P:  66.25%  R:  65.43%  F1:  65.84%  #: 81
per:title                            P:  75.80%  R:  85.80%  F1:  80.49%  #: 500

Final Score:
Calculating Precision: correct_by_relation= 2237.0 / guessed_by_relation= 3393.0
Calculating Recall: correct_by_relation= 2237.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 65.930%
   Recall (micro): 67.278%
       F1 (micro): 66.597%
Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.

############################################################################################




# with weight_rest 1.0


$ python eval.py --model_dir saved_models/x6 --model checkpoint_epoch_70.pt
Loading model from saved_models/x6/checkpoint_epoch_70.pt
Number of heads:  3
d_v and d_k:  120.0
Fine-tune all embeddings.
Using weights [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
Vocab size 55950 loaded from file
Loading data from dataset/tacred/test.json with batch size 50...
100%|██████████| 15509/15509 [00:01<00:00, 12817.34it/s]
311 batches created for dataset/tacred/test.json

Running with the following configs:
        data_dir : dataset/tacred
        vocab_dir : dataset/vocab
        emb_dim : 300
        ner_dim : 30
        pos_dim : 30
        hidden_dim : 360
        hidden_self : 130
        query_size_attn : 360
        num_layers : 2
        num_layers_encoder : 1
        dropout : 0.4
        scaled_dropout : 0.1
        temper_value : 0.5
        word_dropout : 0.06
        lstm_dropout : 0.5
        topn : 10000000000.0
        lower : False
        weight_no_rel : 1.0
        weight_rest : 1.0
        self_att : True
        self_att_and_rnn : False
        use_lemmas : False
        preload_lemmas : False
        obj_sub_pos : True
        use_batch_norm : True
        diagonal_positional_attention : True
        relative_positions : True
        new_residual : True
        n_head : 3
        attn : True
        attn_dim : 200
        pe_dim : 30
        lr : 0.1
        lr_decay : 0.9
        decay_epoch : 15
        optim : sgd
        num_epoch : 100
        batch_size : 50
        max_grad_norm : 5.0
        log_step : 400
        log : logs.txt
        save_epoch : 10
        save_dir : ./saved_models
        id : x6
        info :
        seed : 1234
        cuda : True
        cpu : False
        num_class : 42
        vocab_size : 55950
        model_save_dir : ./saved_models/x6


Per-relation statistics:
org:alternate_names                  P:  75.91%  R:  78.40%  F1:  77.14%  #: 213
org:city_of_headquarters             P:  69.15%  R:  79.27%  F1:  73.86%  #: 82
org:country_of_headquarters          P:  61.84%  R:  43.52%  F1:  51.09%  #: 108
org:dissolved                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 2
org:founded                          P:  82.05%  R:  86.49%  F1:  84.21%  #: 37
org:founded_by                       P:  68.57%  R:  35.29%  F1:  46.60%  #: 68
org:member_of                        P: 100.00%  R:   0.00%  F1:   0.00%  #: 18
org:members                          P:  40.00%  R:   6.45%  F1:  11.11%  #: 31
org:number_of_employees/members      P:  63.64%  R:  73.68%  F1:  68.29%  #: 19
org:parents                          P:  37.50%  R:  14.52%  F1:  20.93%  #: 62
org:political/religious_affiliation  P:  24.24%  R:  80.00%  F1:  37.21%  #: 10
org:shareholders                     P: 100.00%  R:  30.77%  F1:  47.06%  #: 13
org:stateorprovince_of_headquarters  P:  70.91%  R:  76.47%  F1:  73.58%  #: 51
org:subsidiaries                     P:  46.15%  R:  27.27%  F1:  34.29%  #: 44
org:top_members/employees            P:  65.70%  R:  84.68%  F1:  73.99%  #: 346
org:website                          P:  53.33%  R:  92.31%  F1:  67.61%  #: 26
per:age                              P:  82.17%  R:  94.50%  F1:  87.91%  #: 200
per:alternate_names                  P:   0.00%  R:   0.00%  F1:   0.00%  #: 11
per:cause_of_death                   P:  61.11%  R:  42.31%  F1:  50.00%  #: 52
per:charges                          P:  66.20%  R:  91.26%  F1:  76.73%  #: 103
per:children                         P:  32.65%  R:  43.24%  F1:  37.21%  #: 37
per:cities_of_residence              P:  50.83%  R:  65.08%  F1:  57.08%  #: 189
per:city_of_birth                    P:  50.00%  R:  20.00%  F1:  28.57%  #: 5
per:city_of_death                    P: 100.00%  R:  32.14%  F1:  48.65%  #: 28
per:countries_of_residence           P:  45.05%  R:  55.41%  F1:  49.70%  #: 148
per:country_of_birth                 P:   0.00%  R:   0.00%  F1:   0.00%  #: 5
per:country_of_death                 P: 100.00%  R:   0.00%  F1:   0.00%  #: 9
per:date_of_birth                    P:  70.00%  R:  77.78%  F1:  73.68%  #: 9
per:date_of_death                    P:  67.65%  R:  42.59%  F1:  52.27%  #: 54
per:employee_of                      P:  65.14%  R:  70.08%  F1:  67.52%  #: 264
per:origin                           P:  59.15%  R:  63.64%  F1:  61.31%  #: 132
per:other_family                     P:  64.44%  R:  48.33%  F1:  55.24%  #: 60
per:parents                          P:  60.71%  R:  57.95%  F1:  59.30%  #: 88
per:religion                         P:  46.48%  R:  70.21%  F1:  55.93%  #: 47
per:schools_attended                 P:  66.67%  R:  60.00%  F1:  63.16%  #: 30
per:siblings                         P:  66.67%  R:  69.09%  F1:  67.86%  #: 55
per:spouse                           P:  53.01%  R:  66.67%  F1:  59.06%  #: 66
per:stateorprovince_of_birth         P:  50.00%  R:  50.00%  F1:  50.00%  #: 8
per:stateorprovince_of_death         P:  50.00%  R:  28.57%  F1:  36.36%  #: 14
per:stateorprovinces_of_residence    P:  61.36%  R:  66.67%  F1:  63.91%  #: 81
per:title                            P:  75.35%  R:  86.20%  F1:  80.41%  #: 500

Final Score:
Calculating Precision: correct_by_relation= 2281.0 / guessed_by_relation= 3532.0
Calculating Recall: correct_by_relation= 2281.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 64.581%
   Recall (micro): 68.602%
       F1 (micro): 66.531%
Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.
(cuda2)


# epoch 60!


##################################################################################################

for reference:

self attention + lstm
Final Score:
Calculating Precision: correct_by_relation= 2099.0 / guessed_by_relation= 3184.0
Calculating Recall: correct_by_relation= 2099.0 / gold_by_relation= 3325.0
only_non_relation: 3325.0
Precision (micro): 65.923%
   Recall (micro): 63.128%
       F1 (micro): 64.495%
Prediction scores saved to saved_models/out/test_6.pkl.
Evaluation ended.


###################################################################################################