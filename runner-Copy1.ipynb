{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of self-attention layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Input and RNN dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.15, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='29_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/29_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/29_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.15\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 29_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/29_self_attention_dropout\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n",
      "using weights [3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-12 09:54:22.152997: step 400/300200 (epoch 1/200), loss = 0.353793 (0.045 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:54:43.122594: step 800/300200 (epoch 1/200), loss = 0.440872 (0.052 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:55:03.635277: step 1200/300200 (epoch 1/200), loss = 0.343130 (0.040 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2604.0 / guessed_by_relation= 5952.0\n",
      "Calculating Recall: correct_by_relation= 2604.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 43.750%\n",
      "   Recall (micro): 40.316%\n",
      "       F1 (micro): 41.963%\n",
      "epoch 1: train_loss = 0.435784, dev_loss = 0.573945, dev_f1 = 0.4196\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 09:55:33.151644: step 1600/300200 (epoch 2/200), loss = 0.287285 (0.061 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:56:14.107594: step 2000/300200 (epoch 2/200), loss = 0.446140 (0.106 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:56:56.399096: step 2400/300200 (epoch 2/200), loss = 0.326614 (0.109 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:57:38.374982: step 2800/300200 (epoch 2/200), loss = 0.381521 (0.106 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3359.0 / guessed_by_relation= 7274.0\n",
      "Calculating Recall: correct_by_relation= 3359.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 46.178%\n",
      "   Recall (micro): 52.005%\n",
      "       F1 (micro): 48.919%\n",
      "epoch 2: train_loss = 0.333892, dev_loss = 0.528339, dev_f1 = 0.4892\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 09:58:34.807522: step 3200/300200 (epoch 3/200), loss = 0.278538 (0.098 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:59:17.021903: step 3600/300200 (epoch 3/200), loss = 0.315081 (0.098 sec/batch), lr: 0.150000\n",
      "2018-03-12 09:59:59.305385: step 4000/300200 (epoch 3/200), loss = 0.196440 (0.111 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:00:42.183073: step 4400/300200 (epoch 3/200), loss = 0.282313 (0.105 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3933.0 / guessed_by_relation= 9056.0\n",
      "Calculating Recall: correct_by_relation= 3933.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 43.430%\n",
      "   Recall (micro): 60.892%\n",
      "       F1 (micro): 50.699%\n",
      "epoch 3: train_loss = 0.306023, dev_loss = 0.629887, dev_f1 = 0.5070\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:01:40.368815: step 4800/300200 (epoch 4/200), loss = 0.265984 (0.097 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:02:23.475353: step 5200/300200 (epoch 4/200), loss = 0.422926 (0.097 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:03:06.558647: step 5600/300200 (epoch 4/200), loss = 0.474261 (0.114 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:03:49.892727: step 6000/300200 (epoch 4/200), loss = 0.205158 (0.122 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3774.0 / guessed_by_relation= 8177.0\n",
      "Calculating Recall: correct_by_relation= 3774.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 46.154%\n",
      "   Recall (micro): 58.430%\n",
      "       F1 (micro): 51.571%\n",
      "epoch 4: train_loss = 0.290802, dev_loss = 0.552575, dev_f1 = 0.5157\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:04:48.970236: step 6400/300200 (epoch 5/200), loss = 0.273609 (0.123 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:05:32.381212: step 6800/300200 (epoch 5/200), loss = 0.217366 (0.111 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:06:16.424701: step 7200/300200 (epoch 5/200), loss = 0.322822 (0.099 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4039.0 / guessed_by_relation= 8916.0\n",
      "Calculating Recall: correct_by_relation= 4039.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 45.301%\n",
      "   Recall (micro): 62.533%\n",
      "       F1 (micro): 52.540%\n",
      "epoch 5: train_loss = 0.282828, dev_loss = 0.594062, dev_f1 = 0.5254\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:07:16.432484: step 7600/300200 (epoch 6/200), loss = 0.234967 (0.109 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:07:57.474824: step 8000/300200 (epoch 6/200), loss = 0.181603 (0.101 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:08:39.915895: step 8400/300200 (epoch 6/200), loss = 0.249291 (0.107 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:09:22.250987: step 8800/300200 (epoch 6/200), loss = 0.235212 (0.097 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3597.0 / guessed_by_relation= 6811.0\n",
      "Calculating Recall: correct_by_relation= 3597.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.812%\n",
      "   Recall (micro): 55.690%\n",
      "       F1 (micro): 54.213%\n",
      "epoch 6: train_loss = 0.274682, dev_loss = 0.444824, dev_f1 = 0.5421\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:10:19.707353: step 9200/300200 (epoch 7/200), loss = 0.372029 (0.108 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:11:01.157163: step 9600/300200 (epoch 7/200), loss = 0.194792 (0.099 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:11:43.965039: step 10000/300200 (epoch 7/200), loss = 0.229197 (0.089 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:12:26.543304: step 10400/300200 (epoch 7/200), loss = 0.129801 (0.107 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3990.0 / guessed_by_relation= 8373.0\n",
      "Calculating Recall: correct_by_relation= 3990.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.653%\n",
      "   Recall (micro): 61.774%\n",
      "       F1 (micro): 53.803%\n",
      "epoch 7: train_loss = 0.267580, dev_loss = 0.549893, dev_f1 = 0.5380\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "\n",
      "2018-03-12 10:13:22.612689: step 10800/300200 (epoch 8/200), loss = 0.223295 (0.111 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:14:04.881490: step 11200/300200 (epoch 8/200), loss = 0.503508 (0.106 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:14:48.481527: step 11600/300200 (epoch 8/200), loss = 0.252330 (0.103 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:15:31.262816: step 12000/300200 (epoch 8/200), loss = 0.111184 (0.096 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4256.0 / guessed_by_relation= 9310.0\n",
      "Calculating Recall: correct_by_relation= 4256.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 45.714%\n",
      "   Recall (micro): 65.893%\n",
      "       F1 (micro): 53.979%\n",
      "epoch 8: train_loss = 0.263326, dev_loss = 0.612357, dev_f1 = 0.5398\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "\n",
      "2018-03-12 10:16:27.332674: step 12400/300200 (epoch 9/200), loss = 0.271418 (0.101 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:17:09.688233: step 12800/300200 (epoch 9/200), loss = 0.242383 (0.114 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:17:51.850010: step 13200/300200 (epoch 9/200), loss = 0.329707 (0.105 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4208.0 / guessed_by_relation= 8560.0\n",
      "Calculating Recall: correct_by_relation= 4208.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.159%\n",
      "   Recall (micro): 65.149%\n",
      "       F1 (micro): 56.036%\n",
      "epoch 9: train_loss = 0.257499, dev_loss = 0.550985, dev_f1 = 0.5604\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:18:49.538472: step 13600/300200 (epoch 10/200), loss = 0.284144 (0.103 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:19:29.724236: step 14000/300200 (epoch 10/200), loss = 0.269921 (0.110 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:20:11.751307: step 14400/300200 (epoch 10/200), loss = 0.215840 (0.106 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:20:53.810191: step 14800/300200 (epoch 10/200), loss = 0.271495 (0.089 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4334.0 / guessed_by_relation= 8907.0\n",
      "Calculating Recall: correct_by_relation= 4334.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 48.658%\n",
      "   Recall (micro): 67.100%\n",
      "       F1 (micro): 56.410%\n",
      "epoch 10: train_loss = 0.254183, dev_loss = 0.567650, dev_f1 = 0.5641\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:21:51.950550: step 15200/300200 (epoch 11/200), loss = 0.421569 (0.071 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:22:32.486098: step 15600/300200 (epoch 11/200), loss = 0.319079 (0.115 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:23:14.446720: step 16000/300200 (epoch 11/200), loss = 0.231898 (0.109 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:23:56.471513: step 16400/300200 (epoch 11/200), loss = 0.184053 (0.105 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4335.0 / guessed_by_relation= 8646.0\n",
      "Calculating Recall: correct_by_relation= 4335.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.139%\n",
      "   Recall (micro): 67.116%\n",
      "       F1 (micro): 57.398%\n",
      "epoch 11: train_loss = 0.251363, dev_loss = 0.554857, dev_f1 = 0.5740\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:24:53.990524: step 16800/300200 (epoch 12/200), loss = 0.398419 (0.093 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:25:35.075540: step 17200/300200 (epoch 12/200), loss = 0.233372 (0.093 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:26:17.070849: step 17600/300200 (epoch 12/200), loss = 0.201161 (0.100 sec/batch), lr: 0.150000\n",
      "2018-03-12 10:26:59.135749: step 18000/300200 (epoch 12/200), loss = 0.185981 (0.106 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4522.0 / guessed_by_relation= 9716.0\n",
      "Calculating Recall: correct_by_relation= 4522.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 46.542%\n",
      "   Recall (micro): 70.011%\n",
      "       F1 (micro): 55.913%\n",
      "epoch 12: train_loss = 0.247365, dev_loss = 0.639721, dev_f1 = 0.5591\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "\n",
      "2018-03-12 10:27:55.259918: step 18400/300200 (epoch 13/200), loss = 0.330601 (0.110 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:28:37.460048: step 18800/300200 (epoch 13/200), loss = 0.151888 (0.093 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:29:19.491371: step 19200/300200 (epoch 13/200), loss = 0.187704 (0.107 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4285.0 / guessed_by_relation= 8243.0\n",
      "Calculating Recall: correct_by_relation= 4285.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.984%\n",
      "   Recall (micro): 66.342%\n",
      "       F1 (micro): 58.291%\n",
      "epoch 13: train_loss = 0.243873, dev_loss = 0.509804, dev_f1 = 0.5829\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:30:17.527407: step 19600/300200 (epoch 14/200), loss = 0.188126 (0.099 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:30:57.840294: step 20000/300200 (epoch 14/200), loss = 0.228271 (0.104 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:31:39.973375: step 20400/300200 (epoch 14/200), loss = 0.228334 (0.097 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:32:21.995164: step 20800/300200 (epoch 14/200), loss = 0.197855 (0.092 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4247.0 / guessed_by_relation= 8021.0\n",
      "Calculating Recall: correct_by_relation= 4247.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.949%\n",
      "   Recall (micro): 65.753%\n",
      "       F1 (micro): 58.660%\n",
      "epoch 14: train_loss = 0.242168, dev_loss = 0.479705, dev_f1 = 0.5866\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:33:20.244149: step 21200/300200 (epoch 15/200), loss = 0.330868 (0.109 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:34:00.685039: step 21600/300200 (epoch 15/200), loss = 0.194664 (0.100 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:34:43.343184: step 22000/300200 (epoch 15/200), loss = 0.202753 (0.099 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:35:45.463489: step 22400/300200 (epoch 15/200), loss = 0.296681 (0.156 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4238.0 / guessed_by_relation= 7927.0\n",
      "Calculating Recall: correct_by_relation= 4238.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.463%\n",
      "   Recall (micro): 65.614%\n",
      "       F1 (micro): 58.918%\n",
      "epoch 15: train_loss = 0.239889, dev_loss = 0.497069, dev_f1 = 0.5892\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:37:09.448905: step 22800/300200 (epoch 16/200), loss = 0.131594 (0.140 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:38:10.638201: step 23200/300200 (epoch 16/200), loss = 0.224220 (0.148 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:39:11.334450: step 23600/300200 (epoch 16/200), loss = 0.224780 (0.136 sec/batch), lr: 0.142500\n",
      "2018-03-12 10:40:13.685384: step 24000/300200 (epoch 16/200), loss = 0.283262 (0.155 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4643.0 / guessed_by_relation= 9527.0\n",
      "Calculating Recall: correct_by_relation= 4643.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 48.735%\n",
      "   Recall (micro): 71.884%\n",
      "       F1 (micro): 58.088%\n",
      "epoch 16: train_loss = 0.237510, dev_loss = 0.605338, dev_f1 = 0.5809\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "\n",
      "2018-03-12 10:41:37.633616: step 24400/300200 (epoch 17/200), loss = 0.170983 (0.159 sec/batch), lr: 0.135375\n",
      "2018-03-12 10:42:38.902233: step 24800/300200 (epoch 17/200), loss = 0.201636 (0.151 sec/batch), lr: 0.135375\n",
      "2018-03-12 10:43:39.641415: step 25200/300200 (epoch 17/200), loss = 0.262747 (0.134 sec/batch), lr: 0.135375\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4335.0 / guessed_by_relation= 8113.0\n",
      "Calculating Recall: correct_by_relation= 4335.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.433%\n",
      "   Recall (micro): 67.116%\n",
      "       F1 (micro): 59.498%\n",
      "epoch 17: train_loss = 0.232886, dev_loss = 0.479673, dev_f1 = 0.5950\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:45:04.275239: step 25600/300200 (epoch 18/200), loss = 0.178554 (0.168 sec/batch), lr: 0.135375\n",
      "2018-03-12 10:46:05.909195: step 26000/300200 (epoch 18/200), loss = 0.163487 (0.133 sec/batch), lr: 0.135375\n",
      "2018-03-12 10:47:07.366100: step 26400/300200 (epoch 18/200), loss = 0.206139 (0.145 sec/batch), lr: 0.135375\n",
      "2018-03-12 10:48:08.246655: step 26800/300200 (epoch 18/200), loss = 0.290335 (0.134 sec/batch), lr: 0.135375\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4573.0 / guessed_by_relation= 9264.0\n",
      "Calculating Recall: correct_by_relation= 4573.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.363%\n",
      "   Recall (micro): 70.800%\n",
      "       F1 (micro): 58.170%\n",
      "epoch 18: train_loss = 0.232333, dev_loss = 0.591703, dev_f1 = 0.5817\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "\n",
      "2018-03-12 10:49:32.204998: step 27200/300200 (epoch 19/200), loss = 0.296603 (0.132 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:50:32.903467: step 27600/300200 (epoch 19/200), loss = 0.132494 (0.081 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:51:34.506626: step 28000/300200 (epoch 19/200), loss = 0.129801 (0.141 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:52:35.596135: step 28400/300200 (epoch 19/200), loss = 0.277000 (0.156 sec/batch), lr: 0.128606\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4316.0 / guessed_by_relation= 7976.0\n",
      "Calculating Recall: correct_by_relation= 4316.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.112%\n",
      "   Recall (micro): 66.821%\n",
      "       F1 (micro): 59.799%\n",
      "epoch 19: train_loss = 0.229470, dev_loss = 0.479350, dev_f1 = 0.5980\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 10:53:59.788100: step 28800/300200 (epoch 20/200), loss = 0.296154 (0.142 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:55:00.414377: step 29200/300200 (epoch 20/200), loss = 0.156537 (0.164 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:56:00.952937: step 29600/300200 (epoch 20/200), loss = 0.160329 (0.133 sec/batch), lr: 0.128606\n",
      "2018-03-12 10:57:02.954873: step 30000/300200 (epoch 20/200), loss = 0.242769 (0.154 sec/batch), lr: 0.128606\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4383.0 / guessed_by_relation= 8340.0\n",
      "Calculating Recall: correct_by_relation= 4383.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.554%\n",
      "   Recall (micro): 67.859%\n",
      "       F1 (micro): 59.234%\n",
      "epoch 20: train_loss = 0.227689, dev_loss = 0.501421, dev_f1 = 0.5923\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "2018-03-12 10:58:26.751426: step 30400/300200 (epoch 21/200), loss = 0.134809 (0.148 sec/batch), lr: 0.122176\n",
      "2018-03-12 10:59:27.855007: step 30800/300200 (epoch 21/200), loss = 0.307370 (0.165 sec/batch), lr: 0.122176\n",
      "2018-03-12 11:00:28.511362: step 31200/300200 (epoch 21/200), loss = 0.154403 (0.172 sec/batch), lr: 0.122176\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4692.0 / guessed_by_relation= 9539.0\n",
      "Calculating Recall: correct_by_relation= 4692.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.188%\n",
      "   Recall (micro): 72.643%\n",
      "       F1 (micro): 58.657%\n",
      "epoch 21: train_loss = 0.225472, dev_loss = 0.621659, dev_f1 = 0.5866\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "2018-03-12 11:01:52.376666: step 31600/300200 (epoch 22/200), loss = 0.299478 (0.150 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:02:54.469845: step 32000/300200 (epoch 22/200), loss = 0.121057 (0.153 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:03:55.740836: step 32400/300200 (epoch 22/200), loss = 0.211859 (0.155 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:04:57.182170: step 32800/300200 (epoch 22/200), loss = 0.151412 (0.141 sec/batch), lr: 0.116067\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4588.0 / guessed_by_relation= 8905.0\n",
      "Calculating Recall: correct_by_relation= 4588.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.522%\n",
      "   Recall (micro): 71.033%\n",
      "       F1 (micro): 59.724%\n",
      "epoch 22: train_loss = 0.222403, dev_loss = 0.551704, dev_f1 = 0.5972\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "\n",
      "2018-03-12 11:06:21.813305: step 33200/300200 (epoch 23/200), loss = 0.179672 (0.151 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:07:23.905534: step 33600/300200 (epoch 23/200), loss = 0.136033 (0.161 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:08:24.930956: step 34000/300200 (epoch 23/200), loss = 0.301691 (0.154 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:09:25.886108: step 34400/300200 (epoch 23/200), loss = 0.125308 (0.152 sec/batch), lr: 0.116067\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4399.0 / guessed_by_relation= 8182.0\n",
      "Calculating Recall: correct_by_relation= 4399.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.764%\n",
      "   Recall (micro): 68.107%\n",
      "       F1 (micro): 60.092%\n",
      "epoch 23: train_loss = 0.221370, dev_loss = 0.486012, dev_f1 = 0.6009\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:10:50.170424: step 34800/300200 (epoch 24/200), loss = 0.703666 (0.144 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:11:51.792035: step 35200/300200 (epoch 24/200), loss = 0.192714 (0.163 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:12:52.300469: step 35600/300200 (epoch 24/200), loss = 0.187628 (0.153 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:13:55.202970: step 36000/300200 (epoch 24/200), loss = 0.264576 (0.163 sec/batch), lr: 0.116067\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4403.0 / guessed_by_relation= 8109.0\n",
      "Calculating Recall: correct_by_relation= 4403.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.298%\n",
      "   Recall (micro): 68.168%\n",
      "       F1 (micro): 60.448%\n",
      "epoch 24: train_loss = 0.219917, dev_loss = 0.497880, dev_f1 = 0.6045\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:15:19.182544: step 36400/300200 (epoch 25/200), loss = 0.185560 (0.167 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:16:20.751603: step 36800/300200 (epoch 25/200), loss = 0.258249 (0.153 sec/batch), lr: 0.116067\n",
      "2018-03-12 11:17:21.187924: step 37200/300200 (epoch 25/200), loss = 0.184774 (0.160 sec/batch), lr: 0.116067\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4722.0 / guessed_by_relation= 9575.0\n",
      "Calculating Recall: correct_by_relation= 4722.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.316%\n",
      "   Recall (micro): 73.107%\n",
      "       F1 (micro): 58.900%\n",
      "epoch 25: train_loss = 0.218608, dev_loss = 0.608878, dev_f1 = 0.5890\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "2018-03-12 11:18:45.348807: step 37600/300200 (epoch 26/200), loss = 0.242683 (0.150 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:19:47.412908: step 38000/300200 (epoch 26/200), loss = 0.145366 (0.147 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:20:48.386107: step 38400/300200 (epoch 26/200), loss = 0.148697 (0.153 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:21:50.014292: step 38800/300200 (epoch 26/200), loss = 0.186565 (0.166 sec/batch), lr: 0.110264\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4471.0 / guessed_by_relation= 8329.0\n",
      "Calculating Recall: correct_by_relation= 4471.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.680%\n",
      "   Recall (micro): 69.221%\n",
      "       F1 (micro): 60.468%\n",
      "epoch 26: train_loss = 0.215926, dev_loss = 0.504852, dev_f1 = 0.6047\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:23:14.478585: step 39200/300200 (epoch 27/200), loss = 0.089120 (0.154 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:24:16.509259: step 39600/300200 (epoch 27/200), loss = 0.453183 (0.143 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:25:17.146564: step 40000/300200 (epoch 27/200), loss = 0.301189 (0.146 sec/batch), lr: 0.110264\n",
      "2018-03-12 11:26:18.387658: step 40400/300200 (epoch 27/200), loss = 0.279014 (0.164 sec/batch), lr: 0.110264\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4760.0 / guessed_by_relation= 9669.0\n",
      "Calculating Recall: correct_by_relation= 4760.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.229%\n",
      "   Recall (micro): 73.696%\n",
      "       F1 (micro): 59.028%\n",
      "epoch 27: train_loss = 0.215687, dev_loss = 0.631984, dev_f1 = 0.5903\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "2018-03-12 11:27:42.176551: step 40800/300200 (epoch 28/200), loss = 0.199789 (0.141 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:28:44.004023: step 41200/300200 (epoch 28/200), loss = 0.248137 (0.149 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:29:43.861254: step 41600/300200 (epoch 28/200), loss = 0.103901 (0.159 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:30:45.569408: step 42000/300200 (epoch 28/200), loss = 0.273898 (0.163 sec/batch), lr: 0.104751\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4515.0 / guessed_by_relation= 8486.0\n",
      "Calculating Recall: correct_by_relation= 4515.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.205%\n",
      "   Recall (micro): 69.902%\n",
      "       F1 (micro): 60.422%\n",
      "epoch 28: train_loss = 0.214117, dev_loss = 0.526538, dev_f1 = 0.6042\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "\n",
      "2018-03-12 11:32:08.923144: step 42400/300200 (epoch 29/200), loss = 0.306840 (0.145 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:33:10.575149: step 42800/300200 (epoch 29/200), loss = 0.278529 (0.154 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:34:10.037328: step 43200/300200 (epoch 29/200), loss = 0.097640 (0.165 sec/batch), lr: 0.104751\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4591.0 / guessed_by_relation= 8689.0\n",
      "Calculating Recall: correct_by_relation= 4591.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.837%\n",
      "   Recall (micro): 71.079%\n",
      "       F1 (micro): 60.615%\n",
      "epoch 29: train_loss = 0.213071, dev_loss = 0.524208, dev_f1 = 0.6062\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:35:33.321880: step 43600/300200 (epoch 30/200), loss = 0.176289 (0.143 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:36:35.735095: step 44000/300200 (epoch 30/200), loss = 0.226460 (0.146 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:37:08.844929: step 44400/300200 (epoch 30/200), loss = 0.219117 (0.047 sec/batch), lr: 0.104751\n",
      "2018-03-12 11:37:29.551366: step 44800/300200 (epoch 30/200), loss = 0.336645 (0.051 sec/batch), lr: 0.104751\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4490.0 / guessed_by_relation= 8482.0\n",
      "Calculating Recall: correct_by_relation= 4490.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.936%\n",
      "   Recall (micro): 69.515%\n",
      "       F1 (micro): 60.103%\n",
      "epoch 30: train_loss = 0.210946, dev_loss = 0.526752, dev_f1 = 0.6010\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "2018-03-12 11:38:17.316813: step 45200/300200 (epoch 31/200), loss = 0.235561 (0.160 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:39:35.162148: step 45600/300200 (epoch 31/200), loss = 0.229313 (0.200 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:41:02.846175: step 46000/300200 (epoch 31/200), loss = 0.202979 (0.251 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:42:29.067402: step 46400/300200 (epoch 31/200), loss = 0.132766 (0.201 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4522.0 / guessed_by_relation= 8381.0\n",
      "Calculating Recall: correct_by_relation= 4522.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.955%\n",
      "   Recall (micro): 70.011%\n",
      "       F1 (micro): 60.943%\n",
      "epoch 31: train_loss = 0.209194, dev_loss = 0.510304, dev_f1 = 0.6094\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:44:07.848351: step 46800/300200 (epoch 32/200), loss = 0.223055 (0.217 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:45:14.837155: step 47200/300200 (epoch 32/200), loss = 0.382325 (0.166 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:46:22.607519: step 47600/300200 (epoch 32/200), loss = 0.131878 (0.168 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:47:30.117107: step 48000/300200 (epoch 32/200), loss = 0.154637 (0.213 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4630.0 / guessed_by_relation= 8683.0\n",
      "Calculating Recall: correct_by_relation= 4630.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.323%\n",
      "   Recall (micro): 71.683%\n",
      "       F1 (micro): 61.154%\n",
      "epoch 32: train_loss = 0.209482, dev_loss = 0.524125, dev_f1 = 0.6115\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:48:58.184491: step 48400/300200 (epoch 33/200), loss = 0.230173 (0.167 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:50:06.249676: step 48800/300200 (epoch 33/200), loss = 0.399397 (0.170 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:51:12.986733: step 49200/300200 (epoch 33/200), loss = 0.127630 (0.231 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4499.0 / guessed_by_relation= 8166.0\n",
      "Calculating Recall: correct_by_relation= 4499.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.094%\n",
      "   Recall (micro): 69.655%\n",
      "       F1 (micro): 61.525%\n",
      "epoch 33: train_loss = 0.207985, dev_loss = 0.484752, dev_f1 = 0.6152\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:52:40.789249: step 49600/300200 (epoch 34/200), loss = 0.366526 (0.139 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:53:48.234666: step 50000/300200 (epoch 34/200), loss = 0.206411 (0.151 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:54:56.052072: step 50400/300200 (epoch 34/200), loss = 0.135352 (0.171 sec/batch), lr: 0.099513\n",
      "2018-03-12 11:56:03.233787: step 50800/300200 (epoch 34/200), loss = 0.249965 (0.160 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4695.0 / guessed_by_relation= 9277.0\n",
      "Calculating Recall: correct_by_relation= 4695.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.609%\n",
      "   Recall (micro): 72.689%\n",
      "       F1 (micro): 59.672%\n",
      "epoch 34: train_loss = 0.206994, dev_loss = 0.602051, dev_f1 = 0.5967\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "2018-03-12 11:57:34.119859: step 51200/300200 (epoch 35/200), loss = 0.183177 (0.230 sec/batch), lr: 0.094537\n",
      "2018-03-12 11:58:46.305985: step 51600/300200 (epoch 35/200), loss = 0.085723 (0.208 sec/batch), lr: 0.094537\n",
      "2018-03-12 11:59:57.509828: step 52000/300200 (epoch 35/200), loss = 0.202133 (0.165 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:01:07.747313: step 52400/300200 (epoch 35/200), loss = 0.135901 (0.186 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4633.0 / guessed_by_relation= 8870.0\n",
      "Calculating Recall: correct_by_relation= 4633.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.232%\n",
      "   Recall (micro): 71.729%\n",
      "       F1 (micro): 60.448%\n",
      "epoch 35: train_loss = 0.205174, dev_loss = 0.552334, dev_f1 = 0.6045\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "\n",
      "2018-03-12 12:02:40.424556: step 52800/300200 (epoch 36/200), loss = 0.188019 (0.149 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:03:50.951170: step 53200/300200 (epoch 36/200), loss = 0.165351 (0.184 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:05:01.171969: step 53600/300200 (epoch 36/200), loss = 0.251891 (0.146 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:06:10.494482: step 54000/300200 (epoch 36/200), loss = 0.177785 (0.167 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4591.0 / guessed_by_relation= 8573.0\n",
      "Calculating Recall: correct_by_relation= 4591.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.552%\n",
      "   Recall (micro): 71.079%\n",
      "       F1 (micro): 61.083%\n",
      "epoch 36: train_loss = 0.203823, dev_loss = 0.527988, dev_f1 = 0.6108\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "2018-03-12 12:07:46.326412: step 54400/300200 (epoch 37/200), loss = 0.134058 (0.174 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:08:57.133772: step 54800/300200 (epoch 37/200), loss = 0.133806 (0.165 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:10:06.905248: step 55200/300200 (epoch 37/200), loss = 0.270458 (0.144 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4585.0 / guessed_by_relation= 8483.0\n",
      "Calculating Recall: correct_by_relation= 4585.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.049%\n",
      "   Recall (micro): 70.986%\n",
      "       F1 (micro): 61.371%\n",
      "epoch 37: train_loss = 0.202726, dev_loss = 0.511170, dev_f1 = 0.6137\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "2018-03-12 12:11:40.812843: step 55600/300200 (epoch 38/200), loss = 0.121176 (0.151 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:12:51.577272: step 56000/300200 (epoch 38/200), loss = 0.135158 (0.160 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:14:02.091853: step 56400/300200 (epoch 38/200), loss = 0.287775 (0.169 sec/batch), lr: 0.094537\n",
      "2018-03-12 12:15:10.667708: step 56800/300200 (epoch 38/200), loss = 0.128355 (0.197 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4579.0 / guessed_by_relation= 8664.0\n",
      "Calculating Recall: correct_by_relation= 4579.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.851%\n",
      "   Recall (micro): 70.893%\n",
      "       F1 (micro): 60.557%\n",
      "epoch 38: train_loss = 0.201931, dev_loss = 0.546677, dev_f1 = 0.6056\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "2018-03-12 12:16:46.613032: step 57200/300200 (epoch 39/200), loss = 0.211235 (0.175 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:17:56.512413: step 57600/300200 (epoch 39/200), loss = 0.327389 (0.182 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:19:06.916701: step 58000/300200 (epoch 39/200), loss = 0.350174 (0.157 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:20:15.509224: step 58400/300200 (epoch 39/200), loss = 0.210641 (0.182 sec/batch), lr: 0.089811\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4587.0 / guessed_by_relation= 8556.0\n",
      "Calculating Recall: correct_by_relation= 4587.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.612%\n",
      "   Recall (micro): 71.017%\n",
      "       F1 (micro): 61.099%\n",
      "epoch 39: train_loss = 0.201691, dev_loss = 0.529865, dev_f1 = 0.6110\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "2018-03-12 12:21:51.588360: step 58800/300200 (epoch 40/200), loss = 0.275034 (0.164 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:23:01.398066: step 59200/300200 (epoch 40/200), loss = 0.206988 (0.135 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:24:11.187742: step 59600/300200 (epoch 40/200), loss = 0.254607 (0.145 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:25:19.428273: step 60000/300200 (epoch 40/200), loss = 0.287228 (0.182 sec/batch), lr: 0.089811\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4599.0 / guessed_by_relation= 8503.0\n",
      "Calculating Recall: correct_by_relation= 4599.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.087%\n",
      "   Recall (micro): 71.203%\n",
      "       F1 (micro): 61.476%\n",
      "epoch 40: train_loss = 0.200009, dev_loss = 0.533612, dev_f1 = 0.6148\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "2018-03-12 12:26:56.707816: step 60400/300200 (epoch 41/200), loss = 0.228341 (0.169 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:28:08.454012: step 60800/300200 (epoch 41/200), loss = 0.178595 (0.171 sec/batch), lr: 0.089811\n",
      "2018-03-12 12:29:16.492394: step 61200/300200 (epoch 41/200), loss = 0.164392 (0.179 sec/batch), lr: 0.089811\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4577.0 / guessed_by_relation= 8455.0\n",
      "Calculating Recall: correct_by_relation= 4577.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.134%\n",
      "   Recall (micro): 70.862%\n",
      "       F1 (micro): 61.379%\n",
      "epoch 41: train_loss = 0.199950, dev_loss = 0.516503, dev_f1 = 0.6138\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_41.pt\n",
      "\n",
      "2018-03-12 12:30:50.905531: step 61600/300200 (epoch 42/200), loss = 0.191530 (0.212 sec/batch), lr: 0.085320\n",
      "2018-03-12 12:32:00.155750: step 62000/300200 (epoch 42/200), loss = 0.335999 (0.187 sec/batch), lr: 0.085320\n",
      "2018-03-12 12:33:09.541327: step 62400/300200 (epoch 42/200), loss = 0.208470 (0.168 sec/batch), lr: 0.085320\n",
      "2018-03-12 12:34:15.917102: step 62800/300200 (epoch 42/200), loss = 0.166251 (0.177 sec/batch), lr: 0.085320\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4513.0 / guessed_by_relation= 8298.0\n",
      "Calculating Recall: correct_by_relation= 4513.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.387%\n",
      "   Recall (micro): 69.871%\n",
      "       F1 (micro): 61.164%\n",
      "epoch 42: train_loss = 0.197757, dev_loss = 0.515799, dev_f1 = 0.6116\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_42.pt\n",
      "\n",
      "2018-03-12 12:35:51.042872: step 63200/300200 (epoch 43/200), loss = 0.181475 (0.170 sec/batch), lr: 0.081054\n",
      "2018-03-12 12:37:10.454047: step 63600/300200 (epoch 43/200), loss = 0.134619 (0.223 sec/batch), lr: 0.081054\n",
      "2018-03-12 12:38:25.416367: step 64000/300200 (epoch 43/200), loss = 0.241734 (0.211 sec/batch), lr: 0.081054\n",
      "2018-03-12 12:39:44.100293: step 64400/300200 (epoch 43/200), loss = 0.161176 (0.235 sec/batch), lr: 0.081054\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4645.0 / guessed_by_relation= 8787.0\n",
      "Calculating Recall: correct_by_relation= 4645.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.862%\n",
      "   Recall (micro): 71.915%\n",
      "       F1 (micro): 60.934%\n",
      "epoch 43: train_loss = 0.196778, dev_loss = 0.557924, dev_f1 = 0.6093\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_43.pt\n",
      "\n",
      "2018-03-12 12:41:30.928691: step 64800/300200 (epoch 44/200), loss = 0.158853 (0.182 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:42:48.465075: step 65200/300200 (epoch 44/200), loss = 0.158509 (0.206 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:44:02.017738: step 65600/300200 (epoch 44/200), loss = 0.323305 (0.204 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:45:18.990422: step 66000/300200 (epoch 44/200), loss = 0.342150 (0.186 sec/batch), lr: 0.077001\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4493.0 / guessed_by_relation= 8106.0\n",
      "Calculating Recall: correct_by_relation= 4493.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.428%\n",
      "   Recall (micro): 69.562%\n",
      "       F1 (micro): 61.696%\n",
      "epoch 44: train_loss = 0.195030, dev_loss = 0.503035, dev_f1 = 0.6170\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_44.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:47:02.898979: step 66400/300200 (epoch 45/200), loss = 0.163979 (0.216 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:48:18.984186: step 66800/300200 (epoch 45/200), loss = 0.216408 (0.177 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:49:34.274471: step 67200/300200 (epoch 45/200), loss = 0.306691 (0.172 sec/batch), lr: 0.077001\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4594.0 / guessed_by_relation= 8402.0\n",
      "Calculating Recall: correct_by_relation= 4594.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.677%\n",
      "   Recall (micro): 71.126%\n",
      "       F1 (micro): 61.826%\n",
      "epoch 45: train_loss = 0.195334, dev_loss = 0.511907, dev_f1 = 0.6183\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_45.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:51:18.299992: step 67600/300200 (epoch 46/200), loss = 0.230040 (0.202 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:52:35.387057: step 68000/300200 (epoch 46/200), loss = 0.258263 (0.159 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:53:49.555496: step 68400/300200 (epoch 46/200), loss = 0.249902 (0.187 sec/batch), lr: 0.077001\n",
      "2018-03-12 12:55:06.336748: step 68800/300200 (epoch 46/200), loss = 0.196299 (0.171 sec/batch), lr: 0.077001\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4628.0 / guessed_by_relation= 8606.0\n",
      "Calculating Recall: correct_by_relation= 4628.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.776%\n",
      "   Recall (micro): 71.652%\n",
      "       F1 (micro): 61.440%\n",
      "epoch 46: train_loss = 0.194381, dev_loss = 0.543151, dev_f1 = 0.6144\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_46.pt\n",
      "\n",
      "2018-03-12 12:56:50.884841: step 69200/300200 (epoch 47/200), loss = 0.248797 (0.165 sec/batch), lr: 0.073151\n",
      "2018-03-12 12:58:06.998316: step 69600/300200 (epoch 47/200), loss = 0.195704 (0.192 sec/batch), lr: 0.073151\n",
      "2018-03-12 12:59:20.863811: step 70000/300200 (epoch 47/200), loss = 0.359691 (0.198 sec/batch), lr: 0.073151\n",
      "2018-03-12 13:00:37.364315: step 70400/300200 (epoch 47/200), loss = 0.145332 (0.205 sec/batch), lr: 0.073151\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4543.0 / guessed_by_relation= 8132.0\n",
      "Calculating Recall: correct_by_relation= 4543.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.866%\n",
      "   Recall (micro): 70.336%\n",
      "       F1 (micro): 62.271%\n",
      "epoch 47: train_loss = 0.193741, dev_loss = 0.492165, dev_f1 = 0.6227\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_47.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:02:45.559804: step 70800/300200 (epoch 48/200), loss = 0.154195 (0.241 sec/batch), lr: 0.073151\n",
      "2018-03-12 13:04:42.047257: step 71200/300200 (epoch 48/200), loss = 0.298567 (0.310 sec/batch), lr: 0.073151\n",
      "2018-03-12 13:06:38.088559: step 71600/300200 (epoch 48/200), loss = 0.058204 (0.388 sec/batch), lr: 0.073151\n",
      "2018-03-12 13:08:33.829010: step 72000/300200 (epoch 48/200), loss = 0.103093 (0.254 sec/batch), lr: 0.073151\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4616.0 / guessed_by_relation= 8595.0\n",
      "Calculating Recall: correct_by_relation= 4616.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.706%\n",
      "   Recall (micro): 71.466%\n",
      "       F1 (micro): 61.326%\n",
      "epoch 48: train_loss = 0.191977, dev_loss = 0.549800, dev_f1 = 0.6133\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_48.pt\n",
      "\n",
      "2018-03-12 13:11:03.373740: step 72400/300200 (epoch 49/200), loss = 0.238450 (0.226 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:12:57.696880: step 72800/300200 (epoch 49/200), loss = 0.341667 (0.279 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:14:52.974539: step 73200/300200 (epoch 49/200), loss = 0.125392 (0.246 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4684.0 / guessed_by_relation= 8788.0\n",
      "Calculating Recall: correct_by_relation= 4684.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.300%\n",
      "   Recall (micro): 72.519%\n",
      "       F1 (micro): 61.442%\n",
      "epoch 49: train_loss = 0.192542, dev_loss = 0.559967, dev_f1 = 0.6144\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_49.pt\n",
      "\n",
      "2018-03-12 13:17:21.623391: step 73600/300200 (epoch 50/200), loss = 0.259140 (0.329 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:19:16.990380: step 74000/300200 (epoch 50/200), loss = 0.140105 (0.279 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:21:11.381189: step 74400/300200 (epoch 50/200), loss = 0.194481 (0.400 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:23:05.832650: step 74800/300200 (epoch 50/200), loss = 0.129639 (0.304 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4461.0 / guessed_by_relation= 8000.0\n",
      "Calculating Recall: correct_by_relation= 4461.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.763%\n",
      "   Recall (micro): 69.066%\n",
      "       F1 (micro): 61.706%\n",
      "epoch 50: train_loss = 0.191215, dev_loss = 0.494069, dev_f1 = 0.6171\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_50.pt\n",
      "\n",
      "2018-03-12 13:25:34.992310: step 75200/300200 (epoch 51/200), loss = 0.105773 (0.303 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:27:28.835152: step 75600/300200 (epoch 51/200), loss = 0.300719 (0.300 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:29:22.658943: step 76000/300200 (epoch 51/200), loss = 0.205170 (0.400 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:31:15.559278: step 76400/300200 (epoch 51/200), loss = 0.144532 (0.282 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4634.0 / guessed_by_relation= 8455.0\n",
      "Calculating Recall: correct_by_relation= 4634.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.808%\n",
      "   Recall (micro): 71.745%\n",
      "       F1 (micro): 62.143%\n",
      "epoch 51: train_loss = 0.189862, dev_loss = 0.529062, dev_f1 = 0.6214\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_51.pt\n",
      "\n",
      "2018-03-12 13:33:45.621470: step 76800/300200 (epoch 52/200), loss = 0.296620 (0.310 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:35:39.210636: step 77200/300200 (epoch 52/200), loss = 0.129475 (0.262 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:37:33.621084: step 77600/300200 (epoch 52/200), loss = 0.193504 (0.282 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:39:28.794538: step 78000/300200 (epoch 52/200), loss = 0.176738 (0.234 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4523.0 / guessed_by_relation= 8053.0\n",
      "Calculating Recall: correct_by_relation= 4523.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.165%\n",
      "   Recall (micro): 70.026%\n",
      "       F1 (micro): 62.335%\n",
      "epoch 52: train_loss = 0.190723, dev_loss = 0.496696, dev_f1 = 0.6233\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_52.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:41:56.613710: step 78400/300200 (epoch 53/200), loss = 0.236156 (0.285 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:43:50.614973: step 78800/300200 (epoch 53/200), loss = 0.197794 (0.305 sec/batch), lr: 0.069494\n",
      "2018-03-12 13:45:44.956010: step 79200/300200 (epoch 53/200), loss = 0.114072 (0.281 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4534.0 / guessed_by_relation= 8260.0\n",
      "Calculating Recall: correct_by_relation= 4534.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.891%\n",
      "   Recall (micro): 70.197%\n",
      "       F1 (micro): 61.607%\n",
      "epoch 53: train_loss = 0.189920, dev_loss = 0.524926, dev_f1 = 0.6161\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_53.pt\n",
      "\n",
      "2018-03-12 13:48:13.457049: step 79600/300200 (epoch 54/200), loss = 0.100136 (0.294 sec/batch), lr: 0.066019\n",
      "2018-03-12 13:50:08.514121: step 80000/300200 (epoch 54/200), loss = 0.209823 (0.234 sec/batch), lr: 0.066019\n",
      "2018-03-12 13:52:01.439522: step 80400/300200 (epoch 54/200), loss = 0.246280 (0.259 sec/batch), lr: 0.066019\n",
      "2018-03-12 13:53:55.376614: step 80800/300200 (epoch 54/200), loss = 0.057811 (0.324 sec/batch), lr: 0.066019\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4565.0 / guessed_by_relation= 8272.0\n",
      "Calculating Recall: correct_by_relation= 4565.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.186%\n",
      "   Recall (micro): 70.677%\n",
      "       F1 (micro): 61.978%\n",
      "epoch 54: train_loss = 0.188141, dev_loss = 0.520373, dev_f1 = 0.6198\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_54.pt\n",
      "\n",
      "2018-03-12 13:56:23.505663: step 81200/300200 (epoch 55/200), loss = 0.098728 (0.206 sec/batch), lr: 0.066019\n",
      "2018-03-12 13:58:16.778990: step 81600/300200 (epoch 55/200), loss = 0.241195 (0.276 sec/batch), lr: 0.066019\n",
      "2018-03-12 14:00:10.579720: step 82000/300200 (epoch 55/200), loss = 0.169472 (0.312 sec/batch), lr: 0.066019\n",
      "2018-03-12 14:02:03.990412: step 82400/300200 (epoch 55/200), loss = 0.165845 (0.262 sec/batch), lr: 0.066019\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4469.0 / guessed_by_relation= 7978.0\n",
      "Calculating Recall: correct_by_relation= 4469.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.017%\n",
      "   Recall (micro): 69.190%\n",
      "       F1 (micro): 61.910%\n",
      "epoch 55: train_loss = 0.187672, dev_loss = 0.494100, dev_f1 = 0.6191\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_55.pt\n",
      "\n",
      "2018-03-12 14:04:33.270523: step 82800/300200 (epoch 56/200), loss = 0.187463 (0.234 sec/batch), lr: 0.062718\n",
      "2018-03-12 14:06:26.915840: step 83200/300200 (epoch 56/200), loss = 0.201625 (0.310 sec/batch), lr: 0.062718\n",
      "2018-03-12 14:08:20.540100: step 83600/300200 (epoch 56/200), loss = 0.220250 (0.269 sec/batch), lr: 0.062718\n",
      "2018-03-12 14:10:15.170036: step 84000/300200 (epoch 56/200), loss = 0.192962 (0.262 sec/batch), lr: 0.062718\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4480.0 / guessed_by_relation= 7956.0\n",
      "Calculating Recall: correct_by_relation= 4480.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.310%\n",
      "   Recall (micro): 69.361%\n",
      "       F1 (micro): 62.157%\n",
      "epoch 56: train_loss = 0.186253, dev_loss = 0.492407, dev_f1 = 0.6216\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_56.pt\n",
      "\n",
      "2018-03-12 14:12:44.141325: step 84400/300200 (epoch 57/200), loss = 0.196897 (0.245 sec/batch), lr: 0.062718\n",
      "2018-03-12 14:14:37.237179: step 84800/300200 (epoch 57/200), loss = 0.203253 (0.271 sec/batch), lr: 0.062718\n",
      "2018-03-12 14:16:32.141846: step 85200/300200 (epoch 57/200), loss = 0.188480 (0.325 sec/batch), lr: 0.062718\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4519.0 / guessed_by_relation= 8175.0\n",
      "Calculating Recall: correct_by_relation= 4519.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.278%\n",
      "   Recall (micro): 69.964%\n",
      "       F1 (micro): 61.760%\n",
      "epoch 57: train_loss = 0.186593, dev_loss = 0.514054, dev_f1 = 0.6176\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_57.pt\n",
      "\n",
      "2018-03-12 14:18:58.808003: step 85600/300200 (epoch 58/200), loss = 0.156197 (0.244 sec/batch), lr: 0.059582\n",
      "2018-03-12 14:20:54.426569: step 86000/300200 (epoch 58/200), loss = 0.217276 (0.330 sec/batch), lr: 0.059582\n",
      "2018-03-12 14:22:47.127373: step 86400/300200 (epoch 58/200), loss = 0.175975 (0.269 sec/batch), lr: 0.059582\n",
      "2018-03-12 14:24:40.610257: step 86800/300200 (epoch 58/200), loss = 0.170687 (0.281 sec/batch), lr: 0.059582\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4549.0 / guessed_by_relation= 8341.0\n",
      "Calculating Recall: correct_by_relation= 4549.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.538%\n",
      "   Recall (micro): 70.429%\n",
      "       F1 (micro): 61.473%\n",
      "epoch 58: train_loss = 0.185483, dev_loss = 0.534858, dev_f1 = 0.6147\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_58.pt\n",
      "\n",
      "2018-03-12 14:27:11.018368: step 87200/300200 (epoch 59/200), loss = 0.216704 (0.242 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:29:04.620570: step 87600/300200 (epoch 59/200), loss = 0.170974 (0.256 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:30:58.516553: step 88000/300200 (epoch 59/200), loss = 0.240255 (0.280 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:32:51.909197: step 88400/300200 (epoch 59/200), loss = 0.197524 (0.229 sec/batch), lr: 0.056603\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4541.0 / guessed_by_relation= 8054.0\n",
      "Calculating Recall: correct_by_relation= 4541.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.382%\n",
      "   Recall (micro): 70.305%\n",
      "       F1 (micro): 62.578%\n",
      "epoch 59: train_loss = 0.184529, dev_loss = 0.505631, dev_f1 = 0.6258\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_59.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 14:35:20.235772: step 88800/300200 (epoch 60/200), loss = 0.255890 (0.318 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:37:14.066581: step 89200/300200 (epoch 60/200), loss = 0.291687 (0.251 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:39:07.645722: step 89600/300200 (epoch 60/200), loss = 0.230631 (0.326 sec/batch), lr: 0.056603\n",
      "2018-03-12 14:41:01.770313: step 90000/300200 (epoch 60/200), loss = 0.122882 (0.272 sec/batch), lr: 0.056603\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4548.0 / guessed_by_relation= 8127.0\n",
      "Calculating Recall: correct_by_relation= 4548.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.962%\n",
      "   Recall (micro): 70.413%\n",
      "       F1 (micro): 62.361%\n",
      "epoch 60: train_loss = 0.184427, dev_loss = 0.510900, dev_f1 = 0.6236\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_60.pt\n",
      "\n",
      "2018-03-12 14:43:30.524024: step 90400/300200 (epoch 61/200), loss = 0.066579 (0.278 sec/batch), lr: 0.053773\n",
      "2018-03-12 14:45:23.117542: step 90800/300200 (epoch 61/200), loss = 0.161629 (0.305 sec/batch), lr: 0.053773\n",
      "2018-03-12 14:47:17.700352: step 91200/300200 (epoch 61/200), loss = 0.188105 (0.309 sec/batch), lr: 0.053773\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4398.0 / guessed_by_relation= 7611.0\n",
      "Calculating Recall: correct_by_relation= 4398.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.785%\n",
      "   Recall (micro): 68.091%\n",
      "       F1 (micro): 62.516%\n",
      "epoch 61: train_loss = 0.183450, dev_loss = 0.472806, dev_f1 = 0.6252\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_61.pt\n",
      "\n",
      "2018-03-12 14:49:44.600131: step 91600/300200 (epoch 62/200), loss = 0.150108 (0.282 sec/batch), lr: 0.053773\n",
      "2018-03-12 14:51:40.755124: step 92000/300200 (epoch 62/200), loss = 0.175821 (0.313 sec/batch), lr: 0.053773\n",
      "2018-03-12 14:53:33.750712: step 92400/300200 (epoch 62/200), loss = 0.267108 (0.294 sec/batch), lr: 0.053773\n",
      "2018-03-12 14:55:27.294758: step 92800/300200 (epoch 62/200), loss = 0.231573 (0.320 sec/batch), lr: 0.053773\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4486.0 / guessed_by_relation= 8006.0\n",
      "Calculating Recall: correct_by_relation= 4486.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.033%\n",
      "   Recall (micro): 69.453%\n",
      "       F1 (micro): 62.026%\n",
      "epoch 62: train_loss = 0.182123, dev_loss = 0.507396, dev_f1 = 0.6203\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_62.pt\n",
      "\n",
      "2018-03-12 14:57:57.096256: step 93200/300200 (epoch 63/200), loss = 0.177540 (0.312 sec/batch), lr: 0.051084\n",
      "2018-03-12 14:59:50.236228: step 93600/300200 (epoch 63/200), loss = 0.225659 (0.252 sec/batch), lr: 0.051084\n",
      "2018-03-12 15:01:44.191369: step 94000/300200 (epoch 63/200), loss = 0.213245 (0.295 sec/batch), lr: 0.051084\n",
      "2018-03-12 15:03:38.406200: step 94400/300200 (epoch 63/200), loss = 0.170207 (0.276 sec/batch), lr: 0.051084\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4512.0 / guessed_by_relation= 8061.0\n",
      "Calculating Recall: correct_by_relation= 4512.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.973%\n",
      "   Recall (micro): 69.856%\n",
      "       F1 (micro): 62.149%\n",
      "epoch 63: train_loss = 0.180870, dev_loss = 0.511102, dev_f1 = 0.6215\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_63.pt\n",
      "\n",
      "2018-03-12 15:06:07.041596: step 94800/300200 (epoch 64/200), loss = 0.185929 (0.318 sec/batch), lr: 0.051084\n",
      "2018-03-12 15:08:01.200278: step 95200/300200 (epoch 64/200), loss = 0.220518 (0.245 sec/batch), lr: 0.051084\n",
      "2018-03-12 15:09:54.897733: step 95600/300200 (epoch 64/200), loss = 0.099046 (0.296 sec/batch), lr: 0.051084\n",
      "2018-03-12 15:11:49.037364: step 96000/300200 (epoch 64/200), loss = 0.167548 (0.316 sec/batch), lr: 0.051084\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4624.0 / guessed_by_relation= 8549.0\n",
      "Calculating Recall: correct_by_relation= 4624.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.088%\n",
      "   Recall (micro): 71.590%\n",
      "       F1 (micro): 61.620%\n",
      "epoch 64: train_loss = 0.180559, dev_loss = 0.554888, dev_f1 = 0.6162\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_64.pt\n",
      "\n",
      "2018-03-12 15:14:18.599225: step 96400/300200 (epoch 65/200), loss = 0.125740 (0.300 sec/batch), lr: 0.048530\n",
      "2018-03-12 15:16:11.461458: step 96800/300200 (epoch 65/200), loss = 0.264151 (0.323 sec/batch), lr: 0.048530\n",
      "2018-03-12 15:18:05.815660: step 97200/300200 (epoch 65/200), loss = 0.212708 (0.278 sec/batch), lr: 0.048530\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4512.0 / guessed_by_relation= 8177.0\n",
      "Calculating Recall: correct_by_relation= 4512.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.179%\n",
      "   Recall (micro): 69.856%\n",
      "       F1 (micro): 61.656%\n",
      "epoch 65: train_loss = 0.180988, dev_loss = 0.522554, dev_f1 = 0.6166\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_65.pt\n",
      "\n",
      "2018-03-12 15:20:33.499525: step 97600/300200 (epoch 66/200), loss = 0.286034 (0.291 sec/batch), lr: 0.048530\n",
      "2018-03-12 15:22:28.059274: step 98000/300200 (epoch 66/200), loss = 0.163807 (0.296 sec/batch), lr: 0.048530\n",
      "2018-03-12 15:24:21.962276: step 98400/300200 (epoch 66/200), loss = 0.162676 (0.251 sec/batch), lr: 0.048530\n",
      "2018-03-12 15:26:15.386003: step 98800/300200 (epoch 66/200), loss = 0.081536 (0.219 sec/batch), lr: 0.048530\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4562.0 / guessed_by_relation= 8361.0\n",
      "Calculating Recall: correct_by_relation= 4562.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.563%\n",
      "   Recall (micro): 70.630%\n",
      "       F1 (micro): 61.565%\n",
      "epoch 66: train_loss = 0.179742, dev_loss = 0.537722, dev_f1 = 0.6157\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_66.pt\n",
      "\n",
      "2018-03-12 15:28:44.590913: step 99200/300200 (epoch 67/200), loss = 0.193353 (0.247 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:30:38.772657: step 99600/300200 (epoch 67/200), loss = 0.267900 (0.265 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:32:31.533621: step 100000/300200 (epoch 67/200), loss = 0.199551 (0.292 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:34:26.137487: step 100400/300200 (epoch 67/200), loss = 0.070777 (0.314 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4568.0 / guessed_by_relation= 8354.0\n",
      "Calculating Recall: correct_by_relation= 4568.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.680%\n",
      "   Recall (micro): 70.723%\n",
      "       F1 (micro): 61.676%\n",
      "epoch 67: train_loss = 0.179089, dev_loss = 0.539388, dev_f1 = 0.6168\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_67.pt\n",
      "\n",
      "2018-03-12 15:36:54.187325: step 100800/300200 (epoch 68/200), loss = 0.208646 (0.287 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:38:48.669870: step 101200/300200 (epoch 68/200), loss = 0.146477 (0.223 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:40:41.808838: step 101600/300200 (epoch 68/200), loss = 0.094594 (0.251 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:42:35.860234: step 102000/300200 (epoch 68/200), loss = 0.132961 (0.245 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4554.0 / guessed_by_relation= 8298.0\n",
      "Calculating Recall: correct_by_relation= 4554.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.881%\n",
      "   Recall (micro): 70.506%\n",
      "       F1 (micro): 61.720%\n",
      "epoch 68: train_loss = 0.178582, dev_loss = 0.533780, dev_f1 = 0.6172\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_68.pt\n",
      "\n",
      "2018-03-12 15:45:06.205178: step 102400/300200 (epoch 69/200), loss = 0.165212 (0.238 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:46:59.364201: step 102800/300200 (epoch 69/200), loss = 0.130109 (0.305 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:48:53.015533: step 103200/300200 (epoch 69/200), loss = 0.218651 (0.237 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4632.0 / guessed_by_relation= 8517.0\n",
      "Calculating Recall: correct_by_relation= 4632.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.385%\n",
      "   Recall (micro): 71.714%\n",
      "       F1 (micro): 61.859%\n",
      "epoch 69: train_loss = 0.177051, dev_loss = 0.570379, dev_f1 = 0.6186\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_69.pt\n",
      "\n",
      "2018-03-12 15:51:20.033627: step 103600/300200 (epoch 70/200), loss = 0.291234 (0.296 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:53:14.547253: step 104000/300200 (epoch 70/200), loss = 0.376379 (0.263 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:55:08.225657: step 104400/300200 (epoch 70/200), loss = 0.121926 (0.308 sec/batch), lr: 0.046104\n",
      "2018-03-12 15:57:02.316158: step 104800/300200 (epoch 70/200), loss = 0.251013 (0.300 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4571.0 / guessed_by_relation= 8237.0\n",
      "Calculating Recall: correct_by_relation= 4571.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.494%\n",
      "   Recall (micro): 70.769%\n",
      "       F1 (micro): 62.207%\n",
      "epoch 70: train_loss = 0.178185, dev_loss = 0.533942, dev_f1 = 0.6221\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_70.pt\n",
      "\n",
      "2018-03-12 15:59:31.096940: step 105200/300200 (epoch 71/200), loss = 0.185675 (0.288 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:01:25.715846: step 105600/300200 (epoch 71/200), loss = 0.143535 (0.298 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:03:18.513909: step 106000/300200 (epoch 71/200), loss = 0.171541 (0.305 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:05:12.821988: step 106400/300200 (epoch 71/200), loss = 0.173223 (0.305 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4531.0 / guessed_by_relation= 8143.0\n",
      "Calculating Recall: correct_by_relation= 4531.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.643%\n",
      "   Recall (micro): 70.150%\n",
      "       F1 (micro): 62.060%\n",
      "epoch 71: train_loss = 0.177102, dev_loss = 0.529806, dev_f1 = 0.6206\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_71.pt\n",
      "\n",
      "2018-03-12 16:07:41.735889: step 106800/300200 (epoch 72/200), loss = 0.140869 (0.376 sec/batch), lr: 0.043798\n",
      "2018-03-12 16:09:35.746497: step 107200/300200 (epoch 72/200), loss = 0.332973 (0.315 sec/batch), lr: 0.043798\n",
      "2018-03-12 16:11:30.935635: step 107600/300200 (epoch 72/200), loss = 0.104360 (0.230 sec/batch), lr: 0.043798\n",
      "2018-03-12 16:13:28.250273: step 108000/300200 (epoch 72/200), loss = 0.270551 (0.255 sec/batch), lr: 0.043798\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4545.0 / guessed_by_relation= 8168.0\n",
      "Calculating Recall: correct_by_relation= 4545.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.644%\n",
      "   Recall (micro): 70.367%\n",
      "       F1 (micro): 62.145%\n",
      "epoch 72: train_loss = 0.176450, dev_loss = 0.525783, dev_f1 = 0.6215\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_72.pt\n",
      "\n",
      "2018-03-12 16:15:59.387351: step 108400/300200 (epoch 73/200), loss = 0.127470 (0.222 sec/batch), lr: 0.043798\n",
      "2018-03-12 16:17:53.498304: step 108800/300200 (epoch 73/200), loss = 0.105199 (0.298 sec/batch), lr: 0.043798\n",
      "2018-03-12 16:19:48.000023: step 109200/300200 (epoch 73/200), loss = 0.150530 (0.308 sec/batch), lr: 0.043798\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4507.0 / guessed_by_relation= 8129.0\n",
      "Calculating Recall: correct_by_relation= 4507.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.443%\n",
      "   Recall (micro): 69.779%\n",
      "       F1 (micro): 61.791%\n",
      "epoch 73: train_loss = 0.175425, dev_loss = 0.533136, dev_f1 = 0.6179\n",
      "model saved to ./saved_models/29_self_attention_dropout/checkpoint_epoch_73.pt\n",
      "\n",
      "2018-03-12 16:22:16.923616: step 109600/300200 (epoch 74/200), loss = 0.174106 (0.257 sec/batch), lr: 0.041608\n",
      "2018-03-12 16:24:11.596634: step 110000/300200 (epoch 74/200), loss = 0.150132 (0.270 sec/batch), lr: 0.041608\n",
      "2018-03-12 16:26:42.758763: step 110400/300200 (epoch 74/200), loss = 0.246222 (0.318 sec/batch), lr: 0.041608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-dfad53c8c064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save the model during every epoch, if fscore is the best, move it to best_model.pkl\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    # delete single checkpoints based on the save_epoch int\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        # don't go lower than 0.01 lr\n",
    "        if current_lr >= 0.01:\n",
    "            current_lr *= opt['lr_decay']\n",
    "            model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using class weights, dropout 0.4\n",
    "\"\"\"\n",
    "weights = [\n",
    "    1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8\n",
    "]\n",
    "\n",
    "\n",
    "# new\n",
    "weights = [\n",
    "    1.0, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
    "    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
    "    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
    "    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
    "    0.3, 0.3\n",
    "]\n",
    "\n",
    "# newer\n",
    "weights = [\n",
    "    0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "    1.0, 1.0\n",
    "]\n",
    "\n",
    "\n",
    "        weights = [\n",
    "            3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0\n",
    "        ]\n",
    "        dropout=0.5\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without residual and with ulinear\n",
    "# other weights\n",
    "# now dropout 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
