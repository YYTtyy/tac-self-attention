{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of self-attention layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.4, help='Input and RNN dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.2, help='Applies to SGD and Adagrad.')            # lr\n",
    "parser.add_argument('--lr_decay', type=float, default=0.9)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=100)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='20_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/20_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/20_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.4\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.2\n",
      "\tlr_decay : 0.9\n",
      "\toptim : adam\n",
      "\tnum_epoch : 100\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 20_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/20_self_attention_dropout\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 17:02:03.996036: step 400/150100 (epoch 1/100), loss = 0.571810 (0.069 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:02:34.542276: step 800/150100 (epoch 1/100), loss = 0.511746 (0.078 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:03:04.811780: step 1200/150100 (epoch 1/100), loss = 0.573071 (0.060 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2921.0 / guessed_by_relation= 5227.0\n",
      "Calculating Recall: correct_by_relation= 2921.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.883%\n",
      "   Recall (micro): 45.224%\n",
      "       F1 (micro): 49.991%\n",
      "epoch 1: train_loss = 0.654849, dev_loss = 0.630971, dev_f1 = 0.4999\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:03:48.363977: step 1600/150100 (epoch 2/100), loss = 0.384177 (0.080 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:04:18.810834: step 2000/150100 (epoch 2/100), loss = 0.523320 (0.078 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:04:49.230655: step 2400/150100 (epoch 2/100), loss = 0.542269 (0.080 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:05:19.749711: step 2800/150100 (epoch 2/100), loss = 0.655609 (0.078 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3267.0 / guessed_by_relation= 5420.0\n",
      "Calculating Recall: correct_by_relation= 3267.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.277%\n",
      "   Recall (micro): 50.581%\n",
      "       F1 (micro): 55.005%\n",
      "epoch 2: train_loss = 0.468231, dev_loss = 0.578911, dev_f1 = 0.5500\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:06:04.216488: step 3200/150100 (epoch 3/100), loss = 0.512519 (0.065 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:06:34.752769: step 3600/150100 (epoch 3/100), loss = 0.303961 (0.074 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:07:05.357044: step 4000/150100 (epoch 3/100), loss = 0.340849 (0.081 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:07:39.739487: step 4400/150100 (epoch 3/100), loss = 0.305750 (0.145 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3116.0 / guessed_by_relation= 4703.0\n",
      "Calculating Recall: correct_by_relation= 3116.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.256%\n",
      "   Recall (micro): 48.243%\n",
      "       F1 (micro): 55.832%\n",
      "epoch 3: train_loss = 0.422399, dev_loss = 0.566043, dev_f1 = 0.5583\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:09:03.502835: step 4800/150100 (epoch 4/100), loss = 0.159230 (0.128 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:10:05.259532: step 5200/150100 (epoch 4/100), loss = 0.607966 (0.149 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:11:05.787225: step 5600/150100 (epoch 4/100), loss = 0.585121 (0.150 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:12:03.252114: step 6000/150100 (epoch 4/100), loss = 0.249457 (0.176 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3498.0 / guessed_by_relation= 5509.0\n",
      "Calculating Recall: correct_by_relation= 3498.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.496%\n",
      "   Recall (micro): 54.157%\n",
      "       F1 (micro): 58.456%\n",
      "epoch 4: train_loss = 0.393788, dev_loss = 0.553803, dev_f1 = 0.5846\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:13:28.155687: step 6400/150100 (epoch 5/100), loss = 0.315796 (0.146 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:14:28.383740: step 6800/150100 (epoch 5/100), loss = 0.269077 (0.158 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:15:27.470703: step 7200/150100 (epoch 5/100), loss = 0.441884 (0.134 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3088.0 / guessed_by_relation= 4744.0\n",
      "Calculating Recall: correct_by_relation= 3088.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.093%\n",
      "   Recall (micro): 47.809%\n",
      "       F1 (micro): 55.128%\n",
      "epoch 5: train_loss = 0.371374, dev_loss = 0.562137, dev_f1 = 0.5513\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "\n",
      "2018-03-08 17:16:50.566281: step 7600/150100 (epoch 6/100), loss = 0.353969 (0.139 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:17:48.507417: step 8000/150100 (epoch 6/100), loss = 0.296554 (0.139 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:18:45.766825: step 8400/150100 (epoch 6/100), loss = 0.416840 (0.147 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:19:42.231998: step 8800/150100 (epoch 6/100), loss = 0.412740 (0.136 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3379.0 / guessed_by_relation= 5073.0\n",
      "Calculating Recall: correct_by_relation= 3379.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.608%\n",
      "   Recall (micro): 52.315%\n",
      "       F1 (micro): 58.602%\n",
      "epoch 6: train_loss = 0.353734, dev_loss = 0.547268, dev_f1 = 0.5860\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:21:04.964184: step 9200/150100 (epoch 7/100), loss = 0.555031 (0.158 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:22:04.182943: step 9600/150100 (epoch 7/100), loss = 0.174114 (0.152 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:23:04.382046: step 10000/150100 (epoch 7/100), loss = 0.174280 (0.149 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:24:03.021261: step 10400/150100 (epoch 7/100), loss = 0.142169 (0.152 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3514.0 / guessed_by_relation= 5623.0\n",
      "Calculating Recall: correct_by_relation= 3514.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.493%\n",
      "   Recall (micro): 54.405%\n",
      "       F1 (micro): 58.169%\n",
      "epoch 7: train_loss = 0.335403, dev_loss = 0.570355, dev_f1 = 0.5817\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "\n",
      "2018-03-08 17:25:25.431966: step 10800/150100 (epoch 8/100), loss = 0.290275 (0.151 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:26:25.208758: step 11200/150100 (epoch 8/100), loss = 0.408701 (0.170 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:27:22.927839: step 11600/150100 (epoch 8/100), loss = 0.405338 (0.138 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:28:19.465205: step 12000/150100 (epoch 8/100), loss = 0.189702 (0.135 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3351.0 / guessed_by_relation= 5086.0\n",
      "Calculating Recall: correct_by_relation= 3351.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.887%\n",
      "   Recall (micro): 51.881%\n",
      "       F1 (micro): 58.051%\n",
      "epoch 8: train_loss = 0.321931, dev_loss = 0.564369, dev_f1 = 0.5805\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "\n",
      "2018-03-08 17:29:42.708062: step 12400/150100 (epoch 9/100), loss = 0.237793 (0.144 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:30:41.059251: step 12800/150100 (epoch 9/100), loss = 0.290165 (0.149 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:31:37.618458: step 13200/150100 (epoch 9/100), loss = 0.392696 (0.140 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3477.0 / guessed_by_relation= 5399.0\n",
      "Calculating Recall: correct_by_relation= 3477.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.401%\n",
      "   Recall (micro): 53.832%\n",
      "       F1 (micro): 58.644%\n",
      "epoch 9: train_loss = 0.308013, dev_loss = 0.560825, dev_f1 = 0.5864\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-08 17:32:59.187425: step 13600/150100 (epoch 10/100), loss = 0.277461 (0.150 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:33:56.868833: step 14000/150100 (epoch 10/100), loss = 0.370580 (0.153 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:34:54.652659: step 14400/150100 (epoch 10/100), loss = 0.172418 (0.146 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:35:51.075675: step 14800/150100 (epoch 10/100), loss = 0.347199 (0.131 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3375.0 / guessed_by_relation= 5329.0\n",
      "Calculating Recall: correct_by_relation= 3375.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.333%\n",
      "   Recall (micro): 52.253%\n",
      "       F1 (micro): 57.262%\n",
      "epoch 10: train_loss = 0.293492, dev_loss = 0.574884, dev_f1 = 0.5726\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "\n",
      "2018-03-08 17:37:12.299097: step 15200/150100 (epoch 11/100), loss = 0.368755 (0.102 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:38:09.632580: step 15600/150100 (epoch 11/100), loss = 0.282249 (0.148 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:39:07.099417: step 16000/150100 (epoch 11/100), loss = 0.265125 (0.148 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:40:03.475353: step 16400/150100 (epoch 11/100), loss = 0.215912 (0.174 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3239.0 / guessed_by_relation= 4897.0\n",
      "Calculating Recall: correct_by_relation= 3239.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.143%\n",
      "   Recall (micro): 50.147%\n",
      "       F1 (micro): 57.045%\n",
      "epoch 11: train_loss = 0.281079, dev_loss = 0.594680, dev_f1 = 0.5704\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "2018-03-08 17:41:24.632196: step 16800/150100 (epoch 12/100), loss = 0.287342 (0.140 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:42:22.082668: step 17200/150100 (epoch 12/100), loss = 0.176284 (0.139 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:43:19.273863: step 17600/150100 (epoch 12/100), loss = 0.184124 (0.107 sec/batch), lr: 0.200000\n",
      "2018-03-08 17:44:15.782151: step 18000/150100 (epoch 12/100), loss = 0.391125 (0.152 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2824.0 / guessed_by_relation= 4176.0\n",
      "Calculating Recall: correct_by_relation= 2824.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 67.625%\n",
      "   Recall (micro): 43.722%\n",
      "       F1 (micro): 53.108%\n",
      "epoch 12: train_loss = 0.268891, dev_loss = 0.638256, dev_f1 = 0.5311\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "\n",
      "2018-03-08 17:45:37.737533: step 18400/150100 (epoch 13/100), loss = 12.631238 (0.146 sec/batch), lr: 0.180000\n",
      "2018-03-08 17:46:36.099163: step 18800/150100 (epoch 13/100), loss = 10.462550 (0.127 sec/batch), lr: 0.180000\n",
      "2018-03-08 17:47:33.175312: step 19200/150100 (epoch 13/100), loss = 27.144627 (0.149 sec/batch), lr: 0.180000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 13: train_loss = 24.755071, dev_loss = 15.993539, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "\n",
      "2018-03-08 17:48:54.915706: step 19600/150100 (epoch 14/100), loss = 36.135891 (0.126 sec/batch), lr: 0.162000\n",
      "2018-03-08 17:49:52.856805: step 20000/150100 (epoch 14/100), loss = 5.951882 (0.151 sec/batch), lr: 0.162000\n",
      "2018-03-08 17:50:50.624442: step 20400/150100 (epoch 14/100), loss = 5.938647 (0.158 sec/batch), lr: 0.162000\n",
      "2018-03-08 17:51:47.303184: step 20800/150100 (epoch 14/100), loss = 3.635625 (0.146 sec/batch), lr: 0.162000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 14: train_loss = 5.709790, dev_loss = 4.632563, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "2018-03-08 17:53:08.676476: step 21200/150100 (epoch 15/100), loss = 3.707591 (0.138 sec/batch), lr: 0.145800\n",
      "2018-03-08 17:54:06.096300: step 21600/150100 (epoch 15/100), loss = 5.738865 (0.127 sec/batch), lr: 0.145800\n",
      "2018-03-08 17:55:03.844886: step 22000/150100 (epoch 15/100), loss = 2.610272 (0.123 sec/batch), lr: 0.145800\n",
      "2018-03-08 17:56:01.003905: step 22400/150100 (epoch 15/100), loss = 4.767976 (0.136 sec/batch), lr: 0.145800\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 39.0 / guessed_by_relation= 25764.0\n",
      "Calculating Recall: correct_by_relation= 39.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 0.151%\n",
      "   Recall (micro): 0.604%\n",
      "       F1 (micro): 0.242%\n",
      "epoch 15: train_loss = 2.727845, dev_loss = 5.301713, dev_f1 = 0.0024\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "2018-03-08 17:57:22.253269: step 22800/150100 (epoch 16/100), loss = 1.488197 (0.146 sec/batch), lr: 0.145800\n",
      "2018-03-08 17:58:19.999850: step 23200/150100 (epoch 16/100), loss = 1.301194 (0.148 sec/batch), lr: 0.145800\n",
      "2018-03-08 17:59:16.963349: step 23600/150100 (epoch 16/100), loss = 2.805304 (0.135 sec/batch), lr: 0.145800\n",
      "2018-03-08 18:00:14.893418: step 24000/150100 (epoch 16/100), loss = 2.862704 (0.151 sec/batch), lr: 0.145800\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 16: train_loss = 2.700997, dev_loss = 3.234163, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "\n",
      "2018-03-08 18:01:35.902869: step 24400/150100 (epoch 17/100), loss = 2.740499 (0.146 sec/batch), lr: 0.131220\n",
      "2018-03-08 18:02:33.665493: step 24800/150100 (epoch 17/100), loss = 1.527526 (0.156 sec/batch), lr: 0.131220\n",
      "2018-03-08 18:03:30.244971: step 25200/150100 (epoch 17/100), loss = 1.702477 (0.133 sec/batch), lr: 0.131220\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 17: train_loss = 2.136800, dev_loss = 2.725053, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "\n",
      "2018-03-08 18:04:51.920754: step 25600/150100 (epoch 18/100), loss = 1.609770 (0.154 sec/batch), lr: 0.118098\n",
      "2018-03-08 18:05:49.647162: step 26000/150100 (epoch 18/100), loss = 1.003526 (0.141 sec/batch), lr: 0.118098\n",
      "2018-03-08 18:06:47.179173: step 26400/150100 (epoch 18/100), loss = 4.078020 (0.128 sec/batch), lr: 0.118098\n",
      "2018-03-08 18:07:44.216868: step 26800/150100 (epoch 18/100), loss = 2.332161 (0.124 sec/batch), lr: 0.118098\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 18: train_loss = 1.850826, dev_loss = 1.833301, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "\n",
      "2018-03-08 18:09:06.245028: step 27200/150100 (epoch 19/100), loss = 2.483166 (0.139 sec/batch), lr: 0.106288\n",
      "2018-03-08 18:10:03.550436: step 27600/150100 (epoch 19/100), loss = 1.469220 (0.120 sec/batch), lr: 0.106288\n",
      "2018-03-08 18:11:00.446756: step 28000/150100 (epoch 19/100), loss = 0.850682 (0.136 sec/batch), lr: 0.106288\n",
      "2018-03-08 18:11:58.203767: step 28400/150100 (epoch 19/100), loss = 1.523341 (0.137 sec/batch), lr: 0.106288\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 19: train_loss = 1.498740, dev_loss = 1.737436, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "\n",
      "2018-03-08 18:13:20.012343: step 28800/150100 (epoch 20/100), loss = 1.195030 (0.133 sec/batch), lr: 0.095659\n",
      "2018-03-08 18:14:17.390276: step 29200/150100 (epoch 20/100), loss = 1.103195 (0.153 sec/batch), lr: 0.095659\n",
      "2018-03-08 18:15:13.780178: step 29600/150100 (epoch 20/100), loss = 1.428346 (0.116 sec/batch), lr: 0.095659\n",
      "2018-03-08 18:16:11.473619: step 30000/150100 (epoch 20/100), loss = 1.550514 (0.146 sec/batch), lr: 0.095659\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 20: train_loss = 1.388504, dev_loss = 1.778400, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "2018-03-08 18:17:32.834002: step 30400/150100 (epoch 21/100), loss = 0.853060 (0.148 sec/batch), lr: 0.086093\n",
      "2018-03-08 18:18:30.262738: step 30800/150100 (epoch 21/100), loss = 1.334134 (0.148 sec/batch), lr: 0.086093\n",
      "2018-03-08 18:19:26.777105: step 31200/150100 (epoch 21/100), loss = 0.863064 (0.157 sec/batch), lr: 0.086093\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 21: train_loss = 1.316367, dev_loss = 1.605596, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "2018-03-08 18:20:48.542567: step 31600/150100 (epoch 22/100), loss = 1.642640 (0.136 sec/batch), lr: 0.077484\n",
      "2018-03-08 18:21:45.899111: step 32000/150100 (epoch 22/100), loss = 0.859837 (0.140 sec/batch), lr: 0.077484\n",
      "2018-03-08 18:22:42.352252: step 32400/150100 (epoch 22/100), loss = 1.357655 (0.136 sec/batch), lr: 0.077484\n",
      "2018-03-08 18:23:39.980519: step 32800/150100 (epoch 22/100), loss = 1.084560 (0.133 sec/batch), lr: 0.077484\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 22: train_loss = 1.267173, dev_loss = 1.584264, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "\n",
      "2018-03-08 18:25:01.799122: step 33200/150100 (epoch 23/100), loss = 0.945822 (0.133 sec/batch), lr: 0.069736\n",
      "2018-03-08 18:25:59.373245: step 33600/150100 (epoch 23/100), loss = 0.719399 (0.153 sec/batch), lr: 0.069736\n",
      "2018-03-08 18:26:55.886546: step 34000/150100 (epoch 23/100), loss = 1.558511 (0.148 sec/batch), lr: 0.069736\n",
      "2018-03-08 18:27:53.392487: step 34400/150100 (epoch 23/100), loss = 0.985804 (0.150 sec/batch), lr: 0.069736\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 23: train_loss = 1.229407, dev_loss = 1.539763, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "\n",
      "2018-03-08 18:29:15.064166: step 34800/150100 (epoch 24/100), loss = 2.674623 (0.152 sec/batch), lr: 0.062762\n",
      "2018-03-08 18:30:12.440045: step 35200/150100 (epoch 24/100), loss = 0.979357 (0.143 sec/batch), lr: 0.062762\n",
      "2018-03-08 18:31:08.870124: step 35600/150100 (epoch 24/100), loss = 1.156574 (0.143 sec/batch), lr: 0.062762\n",
      "2018-03-08 18:32:06.721986: step 36000/150100 (epoch 24/100), loss = 0.823420 (0.143 sec/batch), lr: 0.062762\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 24: train_loss = 1.211697, dev_loss = 1.536316, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "\n",
      "2018-03-08 18:33:27.953026: step 36400/150100 (epoch 25/100), loss = 0.870601 (0.145 sec/batch), lr: 0.056486\n",
      "2018-03-08 18:34:24.370020: step 36800/150100 (epoch 25/100), loss = 1.951708 (0.146 sec/batch), lr: 0.056486\n",
      "2018-03-08 18:35:21.442809: step 37200/150100 (epoch 25/100), loss = 1.440660 (0.155 sec/batch), lr: 0.056486\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 25: train_loss = 1.197080, dev_loss = 1.519059, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "2018-03-08 18:36:42.847834: step 37600/150100 (epoch 26/100), loss = 1.474645 (0.138 sec/batch), lr: 0.050837\n",
      "2018-03-08 18:37:40.157253: step 38000/150100 (epoch 26/100), loss = 0.950709 (0.135 sec/batch), lr: 0.050837\n",
      "2018-03-08 18:38:36.424901: step 38400/150100 (epoch 26/100), loss = 0.904749 (0.147 sec/batch), lr: 0.050837\n",
      "2018-03-08 18:39:33.725296: step 38800/150100 (epoch 26/100), loss = 1.397326 (0.151 sec/batch), lr: 0.050837\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 26: train_loss = 1.183040, dev_loss = 1.503690, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "\n",
      "2018-03-08 18:40:55.503696: step 39200/150100 (epoch 27/100), loss = 0.858181 (0.153 sec/batch), lr: 0.045754\n",
      "2018-03-08 18:41:53.029690: step 39600/150100 (epoch 27/100), loss = 2.066067 (0.141 sec/batch), lr: 0.045754\n",
      "2018-03-08 18:42:49.378554: step 40000/150100 (epoch 27/100), loss = 1.643265 (0.133 sec/batch), lr: 0.045754\n",
      "2018-03-08 18:43:46.967717: step 40400/150100 (epoch 27/100), loss = 1.144320 (0.142 sec/batch), lr: 0.045754\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 27: train_loss = 1.170782, dev_loss = 1.483003, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "2018-03-08 18:45:08.766266: step 40800/150100 (epoch 28/100), loss = 1.102840 (0.137 sec/batch), lr: 0.041178\n",
      "2018-03-08 18:46:04.861455: step 41200/150100 (epoch 28/100), loss = 1.181245 (0.148 sec/batch), lr: 0.041178\n",
      "2018-03-08 18:47:02.507770: step 41600/150100 (epoch 28/100), loss = 0.741285 (0.143 sec/batch), lr: 0.041178\n",
      "2018-03-08 18:48:00.225274: step 42000/150100 (epoch 28/100), loss = 1.304105 (0.149 sec/batch), lr: 0.041178\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 28: train_loss = 1.161086, dev_loss = 1.481132, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "\n",
      "2018-03-08 18:49:21.703888: step 42400/150100 (epoch 29/100), loss = 1.346619 (0.144 sec/batch), lr: 0.037060\n",
      "2018-03-08 18:50:18.348539: step 42800/150100 (epoch 29/100), loss = 1.258686 (0.136 sec/batch), lr: 0.037060\n",
      "2018-03-08 18:51:16.015909: step 43200/150100 (epoch 29/100), loss = 0.864745 (0.149 sec/batch), lr: 0.037060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 29: train_loss = 1.152428, dev_loss = 1.472487, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "\n",
      "2018-03-08 18:52:37.507836: step 43600/150100 (epoch 30/100), loss = 1.046030 (0.129 sec/batch), lr: 0.033354\n",
      "2018-03-08 18:53:34.906077: step 44000/150100 (epoch 30/100), loss = 1.047357 (0.129 sec/batch), lr: 0.033354\n",
      "2018-03-08 18:54:31.311147: step 44400/150100 (epoch 30/100), loss = 1.269440 (0.141 sec/batch), lr: 0.033354\n",
      "2018-03-08 18:55:28.594497: step 44800/150100 (epoch 30/100), loss = 1.561453 (0.150 sec/batch), lr: 0.033354\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 30: train_loss = 1.144936, dev_loss = 1.463022, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "2018-03-08 18:56:49.935830: step 45200/150100 (epoch 31/100), loss = 1.324143 (0.144 sec/batch), lr: 0.030019\n",
      "2018-03-08 18:57:46.402106: step 45600/150100 (epoch 31/100), loss = 1.127649 (0.143 sec/batch), lr: 0.030019\n",
      "2018-03-08 18:58:43.934117: step 46000/150100 (epoch 31/100), loss = 0.874805 (0.158 sec/batch), lr: 0.030019\n",
      "2018-03-08 18:59:41.552357: step 46400/150100 (epoch 31/100), loss = 1.162254 (0.142 sec/batch), lr: 0.030019\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 31: train_loss = 1.139505, dev_loss = 1.469579, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "\n",
      "2018-03-08 19:01:03.551439: step 46800/150100 (epoch 32/100), loss = 0.941853 (0.153 sec/batch), lr: 0.027017\n",
      "2018-03-08 19:01:59.797029: step 47200/150100 (epoch 32/100), loss = 1.623457 (0.157 sec/batch), lr: 0.027017\n",
      "2018-03-08 19:02:57.282917: step 47600/150100 (epoch 32/100), loss = 0.863359 (0.155 sec/batch), lr: 0.027017\n",
      "2018-03-08 19:03:55.091663: step 48000/150100 (epoch 32/100), loss = 1.214994 (0.161 sec/batch), lr: 0.027017\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 32: train_loss = 1.133613, dev_loss = 1.455932, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "\n",
      "2018-03-08 19:05:16.614479: step 48400/150100 (epoch 33/100), loss = 1.529584 (0.120 sec/batch), lr: 0.024315\n",
      "2018-03-08 19:06:13.056592: step 48800/150100 (epoch 33/100), loss = 1.214683 (0.145 sec/batch), lr: 0.024315\n",
      "2018-03-08 19:07:10.632719: step 49200/150100 (epoch 33/100), loss = 1.025340 (0.156 sec/batch), lr: 0.024315\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 33: train_loss = 1.128971, dev_loss = 1.454869, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "\n",
      "2018-03-08 19:08:32.557605: step 49600/150100 (epoch 34/100), loss = 1.482973 (0.123 sec/batch), lr: 0.021884\n",
      "2018-03-08 19:09:28.848314: step 50000/150100 (epoch 34/100), loss = 0.779402 (0.135 sec/batch), lr: 0.021884\n",
      "2018-03-08 19:10:26.463546: step 50400/150100 (epoch 34/100), loss = 1.041975 (0.125 sec/batch), lr: 0.021884\n",
      "2018-03-08 19:11:24.063738: step 50800/150100 (epoch 34/100), loss = 0.994575 (0.148 sec/batch), lr: 0.021884\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 34: train_loss = 1.124893, dev_loss = 1.453415, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "2018-03-08 19:12:45.792101: step 51200/150100 (epoch 35/100), loss = 1.428714 (0.160 sec/batch), lr: 0.019695\n",
      "2018-03-08 19:13:42.154000: step 51600/150100 (epoch 35/100), loss = 1.062410 (0.155 sec/batch), lr: 0.019695\n",
      "2018-03-08 19:14:39.712079: step 52000/150100 (epoch 35/100), loss = 1.186528 (0.125 sec/batch), lr: 0.019695\n",
      "2018-03-08 19:15:37.372432: step 52400/150100 (epoch 35/100), loss = 0.925160 (0.142 sec/batch), lr: 0.019695\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 35: train_loss = 1.121199, dev_loss = 1.450340, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "\n",
      "2018-03-08 19:16:59.424655: step 52800/150100 (epoch 36/100), loss = 1.257040 (0.127 sec/batch), lr: 0.017726\n",
      "2018-03-08 19:17:56.090775: step 53200/150100 (epoch 36/100), loss = 0.800612 (0.142 sec/batch), lr: 0.017726\n",
      "2018-03-08 19:18:53.750125: step 53600/150100 (epoch 36/100), loss = 1.165896 (0.121 sec/batch), lr: 0.017726\n",
      "2018-03-08 19:19:51.194784: step 54000/150100 (epoch 36/100), loss = 0.961287 (0.132 sec/batch), lr: 0.017726\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 36: train_loss = 1.117392, dev_loss = 1.451089, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "2018-03-08 19:21:12.279914: step 54400/150100 (epoch 37/100), loss = 0.790687 (0.141 sec/batch), lr: 0.015953\n",
      "2018-03-08 19:22:10.044543: step 54800/150100 (epoch 37/100), loss = 0.909947 (0.126 sec/batch), lr: 0.015953\n",
      "2018-03-08 19:23:07.675818: step 55200/150100 (epoch 37/100), loss = 1.527652 (0.137 sec/batch), lr: 0.015953\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 37: train_loss = 1.114202, dev_loss = 1.449906, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "2018-03-08 19:24:29.742100: step 55600/150100 (epoch 38/100), loss = 1.139159 (0.131 sec/batch), lr: 0.014358\n",
      "2018-03-08 19:25:26.795839: step 56000/150100 (epoch 38/100), loss = 0.711064 (0.133 sec/batch), lr: 0.014358\n",
      "2018-03-08 19:26:24.565481: step 56400/150100 (epoch 38/100), loss = 1.535399 (0.143 sec/batch), lr: 0.014358\n",
      "2018-03-08 19:27:22.052372: step 56800/150100 (epoch 38/100), loss = 0.616819 (0.151 sec/batch), lr: 0.014358\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 38: train_loss = 1.111278, dev_loss = 1.452680, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "2018-03-08 19:28:43.826839: step 57200/150100 (epoch 39/100), loss = 0.842594 (0.144 sec/batch), lr: 0.012922\n",
      "2018-03-08 19:29:40.774294: step 57600/150100 (epoch 39/100), loss = 1.266694 (0.146 sec/batch), lr: 0.012922\n",
      "2018-03-08 19:30:38.442668: step 58000/150100 (epoch 39/100), loss = 1.644352 (0.148 sec/batch), lr: 0.012922\n",
      "2018-03-08 19:31:35.982750: step 58400/150100 (epoch 39/100), loss = 1.602415 (0.139 sec/batch), lr: 0.012922\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 39: train_loss = 1.109174, dev_loss = 1.454253, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "2018-03-08 19:32:57.486516: step 58800/150100 (epoch 40/100), loss = 1.148920 (0.140 sec/batch), lr: 0.011630\n",
      "2018-03-08 19:33:54.916254: step 59200/150100 (epoch 40/100), loss = 1.299148 (0.128 sec/batch), lr: 0.011630\n",
      "2018-03-08 19:34:52.704947: step 59600/150100 (epoch 40/100), loss = 1.307259 (0.131 sec/batch), lr: 0.011630\n",
      "2018-03-08 19:35:50.070530: step 60000/150100 (epoch 40/100), loss = 1.576116 (0.149 sec/batch), lr: 0.011630\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 0.0 / guessed_by_relation= 0.0\n",
      "Calculating Recall: correct_by_relation= 0.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 100.000%\n",
      "   Recall (micro): 0.000%\n",
      "       F1 (micro): 0.000%\n",
      "epoch 40: train_loss = 1.107294, dev_loss = 1.452713, dev_f1 = 0.0000\n",
      "model saved to ./saved_models/20_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "2018-03-08 19:37:11.018818: step 60400/150100 (epoch 41/100), loss = 0.917112 (0.141 sec/batch), lr: 0.010467\n",
      "2018-03-08 19:38:08.546253: step 60800/150100 (epoch 41/100), loss = 1.013036 (0.153 sec/batch), lr: 0.010467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6bc31db1d5bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    351\u001b[0m             outputs, enc_slf_attn = self.self_attention_encoder(\n\u001b[0;32m    352\u001b[0m                 \u001b[0menc_non_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                 \u001b[0msrc_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_self\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minst_position\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m             )\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\Models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, enc_non_embedded, src_seq, src_pos)\u001b[0m\n\u001b[0;32m    111\u001b[0m             enc_output, enc_slf_attn = enc_layer(\n\u001b[0;32m    112\u001b[0m                 \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                 \u001b[0mslf_attn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_slf_attn_mask\u001b[0m  \u001b[1;31m# enc_slf_attn_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             )\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\Layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, enc_input, slf_attn_mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# do feed forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0menc_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_ffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_slf_attn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\SubLayers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, residual)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias + dropout 4 + adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
