{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurrent params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\\n weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
    " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of lstm layers.')\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=3, help='Num of self-attention encoders.')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='Input and attn dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')        # 0.1 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=2.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=1.0, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "parser.add_argument('--obj_sub_pos', dest='obj_sub_pos', action='store_true', \n",
    "    help='In self-attention add obj/subg positional vectors.', default=True)\n",
    "parser.add_argument('--use_batch_norm', dest='use_batch_norm', action='store_true', \n",
    "    help='BatchNorm if True, else LayerNorm in self-attention.', default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=1, help='Number of self-attention heads.')\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.3, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=10, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='39_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Directory ./saved_models/39_self_attention_dropout do not exist; creating...\n",
      "Config saved to file ./saved_models/39_self_attention_dropout/config.json\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tnum_layers_encoder : 3\n",
      "\tdropout : 0.1\n",
      "\tscaled_dropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 2.0\n",
      "\tweight_rest : 1.0\n",
      "\tself_att : True\n",
      "\tn_head : 1\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.3\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 1.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 10\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 39_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/39_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  1\n",
      "d_v and d_k:  360.0\n",
      "Finetune all embeddings.\n",
      "Using weights [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-22 07:34:54.850530: step 400/300200 (epoch 1/200), loss = 0.334650 (0.133 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:36:24.061935: step 800/300200 (epoch 1/200), loss = 0.404965 (0.236 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:38:00.151420: step 1200/300200 (epoch 1/200), loss = 0.340535 (0.190 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2832.0 / guessed_by_relation= 5033.0\n",
      "Calculating Recall: correct_by_relation= 2832.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.269%\n",
      "   Recall (micro): 43.846%\n",
      "       F1 (micro): 49.286%\n",
      "epoch 1: train_loss = 0.438759, dev_loss = 0.470341, dev_f1 = 0.4929\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 07:39:46.320653: step 1600/300200 (epoch 2/200), loss = 0.254359 (0.189 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:40:59.901391: step 2000/300200 (epoch 2/200), loss = 0.401434 (0.174 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:42:13.086063: step 2400/300200 (epoch 2/200), loss = 0.406155 (0.197 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:43:25.686191: step 2800/300200 (epoch 2/200), loss = 0.441240 (0.165 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2920.0 / guessed_by_relation= 4589.0\n",
      "Calculating Recall: correct_by_relation= 2920.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.630%\n",
      "   Recall (micro): 45.208%\n",
      "       F1 (micro): 52.860%\n",
      "epoch 2: train_loss = 0.331207, dev_loss = 0.399209, dev_f1 = 0.5286\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 07:45:02.171393: step 3200/300200 (epoch 3/200), loss = 0.334025 (0.167 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:46:12.519930: step 3600/300200 (epoch 3/200), loss = 0.243798 (0.158 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:47:24.888482: step 4000/300200 (epoch 3/200), loss = 0.226611 (0.171 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:48:37.143826: step 4400/300200 (epoch 3/200), loss = 0.262072 (0.209 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3191.0 / guessed_by_relation= 4829.0\n",
      "Calculating Recall: correct_by_relation= 3191.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.080%\n",
      "   Recall (micro): 49.404%\n",
      "       F1 (micro): 56.538%\n",
      "epoch 3: train_loss = 0.307013, dev_loss = 0.390484, dev_f1 = 0.5654\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 07:50:13.104989: step 4800/300200 (epoch 4/200), loss = 0.208053 (0.165 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:51:23.265281: step 5200/300200 (epoch 4/200), loss = 0.452700 (0.163 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:52:35.954647: step 5600/300200 (epoch 4/200), loss = 0.490940 (0.194 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:53:48.212867: step 6000/300200 (epoch 4/200), loss = 0.180607 (0.175 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3449.0 / guessed_by_relation= 5558.0\n",
      "Calculating Recall: correct_by_relation= 3449.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.055%\n",
      "   Recall (micro): 53.398%\n",
      "       F1 (micro): 57.402%\n",
      "epoch 4: train_loss = 0.294166, dev_loss = 0.412033, dev_f1 = 0.5740\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 07:55:24.118485: step 6400/300200 (epoch 5/200), loss = 0.311834 (0.197 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:56:34.887744: step 6800/300200 (epoch 5/200), loss = 0.262421 (0.210 sec/batch), lr: 0.300000\n",
      "2018-03-22 07:57:46.906905: step 7200/300200 (epoch 5/200), loss = 0.272220 (0.169 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3337.0 / guessed_by_relation= 4993.0\n",
      "Calculating Recall: correct_by_relation= 3337.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.834%\n",
      "   Recall (micro): 51.664%\n",
      "       F1 (micro): 58.278%\n",
      "epoch 5: train_loss = 0.284383, dev_loss = 0.378477, dev_f1 = 0.5828\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 07:59:22.882216: step 7600/300200 (epoch 6/200), loss = 0.263208 (0.174 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:00:34.891068: step 8000/300200 (epoch 6/200), loss = 0.186223 (0.172 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:01:45.894596: step 8400/300200 (epoch 6/200), loss = 0.296225 (0.220 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:02:59.699751: step 8800/300200 (epoch 6/200), loss = 0.304230 (0.130 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3693.0 / guessed_by_relation= 6097.0\n",
      "Calculating Recall: correct_by_relation= 3693.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.571%\n",
      "   Recall (micro): 57.176%\n",
      "       F1 (micro): 58.824%\n",
      "epoch 6: train_loss = 0.275046, dev_loss = 0.417560, dev_f1 = 0.5882\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:04:33.009692: step 9200/300200 (epoch 7/200), loss = 0.400270 (0.164 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:05:40.764136: step 9600/300200 (epoch 7/200), loss = 0.139694 (0.149 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:06:48.803131: step 10000/300200 (epoch 7/200), loss = 0.154583 (0.160 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:07:57.822357: step 10400/300200 (epoch 7/200), loss = 0.117767 (0.159 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3859.0 / guessed_by_relation= 6405.0\n",
      "Calculating Recall: correct_by_relation= 3859.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.250%\n",
      "   Recall (micro): 59.746%\n",
      "       F1 (micro): 59.997%\n",
      "epoch 7: train_loss = 0.268883, dev_loss = 0.423025, dev_f1 = 0.6000\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:09:30.153758: step 10800/300200 (epoch 8/200), loss = 0.234132 (0.159 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:10:36.327243: step 11200/300200 (epoch 8/200), loss = 0.392367 (0.176 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:11:44.830473: step 11600/300200 (epoch 8/200), loss = 0.230633 (0.182 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:12:53.802952: step 12000/300200 (epoch 8/200), loss = 0.185939 (0.150 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3701.0 / guessed_by_relation= 5886.0\n",
      "Calculating Recall: correct_by_relation= 3701.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.878%\n",
      "   Recall (micro): 57.300%\n",
      "       F1 (micro): 59.959%\n",
      "epoch 8: train_loss = 0.262090, dev_loss = 0.398660, dev_f1 = 0.5996\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:14:25.496874: step 12400/300200 (epoch 9/200), loss = 0.193717 (0.202 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:15:32.365595: step 12800/300200 (epoch 9/200), loss = 0.171812 (0.180 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:16:40.746625: step 13200/300200 (epoch 9/200), loss = 0.327979 (0.168 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3663.0 / guessed_by_relation= 5869.0\n",
      "Calculating Recall: correct_by_relation= 3663.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.413%\n",
      "   Recall (micro): 56.712%\n",
      "       F1 (micro): 59.426%\n",
      "epoch 9: train_loss = 0.258242, dev_loss = 0.408048, dev_f1 = 0.5943\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:18:12.925677: step 13600/300200 (epoch 10/200), loss = 0.264538 (0.178 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:19:21.351703: step 14000/300200 (epoch 10/200), loss = 0.294172 (0.176 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:20:27.630014: step 14400/300200 (epoch 10/200), loss = 0.157089 (0.171 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:21:36.463122: step 14800/300200 (epoch 10/200), loss = 0.289229 (0.144 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3803.0 / guessed_by_relation= 5914.0\n",
      "Calculating Recall: correct_by_relation= 3803.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.305%\n",
      "   Recall (micro): 58.879%\n",
      "       F1 (micro): 61.473%\n",
      "epoch 10: train_loss = 0.252960, dev_loss = 0.391808, dev_f1 = 0.6147\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:23:09.006303: step 15200/300200 (epoch 11/200), loss = 0.366066 (0.145 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:24:17.225779: step 15600/300200 (epoch 11/200), loss = 0.310300 (0.198 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:25:23.115056: step 16000/300200 (epoch 11/200), loss = 0.265917 (0.165 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:26:32.033391: step 16400/300200 (epoch 11/200), loss = 0.145241 (0.162 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3516.0 / guessed_by_relation= 5106.0\n",
      "Calculating Recall: correct_by_relation= 3516.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.860%\n",
      "   Recall (micro): 54.436%\n",
      "       F1 (micro): 60.804%\n",
      "epoch 11: train_loss = 0.248486, dev_loss = 0.368230, dev_f1 = 0.6080\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:28:04.172497: step 16800/300200 (epoch 12/200), loss = 0.417032 (0.169 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:29:12.066105: step 17200/300200 (epoch 12/200), loss = 0.191067 (0.133 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:30:19.133516: step 17600/300200 (epoch 12/200), loss = 0.187783 (0.161 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:31:28.247371: step 18000/300200 (epoch 12/200), loss = 0.230644 (0.175 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3649.0 / guessed_by_relation= 5435.0\n",
      "Calculating Recall: correct_by_relation= 3649.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 67.139%\n",
      "   Recall (micro): 56.495%\n",
      "       F1 (micro): 61.359%\n",
      "epoch 12: train_loss = 0.244599, dev_loss = 0.373552, dev_f1 = 0.6136\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:32:59.645506: step 18400/300200 (epoch 13/200), loss = 0.395484 (0.191 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:34:06.866326: step 18800/300200 (epoch 13/200), loss = 0.183252 (0.141 sec/batch), lr: 0.300000\n",
      "2018-03-22 08:35:14.637609: step 19200/300200 (epoch 13/200), loss = 0.193459 (0.180 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3698.0 / guessed_by_relation= 5728.0\n",
      "Calculating Recall: correct_by_relation= 3698.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.560%\n",
      "   Recall (micro): 57.253%\n",
      "       F1 (micro): 60.688%\n",
      "epoch 13: train_loss = 0.239496, dev_loss = 0.387509, dev_f1 = 0.6069\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:36:47.100576: step 19600/300200 (epoch 14/200), loss = 0.171939 (0.158 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:37:55.360159: step 20000/300200 (epoch 14/200), loss = 0.185339 (0.175 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:39:01.354716: step 20400/300200 (epoch 14/200), loss = 0.289746 (0.177 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:40:09.906074: step 20800/300200 (epoch 14/200), loss = 0.189249 (0.169 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3672.0 / guessed_by_relation= 5581.0\n",
      "Calculating Recall: correct_by_relation= 3672.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.795%\n",
      "   Recall (micro): 56.851%\n",
      "       F1 (micro): 60.997%\n",
      "epoch 14: train_loss = 0.233933, dev_loss = 0.381563, dev_f1 = 0.6100\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:41:42.263762: step 21200/300200 (epoch 15/200), loss = 0.355326 (0.172 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:42:50.071141: step 21600/300200 (epoch 15/200), loss = 0.207082 (0.155 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:43:56.469773: step 22000/300200 (epoch 15/200), loss = 0.233956 (0.151 sec/batch), lr: 0.285000\n",
      "2018-03-22 08:45:05.390113: step 22400/300200 (epoch 15/200), loss = 0.271023 (0.189 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3572.0 / guessed_by_relation= 5277.0\n",
      "Calculating Recall: correct_by_relation= 3572.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 67.690%\n",
      "   Recall (micro): 55.303%\n",
      "       F1 (micro): 60.873%\n",
      "epoch 15: train_loss = 0.231371, dev_loss = 0.370194, dev_f1 = 0.6087\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:46:36.852419: step 22800/300200 (epoch 16/200), loss = 0.196517 (0.149 sec/batch), lr: 0.270750\n",
      "2018-03-22 08:47:44.851307: step 23200/300200 (epoch 16/200), loss = 0.278352 (0.156 sec/batch), lr: 0.270750\n",
      "2018-03-22 08:48:50.501950: step 23600/300200 (epoch 16/200), loss = 0.200989 (0.186 sec/batch), lr: 0.270750\n",
      "2018-03-22 08:49:59.232786: step 24000/300200 (epoch 16/200), loss = 0.193143 (0.174 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3809.0 / guessed_by_relation= 5801.0\n",
      "Calculating Recall: correct_by_relation= 3809.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.661%\n",
      "   Recall (micro): 58.972%\n",
      "       F1 (micro): 62.137%\n",
      "epoch 16: train_loss = 0.226605, dev_loss = 0.380740, dev_f1 = 0.6214\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:51:31.360862: step 24400/300200 (epoch 17/200), loss = 0.155664 (0.172 sec/batch), lr: 0.270750\n",
      "2018-03-22 08:52:39.975389: step 24800/300200 (epoch 17/200), loss = 0.286691 (0.164 sec/batch), lr: 0.270750\n",
      "2018-03-22 08:53:46.679834: step 25200/300200 (epoch 17/200), loss = 0.245045 (0.152 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3528.0 / guessed_by_relation= 5060.0\n",
      "Calculating Recall: correct_by_relation= 3528.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 69.723%\n",
      "   Recall (micro): 54.621%\n",
      "       F1 (micro): 61.255%\n",
      "epoch 17: train_loss = 0.222448, dev_loss = 0.356903, dev_f1 = 0.6126\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 08:55:19.299218: step 25600/300200 (epoch 18/200), loss = 0.217331 (0.191 sec/batch), lr: 0.257212\n",
      "2018-03-22 08:56:27.547771: step 26000/300200 (epoch 18/200), loss = 0.145039 (0.141 sec/batch), lr: 0.257212\n",
      "2018-03-22 08:57:35.801337: step 26400/300200 (epoch 18/200), loss = 0.167967 (0.158 sec/batch), lr: 0.257212\n",
      "2018-03-22 08:58:42.767479: step 26800/300200 (epoch 18/200), loss = 0.331991 (0.166 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3528.0 / guessed_by_relation= 5049.0\n",
      "Calculating Recall: correct_by_relation= 3528.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 69.875%\n",
      "   Recall (micro): 54.621%\n",
      "       F1 (micro): 61.314%\n",
      "epoch 18: train_loss = 0.217755, dev_loss = 0.361201, dev_f1 = 0.6131\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:00:14.769219: step 27200/300200 (epoch 19/200), loss = 0.384487 (0.183 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:01:23.154136: step 27600/300200 (epoch 19/200), loss = 0.082890 (0.144 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:02:30.832171: step 28000/300200 (epoch 19/200), loss = 0.168950 (0.144 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:03:38.487145: step 28400/300200 (epoch 19/200), loss = 0.267156 (0.170 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3644.0 / guessed_by_relation= 5356.0\n",
      "Calculating Recall: correct_by_relation= 3644.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.036%\n",
      "   Recall (micro): 56.417%\n",
      "       F1 (micro): 61.684%\n",
      "epoch 19: train_loss = 0.213013, dev_loss = 0.371642, dev_f1 = 0.6168\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:05:10.557066: step 28800/300200 (epoch 20/200), loss = 0.314548 (0.145 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:06:19.069321: step 29200/300200 (epoch 20/200), loss = 0.157588 (0.157 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:07:25.475974: step 29600/300200 (epoch 20/200), loss = 0.166949 (0.131 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:08:34.421381: step 30000/300200 (epoch 20/200), loss = 0.176141 (0.164 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3675.0 / guessed_by_relation= 5397.0\n",
      "Calculating Recall: correct_by_relation= 3675.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.093%\n",
      "   Recall (micro): 56.897%\n",
      "       F1 (micro): 61.994%\n",
      "epoch 20: train_loss = 0.209458, dev_loss = 0.376463, dev_f1 = 0.6199\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:10:05.371324: step 30400/300200 (epoch 21/200), loss = 0.110634 (0.175 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:11:13.934714: step 30800/300200 (epoch 21/200), loss = 0.260250 (0.189 sec/batch), lr: 0.257212\n",
      "2018-03-22 09:12:20.714360: step 31200/300200 (epoch 21/200), loss = 0.124347 (0.171 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3504.0 / guessed_by_relation= 5120.0\n",
      "Calculating Recall: correct_by_relation= 3504.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.438%\n",
      "   Recall (micro): 54.250%\n",
      "       F1 (micro): 60.523%\n",
      "epoch 21: train_loss = 0.204666, dev_loss = 0.375627, dev_f1 = 0.6052\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:13:52.662959: step 31600/300200 (epoch 22/200), loss = 0.185627 (0.150 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:15:01.430894: step 32000/300200 (epoch 22/200), loss = 0.106287 (0.183 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:16:10.783383: step 32400/300200 (epoch 22/200), loss = 0.143886 (0.157 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:17:17.370516: step 32800/300200 (epoch 22/200), loss = 0.148170 (0.171 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3706.0 / guessed_by_relation= 5553.0\n",
      "Calculating Recall: correct_by_relation= 3706.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.739%\n",
      "   Recall (micro): 57.377%\n",
      "       F1 (micro): 61.705%\n",
      "epoch 22: train_loss = 0.201442, dev_loss = 0.385332, dev_f1 = 0.6170\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:18:49.564769: step 33200/300200 (epoch 23/200), loss = 0.149009 (0.166 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:19:58.393867: step 33600/300200 (epoch 23/200), loss = 0.085384 (0.171 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:21:07.167817: step 34000/300200 (epoch 23/200), loss = 0.214393 (0.179 sec/batch), lr: 0.244352\n",
      "2018-03-22 09:22:13.821127: step 34400/300200 (epoch 23/200), loss = 0.101960 (0.190 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3329.0 / guessed_by_relation= 4764.0\n",
      "Calculating Recall: correct_by_relation= 3329.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 69.878%\n",
      "   Recall (micro): 51.540%\n",
      "       F1 (micro): 59.325%\n",
      "epoch 23: train_loss = 0.195642, dev_loss = 0.380807, dev_f1 = 0.5932\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:23:46.810495: step 34800/300200 (epoch 24/200), loss = 0.507653 (0.169 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:24:55.185384: step 35200/300200 (epoch 24/200), loss = 0.180071 (0.161 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:26:03.803922: step 35600/300200 (epoch 24/200), loss = 0.117788 (0.145 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:27:10.954553: step 36000/300200 (epoch 24/200), loss = 0.207982 (0.185 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3642.0 / guessed_by_relation= 5487.0\n",
      "Calculating Recall: correct_by_relation= 3642.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.375%\n",
      "   Recall (micro): 56.386%\n",
      "       F1 (micro): 60.974%\n",
      "epoch 24: train_loss = 0.191465, dev_loss = 0.404067, dev_f1 = 0.6097\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:28:43.345329: step 36400/300200 (epoch 25/200), loss = 0.152836 (0.167 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:29:51.938799: step 36800/300200 (epoch 25/200), loss = 0.138369 (0.163 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:30:58.901933: step 37200/300200 (epoch 25/200), loss = 0.125003 (0.167 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3521.0 / guessed_by_relation= 5072.0\n",
      "Calculating Recall: correct_by_relation= 3521.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 69.420%\n",
      "   Recall (micro): 54.513%\n",
      "       F1 (micro): 61.070%\n",
      "epoch 25: train_loss = 0.185375, dev_loss = 0.384946, dev_f1 = 0.6107\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:32:31.429071: step 37600/300200 (epoch 26/200), loss = 0.191266 (0.144 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:33:40.752483: step 38000/300200 (epoch 26/200), loss = 0.103398 (0.143 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:34:49.578573: step 38400/300200 (epoch 26/200), loss = 0.106489 (0.173 sec/batch), lr: 0.232134\n",
      "2018-03-22 09:35:56.460490: step 38800/300200 (epoch 26/200), loss = 0.187716 (0.173 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3484.0 / guessed_by_relation= 5119.0\n",
      "Calculating Recall: correct_by_relation= 3484.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.060%\n",
      "   Recall (micro): 53.940%\n",
      "       F1 (micro): 60.183%\n",
      "epoch 26: train_loss = 0.180275, dev_loss = 0.414661, dev_f1 = 0.6018\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:37:30.133677: step 39200/300200 (epoch 27/200), loss = 0.077125 (0.168 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:38:39.376876: step 39600/300200 (epoch 27/200), loss = 0.431140 (0.183 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:39:48.172886: step 40000/300200 (epoch 27/200), loss = 0.163031 (0.166 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:40:55.326526: step 40400/300200 (epoch 27/200), loss = 0.181620 (0.179 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3562.0 / guessed_by_relation= 5336.0\n",
      "Calculating Recall: correct_by_relation= 3562.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.754%\n",
      "   Recall (micro): 55.148%\n",
      "       F1 (micro): 60.398%\n",
      "epoch 27: train_loss = 0.174003, dev_loss = 0.421319, dev_f1 = 0.6040\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:42:28.368033: step 40800/300200 (epoch 28/200), loss = 0.153329 (0.154 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:43:37.406687: step 41200/300200 (epoch 28/200), loss = 0.179822 (0.177 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:44:46.842398: step 41600/300200 (epoch 28/200), loss = 0.082557 (0.195 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:45:54.096305: step 42000/300200 (epoch 28/200), loss = 0.322675 (0.181 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3740.0 / guessed_by_relation= 5871.0\n",
      "Calculating Recall: correct_by_relation= 3740.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.703%\n",
      "   Recall (micro): 57.904%\n",
      "       F1 (micro): 60.665%\n",
      "epoch 28: train_loss = 0.171341, dev_loss = 0.460102, dev_f1 = 0.6067\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:47:27.263145: step 42400/300200 (epoch 29/200), loss = 0.200122 (0.155 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:48:36.291774: step 42800/300200 (epoch 29/200), loss = 0.178012 (0.170 sec/batch), lr: 0.220528\n",
      "2018-03-22 09:49:45.702418: step 43200/300200 (epoch 29/200), loss = 0.065310 (0.160 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3587.0 / guessed_by_relation= 5388.0\n",
      "Calculating Recall: correct_by_relation= 3587.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.574%\n",
      "   Recall (micro): 55.535%\n",
      "       F1 (micro): 60.555%\n",
      "epoch 29: train_loss = 0.167163, dev_loss = 0.444358, dev_f1 = 0.6056\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:51:16.937118: step 43600/300200 (epoch 30/200), loss = 0.067864 (0.127 sec/batch), lr: 0.209501\n",
      "2018-03-22 09:52:26.765875: step 44000/300200 (epoch 30/200), loss = 0.245281 (0.173 sec/batch), lr: 0.209501\n",
      "2018-03-22 09:53:35.539825: step 44400/300200 (epoch 30/200), loss = 0.160311 (0.158 sec/batch), lr: 0.209501\n",
      "2018-03-22 09:54:44.131291: step 44800/300200 (epoch 30/200), loss = 0.140730 (0.181 sec/batch), lr: 0.209501\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3598.0 / guessed_by_relation= 5481.0\n",
      "Calculating Recall: correct_by_relation= 3598.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.645%\n",
      "   Recall (micro): 55.705%\n",
      "       F1 (micro): 60.268%\n",
      "epoch 30: train_loss = 0.160682, dev_loss = 0.442725, dev_f1 = 0.6027\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 09:56:15.286780: step 45200/300200 (epoch 31/200), loss = 0.112160 (0.172 sec/batch), lr: 0.199026\n",
      "2018-03-22 09:57:24.540006: step 45600/300200 (epoch 31/200), loss = 0.203787 (0.179 sec/batch), lr: 0.199026\n",
      "2018-03-22 09:58:34.103055: step 46000/300200 (epoch 31/200), loss = 0.173960 (0.173 sec/batch), lr: 0.199026\n",
      "2018-03-22 09:59:42.016718: step 46400/300200 (epoch 31/200), loss = 0.096361 (0.163 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3637.0 / guessed_by_relation= 5514.0\n",
      "Calculating Recall: correct_by_relation= 3637.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.959%\n",
      "   Recall (micro): 56.309%\n",
      "       F1 (micro): 60.753%\n",
      "epoch 31: train_loss = 0.154356, dev_loss = 0.453328, dev_f1 = 0.6075\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:01:13.920197: step 46800/300200 (epoch 32/200), loss = 0.114667 (0.176 sec/batch), lr: 0.199026\n",
      "2018-03-22 10:02:22.785390: step 47200/300200 (epoch 32/200), loss = 0.173927 (0.177 sec/batch), lr: 0.199026\n",
      "2018-03-22 10:03:31.775917: step 47600/300200 (epoch 32/200), loss = 0.134687 (0.169 sec/batch), lr: 0.199026\n",
      "2018-03-22 10:04:38.793195: step 48000/300200 (epoch 32/200), loss = 0.039824 (0.179 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3559.0 / guessed_by_relation= 5381.0\n",
      "Calculating Recall: correct_by_relation= 3559.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.140%\n",
      "   Recall (micro): 55.101%\n",
      "       F1 (micro): 60.118%\n",
      "epoch 32: train_loss = 0.149565, dev_loss = 0.478046, dev_f1 = 0.6012\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:06:11.067662: step 48400/300200 (epoch 33/200), loss = 0.228848 (0.177 sec/batch), lr: 0.189075\n",
      "2018-03-22 10:07:20.696887: step 48800/300200 (epoch 33/200), loss = 0.321954 (0.177 sec/batch), lr: 0.189075\n",
      "2018-03-22 10:08:29.696438: step 49200/300200 (epoch 33/200), loss = 0.128935 (0.208 sec/batch), lr: 0.189075\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3450.0 / guessed_by_relation= 5246.0\n",
      "Calculating Recall: correct_by_relation= 3450.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.764%\n",
      "   Recall (micro): 53.414%\n",
      "       F1 (micro): 58.949%\n",
      "epoch 33: train_loss = 0.144414, dev_loss = 0.489284, dev_f1 = 0.5895\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:09:59.140374: step 49600/300200 (epoch 34/200), loss = 0.225125 (0.169 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:11:08.700416: step 50000/300200 (epoch 34/200), loss = 0.139899 (0.173 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:12:18.192277: step 50400/300200 (epoch 34/200), loss = 0.119541 (0.162 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:13:27.585875: step 50800/300200 (epoch 34/200), loss = 0.091511 (0.193 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3647.0 / guessed_by_relation= 5556.0\n",
      "Calculating Recall: correct_by_relation= 3647.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.641%\n",
      "   Recall (micro): 56.464%\n",
      "       F1 (micro): 60.707%\n",
      "epoch 34: train_loss = 0.136786, dev_loss = 0.500042, dev_f1 = 0.6071\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:14:57.766772: step 51200/300200 (epoch 35/200), loss = 0.072049 (0.178 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:16:07.510303: step 51600/300200 (epoch 35/200), loss = 0.074866 (0.178 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:17:16.888861: step 52000/300200 (epoch 35/200), loss = 0.143929 (0.166 sec/batch), lr: 0.179621\n",
      "2018-03-22 10:18:26.148103: step 52400/300200 (epoch 35/200), loss = 0.072870 (0.157 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3578.0 / guessed_by_relation= 5405.0\n",
      "Calculating Recall: correct_by_relation= 3578.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 66.198%\n",
      "   Recall (micro): 55.396%\n",
      "       F1 (micro): 60.317%\n",
      "epoch 35: train_loss = 0.132223, dev_loss = 0.518800, dev_f1 = 0.6032\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:19:56.271848: step 52800/300200 (epoch 36/200), loss = 0.204977 (0.166 sec/batch), lr: 0.170640\n",
      "2018-03-22 10:21:05.792786: step 53200/300200 (epoch 36/200), loss = 0.081747 (0.168 sec/batch), lr: 0.170640\n",
      "2018-03-22 10:22:14.898619: step 53600/300200 (epoch 36/200), loss = 0.057398 (0.165 sec/batch), lr: 0.170640\n",
      "2018-03-22 10:23:22.869433: step 54000/300200 (epoch 36/200), loss = 0.136131 (0.147 sec/batch), lr: 0.170640\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3530.0 / guessed_by_relation= 5400.0\n",
      "Calculating Recall: correct_by_relation= 3530.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.370%\n",
      "   Recall (micro): 54.652%\n",
      "       F1 (micro): 59.533%\n",
      "epoch 36: train_loss = 0.126266, dev_loss = 0.513811, dev_f1 = 0.5953\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:24:54.479131: step 54400/300200 (epoch 37/200), loss = 0.095245 (0.186 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:26:04.739034: step 54800/300200 (epoch 37/200), loss = 0.053657 (0.171 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:27:13.700484: step 55200/300200 (epoch 37/200), loss = 0.161137 (0.154 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3620.0 / guessed_by_relation= 5598.0\n",
      "Calculating Recall: correct_by_relation= 3620.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.666%\n",
      "   Recall (micro): 56.046%\n",
      "       F1 (micro): 60.048%\n",
      "epoch 37: train_loss = 0.120147, dev_loss = 0.530663, dev_f1 = 0.6005\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:28:44.749690: step 55600/300200 (epoch 38/200), loss = 0.063523 (0.159 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:29:54.580452: step 56000/300200 (epoch 38/200), loss = 0.151037 (0.152 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:31:04.081337: step 56400/300200 (epoch 38/200), loss = 0.241068 (0.192 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:32:12.994659: step 56800/300200 (epoch 38/200), loss = 0.051599 (0.178 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3716.0 / guessed_by_relation= 5883.0\n",
      "Calculating Recall: correct_by_relation= 3716.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.165%\n",
      "   Recall (micro): 57.532%\n",
      "       F1 (micro): 60.217%\n",
      "epoch 38: train_loss = 0.117688, dev_loss = 0.548801, dev_f1 = 0.6022\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:33:44.796868: step 57200/300200 (epoch 39/200), loss = 0.181721 (0.211 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:34:54.648687: step 57600/300200 (epoch 39/200), loss = 0.230291 (0.182 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:36:04.388205: step 58000/300200 (epoch 39/200), loss = 0.236316 (0.168 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:37:13.227329: step 58400/300200 (epoch 39/200), loss = 0.049216 (0.178 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3736.0 / guessed_by_relation= 5868.0\n",
      "Calculating Recall: correct_by_relation= 3736.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.667%\n",
      "   Recall (micro): 57.842%\n",
      "       F1 (micro): 60.615%\n",
      "epoch 39: train_loss = 0.115733, dev_loss = 0.549495, dev_f1 = 0.6061\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:38:44.607416: step 58800/300200 (epoch 40/200), loss = 0.109095 (0.164 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:39:54.014050: step 59200/300200 (epoch 40/200), loss = 0.115109 (0.156 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:41:02.843147: step 59600/300200 (epoch 40/200), loss = 0.079666 (0.152 sec/batch), lr: 0.162108\n",
      "2018-03-22 10:42:12.326986: step 60000/300200 (epoch 40/200), loss = 0.254049 (0.195 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3669.0 / guessed_by_relation= 5867.0\n",
      "Calculating Recall: correct_by_relation= 3669.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.536%\n",
      "   Recall (micro): 56.804%\n",
      "       F1 (micro): 59.533%\n",
      "epoch 40: train_loss = 0.112122, dev_loss = 0.557743, dev_f1 = 0.5953\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:43:44.849111: step 60400/300200 (epoch 41/200), loss = 0.101910 (0.173 sec/batch), lr: 0.154003\n",
      "2018-03-22 10:44:54.488364: step 60800/300200 (epoch 41/200), loss = 0.037161 (0.184 sec/batch), lr: 0.154003\n",
      "2018-03-22 10:46:03.182101: step 61200/300200 (epoch 41/200), loss = 0.060257 (0.208 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3672.0 / guessed_by_relation= 5842.0\n",
      "Calculating Recall: correct_by_relation= 3672.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.855%\n",
      "   Recall (micro): 56.851%\n",
      "       F1 (micro): 59.702%\n",
      "epoch 41: train_loss = 0.106295, dev_loss = 0.573980, dev_f1 = 0.5970\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_41.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:47:34.949218: step 61600/300200 (epoch 42/200), loss = 0.083285 (0.167 sec/batch), lr: 0.154003\n",
      "2018-03-22 10:48:44.540342: step 62000/300200 (epoch 42/200), loss = 0.267845 (0.164 sec/batch), lr: 0.154003\n",
      "2018-03-22 10:49:53.776523: step 62400/300200 (epoch 42/200), loss = 0.027556 (0.159 sec/batch), lr: 0.154003\n",
      "2018-03-22 10:51:02.912436: step 62800/300200 (epoch 42/200), loss = 0.075964 (0.149 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3626.0 / guessed_by_relation= 5896.0\n",
      "Calculating Recall: correct_by_relation= 3626.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.499%\n",
      "   Recall (micro): 56.139%\n",
      "       F1 (micro): 58.697%\n",
      "epoch 42: train_loss = 0.103942, dev_loss = 0.590990, dev_f1 = 0.5870\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_42.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:52:34.358699: step 63200/300200 (epoch 43/200), loss = 0.104632 (0.191 sec/batch), lr: 0.146302\n",
      "2018-03-22 10:53:43.439466: step 63600/300200 (epoch 43/200), loss = 0.078992 (0.178 sec/batch), lr: 0.146302\n",
      "2018-03-22 10:54:53.046633: step 64000/300200 (epoch 43/200), loss = 0.195805 (0.177 sec/batch), lr: 0.146302\n",
      "2018-03-22 10:56:02.096317: step 64400/300200 (epoch 43/200), loss = 0.042421 (0.193 sec/batch), lr: 0.146302\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3465.0 / guessed_by_relation= 5386.0\n",
      "Calculating Recall: correct_by_relation= 3465.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.333%\n",
      "   Recall (micro): 53.646%\n",
      "       F1 (micro): 58.506%\n",
      "epoch 43: train_loss = 0.098067, dev_loss = 0.597312, dev_f1 = 0.5851\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_43.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 10:57:34.233418: step 64800/300200 (epoch 44/200), loss = 0.106302 (0.170 sec/batch), lr: 0.138987\n",
      "2018-03-22 10:58:43.587912: step 65200/300200 (epoch 44/200), loss = 0.046859 (0.179 sec/batch), lr: 0.138987\n",
      "2018-03-22 10:59:53.137928: step 65600/300200 (epoch 44/200), loss = 0.152973 (0.199 sec/batch), lr: 0.138987\n",
      "2018-03-22 11:01:02.307932: step 66000/300200 (epoch 44/200), loss = 0.062870 (0.174 sec/batch), lr: 0.138987\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3542.0 / guessed_by_relation= 5664.0\n",
      "Calculating Recall: correct_by_relation= 3542.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.535%\n",
      "   Recall (micro): 54.838%\n",
      "       F1 (micro): 58.434%\n",
      "epoch 44: train_loss = 0.094321, dev_loss = 0.621253, dev_f1 = 0.5843\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_44.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:02:34.256531: step 66400/300200 (epoch 45/200), loss = 0.188582 (0.176 sec/batch), lr: 0.132038\n",
      "2018-03-22 11:03:43.626066: step 66800/300200 (epoch 45/200), loss = 0.036806 (0.151 sec/batch), lr: 0.132038\n",
      "2018-03-22 11:04:52.853223: step 67200/300200 (epoch 45/200), loss = 0.197477 (0.154 sec/batch), lr: 0.132038\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3602.0 / guessed_by_relation= 5687.0\n",
      "Calculating Recall: correct_by_relation= 3602.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.337%\n",
      "   Recall (micro): 55.767%\n",
      "       F1 (micro): 59.312%\n",
      "epoch 45: train_loss = 0.090712, dev_loss = 0.616952, dev_f1 = 0.5931\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_45.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:06:24.248350: step 67600/300200 (epoch 46/200), loss = 0.053434 (0.200 sec/batch), lr: 0.132038\n",
      "2018-03-22 11:07:32.634268: step 68000/300200 (epoch 46/200), loss = 0.248224 (0.148 sec/batch), lr: 0.132038\n",
      "2018-03-22 11:08:42.602395: step 68400/300200 (epoch 46/200), loss = 0.092221 (0.164 sec/batch), lr: 0.132038\n",
      "2018-03-22 11:09:51.627013: step 68800/300200 (epoch 46/200), loss = 0.081865 (0.164 sec/batch), lr: 0.132038\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3564.0 / guessed_by_relation= 5597.0\n",
      "Calculating Recall: correct_by_relation= 3564.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.677%\n",
      "   Recall (micro): 55.179%\n",
      "       F1 (micro): 59.124%\n",
      "epoch 46: train_loss = 0.087456, dev_loss = 0.640185, dev_f1 = 0.5912\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_46.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:11:22.648145: step 69200/300200 (epoch 47/200), loss = 0.059275 (0.176 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:12:32.298427: step 69600/300200 (epoch 47/200), loss = 0.067340 (0.159 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:13:42.410938: step 70000/300200 (epoch 47/200), loss = 0.126260 (0.176 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:14:51.643108: step 70400/300200 (epoch 47/200), loss = 0.016934 (0.162 sec/batch), lr: 0.125436\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3609.0 / guessed_by_relation= 5740.0\n",
      "Calculating Recall: correct_by_relation= 3609.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.875%\n",
      "   Recall (micro): 55.876%\n",
      "       F1 (micro): 59.169%\n",
      "epoch 47: train_loss = 0.082859, dev_loss = 0.653282, dev_f1 = 0.5917\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_47.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:16:22.027546: step 70800/300200 (epoch 48/200), loss = 0.068414 (0.179 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:17:31.267737: step 71200/300200 (epoch 48/200), loss = 0.091485 (0.163 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:18:41.035331: step 71600/300200 (epoch 48/200), loss = 0.036736 (0.176 sec/batch), lr: 0.125436\n",
      "2018-03-22 11:19:50.383810: step 72000/300200 (epoch 48/200), loss = 0.022084 (0.142 sec/batch), lr: 0.125436\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3539.0 / guessed_by_relation= 5564.0\n",
      "Calculating Recall: correct_by_relation= 3539.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.605%\n",
      "   Recall (micro): 54.792%\n",
      "       F1 (micro): 58.870%\n",
      "epoch 48: train_loss = 0.080143, dev_loss = 0.673513, dev_f1 = 0.5887\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_48.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:21:21.190371: step 72400/300200 (epoch 49/200), loss = 0.094307 (0.135 sec/batch), lr: 0.119164\n",
      "2018-03-22 11:22:30.666189: step 72800/300200 (epoch 49/200), loss = 0.194607 (0.172 sec/batch), lr: 0.119164\n",
      "2018-03-22 11:23:39.941473: step 73200/300200 (epoch 49/200), loss = 0.080167 (0.154 sec/batch), lr: 0.119164\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3697.0 / guessed_by_relation= 5931.0\n",
      "Calculating Recall: correct_by_relation= 3697.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.334%\n",
      "   Recall (micro): 57.238%\n",
      "       F1 (micro): 59.677%\n",
      "epoch 49: train_loss = 0.077335, dev_loss = 0.676099, dev_f1 = 0.5968\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_49.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:25:11.494019: step 73600/300200 (epoch 50/200), loss = 0.165381 (0.169 sec/batch), lr: 0.119164\n",
      "2018-03-22 11:26:19.613228: step 74000/300200 (epoch 50/200), loss = 0.030930 (0.189 sec/batch), lr: 0.119164\n",
      "2018-03-22 11:27:29.219393: step 74400/300200 (epoch 50/200), loss = 0.090969 (0.186 sec/batch), lr: 0.119164\n",
      "2018-03-22 11:28:38.177834: step 74800/300200 (epoch 50/200), loss = 0.052606 (0.168 sec/batch), lr: 0.119164\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3550.0 / guessed_by_relation= 5586.0\n",
      "Calculating Recall: correct_by_relation= 3550.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.552%\n",
      "   Recall (micro): 54.962%\n",
      "       F1 (micro): 58.946%\n",
      "epoch 50: train_loss = 0.075563, dev_loss = 0.677884, dev_f1 = 0.5895\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_50.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:30:09.500769: step 75200/300200 (epoch 51/200), loss = 0.028032 (0.149 sec/batch), lr: 0.113206\n",
      "2018-03-22 11:31:18.006005: step 75600/300200 (epoch 51/200), loss = 0.148929 (0.174 sec/batch), lr: 0.113206\n",
      "2018-03-22 11:32:27.750537: step 76000/300200 (epoch 51/200), loss = 0.130523 (0.198 sec/batch), lr: 0.113206\n",
      "2018-03-22 11:33:36.260786: step 76400/300200 (epoch 51/200), loss = 0.077121 (0.170 sec/batch), lr: 0.113206\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3601.0 / guessed_by_relation= 5763.0\n",
      "Calculating Recall: correct_by_relation= 3601.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.485%\n",
      "   Recall (micro): 55.752%\n",
      "       F1 (micro): 58.927%\n",
      "epoch 51: train_loss = 0.074584, dev_loss = 0.681729, dev_f1 = 0.5893\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_51.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:35:06.567016: step 76800/300200 (epoch 52/200), loss = 0.110668 (0.181 sec/batch), lr: 0.107546\n",
      "2018-03-22 11:36:15.530471: step 77200/300200 (epoch 52/200), loss = 0.045531 (0.139 sec/batch), lr: 0.107546\n",
      "2018-03-22 11:37:25.132625: step 77600/300200 (epoch 52/200), loss = 0.033141 (0.182 sec/batch), lr: 0.107546\n",
      "2018-03-22 11:38:33.649893: step 78000/300200 (epoch 52/200), loss = 0.048854 (0.179 sec/batch), lr: 0.107546\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3542.0 / guessed_by_relation= 5606.0\n",
      "Calculating Recall: correct_by_relation= 3542.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.182%\n",
      "   Recall (micro): 54.838%\n",
      "       F1 (micro): 58.715%\n",
      "epoch 52: train_loss = 0.071405, dev_loss = 0.689033, dev_f1 = 0.5872\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_52.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:40:04.050374: step 78400/300200 (epoch 53/200), loss = 0.066685 (0.162 sec/batch), lr: 0.102168\n",
      "2018-03-22 11:41:13.858075: step 78800/300200 (epoch 53/200), loss = 0.034275 (0.180 sec/batch), lr: 0.102168\n",
      "2018-03-22 11:42:23.068185: step 79200/300200 (epoch 53/200), loss = 0.065910 (0.170 sec/batch), lr: 0.102168\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3666.0 / guessed_by_relation= 5871.0\n",
      "Calculating Recall: correct_by_relation= 3666.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.443%\n",
      "   Recall (micro): 56.758%\n",
      "       F1 (micro): 59.465%\n",
      "epoch 53: train_loss = 0.065895, dev_loss = 0.722010, dev_f1 = 0.5946\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_53.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:43:55.694588: step 79600/300200 (epoch 54/200), loss = 0.007782 (0.171 sec/batch), lr: 0.102168\n",
      "2018-03-22 11:45:03.566138: step 80000/300200 (epoch 54/200), loss = 0.057274 (0.164 sec/batch), lr: 0.102168\n",
      "2018-03-22 11:46:13.226447: step 80400/300200 (epoch 54/200), loss = 0.155736 (0.169 sec/batch), lr: 0.102168\n",
      "2018-03-22 11:47:22.636088: step 80800/300200 (epoch 54/200), loss = 0.011790 (0.180 sec/batch), lr: 0.102168\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3605.0 / guessed_by_relation= 5757.0\n",
      "Calculating Recall: correct_by_relation= 3605.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.619%\n",
      "   Recall (micro): 55.814%\n",
      "       F1 (micro): 59.021%\n",
      "epoch 54: train_loss = 0.065473, dev_loss = 0.731732, dev_f1 = 0.5902\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_54.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:48:55.118106: step 81200/300200 (epoch 55/200), loss = 0.047505 (0.159 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:50:02.451225: step 81600/300200 (epoch 55/200), loss = 0.117255 (0.179 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:51:11.821762: step 82000/300200 (epoch 55/200), loss = 0.061265 (0.176 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:52:21.115094: step 82400/300200 (epoch 55/200), loss = 0.048425 (0.165 sec/batch), lr: 0.097060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3690.0 / guessed_by_relation= 5955.0\n",
      "Calculating Recall: correct_by_relation= 3690.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.965%\n",
      "   Recall (micro): 57.130%\n",
      "       F1 (micro): 59.449%\n",
      "epoch 55: train_loss = 0.061964, dev_loss = 0.737847, dev_f1 = 0.5945\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_55.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:53:53.647246: step 82800/300200 (epoch 56/200), loss = 0.032648 (0.157 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:55:01.464652: step 83200/300200 (epoch 56/200), loss = 0.035109 (0.161 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:56:10.889334: step 83600/300200 (epoch 56/200), loss = 0.028204 (0.153 sec/batch), lr: 0.097060\n",
      "2018-03-22 11:57:20.270901: step 84000/300200 (epoch 56/200), loss = 0.006311 (0.162 sec/batch), lr: 0.097060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3610.0 / guessed_by_relation= 5767.0\n",
      "Calculating Recall: correct_by_relation= 3610.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.598%\n",
      "   Recall (micro): 55.891%\n",
      "       F1 (micro): 59.054%\n",
      "epoch 56: train_loss = 0.061557, dev_loss = 0.757492, dev_f1 = 0.5905\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_56.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 11:58:52.149313: step 84400/300200 (epoch 57/200), loss = 0.085963 (0.142 sec/batch), lr: 0.092207\n",
      "2018-03-22 12:00:00.174271: step 84800/300200 (epoch 57/200), loss = 0.030655 (0.155 sec/batch), lr: 0.092207\n",
      "2018-03-22 12:01:09.818538: step 85200/300200 (epoch 57/200), loss = 0.112053 (0.184 sec/batch), lr: 0.092207\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3527.0 / guessed_by_relation= 5653.0\n",
      "Calculating Recall: correct_by_relation= 3527.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.392%\n",
      "   Recall (micro): 54.606%\n",
      "       F1 (micro): 58.240%\n",
      "epoch 57: train_loss = 0.057198, dev_loss = 0.750921, dev_f1 = 0.5824\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_57.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:02:42.690593: step 85600/300200 (epoch 58/200), loss = 0.076648 (0.178 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:03:49.708873: step 86000/300200 (epoch 58/200), loss = 0.040784 (0.165 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:04:58.993181: step 86400/300200 (epoch 58/200), loss = 0.035442 (0.179 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:06:08.296541: step 86800/300200 (epoch 58/200), loss = 0.008458 (0.164 sec/batch), lr: 0.087597\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3621.0 / guessed_by_relation= 5684.0\n",
      "Calculating Recall: correct_by_relation= 3621.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.705%\n",
      "   Recall (micro): 56.061%\n",
      "       F1 (micro): 59.639%\n",
      "epoch 58: train_loss = 0.056265, dev_loss = 0.744797, dev_f1 = 0.5964\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_58.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:07:41.043264: step 87200/300200 (epoch 59/200), loss = 0.024646 (0.198 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:08:48.124711: step 87600/300200 (epoch 59/200), loss = 0.019790 (0.162 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:09:57.702801: step 88000/300200 (epoch 59/200), loss = 0.078185 (0.174 sec/batch), lr: 0.087597\n",
      "2018-03-22 12:11:06.859771: step 88400/300200 (epoch 59/200), loss = 0.073723 (0.181 sec/batch), lr: 0.087597\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3626.0 / guessed_by_relation= 5808.0\n",
      "Calculating Recall: correct_by_relation= 3626.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.431%\n",
      "   Recall (micro): 56.139%\n",
      "       F1 (micro): 59.118%\n",
      "epoch 59: train_loss = 0.054063, dev_loss = 0.783779, dev_f1 = 0.5912\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_59.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:12:38.926685: step 88800/300200 (epoch 60/200), loss = 0.032949 (0.184 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:13:46.259803: step 89200/300200 (epoch 60/200), loss = 0.402851 (0.163 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:14:55.807812: step 89600/300200 (epoch 60/200), loss = 0.063838 (0.203 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:16:05.264580: step 90000/300200 (epoch 60/200), loss = 0.048732 (0.182 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3644.0 / guessed_by_relation= 5844.0\n",
      "Calculating Recall: correct_by_relation= 3644.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.355%\n",
      "   Recall (micro): 56.417%\n",
      "       F1 (micro): 59.238%\n",
      "epoch 60: train_loss = 0.051428, dev_loss = 0.770761, dev_f1 = 0.5924\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_60.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:17:37.580155: step 90400/300200 (epoch 61/200), loss = 0.009591 (0.160 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:18:44.339747: step 90800/300200 (epoch 61/200), loss = 0.012200 (0.187 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:19:53.688226: step 91200/300200 (epoch 61/200), loss = 0.014023 (0.156 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3622.0 / guessed_by_relation= 5681.0\n",
      "Calculating Recall: correct_by_relation= 3622.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.756%\n",
      "   Recall (micro): 56.077%\n",
      "       F1 (micro): 59.671%\n",
      "epoch 61: train_loss = 0.048918, dev_loss = 0.787674, dev_f1 = 0.5967\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_61.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:21:24.668249: step 91600/300200 (epoch 62/200), loss = 0.020504 (0.160 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:22:33.521410: step 92000/300200 (epoch 62/200), loss = 0.106607 (0.154 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:23:40.915691: step 92400/300200 (epoch 62/200), loss = 0.166393 (0.185 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:24:50.436628: step 92800/300200 (epoch 62/200), loss = 0.047763 (0.168 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3729.0 / guessed_by_relation= 5992.0\n",
      "Calculating Recall: correct_by_relation= 3729.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.233%\n",
      "   Recall (micro): 57.733%\n",
      "       F1 (micro): 59.899%\n",
      "epoch 62: train_loss = 0.050306, dev_loss = 0.784385, dev_f1 = 0.5990\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_62.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:26:22.139574: step 93200/300200 (epoch 63/200), loss = 0.133366 (0.176 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:27:30.400159: step 93600/300200 (epoch 63/200), loss = 0.013281 (0.131 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:28:38.939486: step 94000/300200 (epoch 63/200), loss = 0.085437 (0.181 sec/batch), lr: 0.083217\n",
      "2018-03-22 12:29:48.291975: step 94400/300200 (epoch 63/200), loss = 0.016415 (0.171 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3491.0 / guessed_by_relation= 5486.0\n",
      "Calculating Recall: correct_by_relation= 3491.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.635%\n",
      "   Recall (micro): 54.049%\n",
      "       F1 (micro): 58.451%\n",
      "epoch 63: train_loss = 0.049962, dev_loss = 0.805277, dev_f1 = 0.5845\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_63.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:31:20.389973: step 94800/300200 (epoch 64/200), loss = 0.008578 (0.168 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:32:27.192679: step 95200/300200 (epoch 64/200), loss = 0.040559 (0.174 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:33:36.698577: step 95600/300200 (epoch 64/200), loss = 0.036957 (0.169 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:34:46.374928: step 96000/300200 (epoch 64/200), loss = 0.014435 (0.175 sec/batch), lr: 0.079056\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3586.0 / guessed_by_relation= 5737.0\n",
      "Calculating Recall: correct_by_relation= 3586.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.507%\n",
      "   Recall (micro): 55.519%\n",
      "       F1 (micro): 58.806%\n",
      "epoch 64: train_loss = 0.045327, dev_loss = 0.810232, dev_f1 = 0.5881\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_64.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:36:19.288093: step 96400/300200 (epoch 65/200), loss = 0.016213 (0.162 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:37:26.522949: step 96800/300200 (epoch 65/200), loss = 0.042933 (0.165 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:38:36.272495: step 97200/300200 (epoch 65/200), loss = 0.044833 (0.176 sec/batch), lr: 0.079056\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3662.0 / guessed_by_relation= 5874.0\n",
      "Calculating Recall: correct_by_relation= 3662.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.343%\n",
      "   Recall (micro): 56.696%\n",
      "       F1 (micro): 59.385%\n",
      "epoch 65: train_loss = 0.045299, dev_loss = 0.813677, dev_f1 = 0.5939\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_65.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:40:08.935996: step 97600/300200 (epoch 66/200), loss = 0.124697 (0.174 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:41:18.473979: step 98000/300200 (epoch 66/200), loss = 0.016895 (0.170 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:42:25.448142: step 98400/300200 (epoch 66/200), loss = 0.017035 (0.171 sec/batch), lr: 0.079056\n",
      "2018-03-22 12:43:34.535928: step 98800/300200 (epoch 66/200), loss = 0.011112 (0.141 sec/batch), lr: 0.079056\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3624.0 / guessed_by_relation= 5759.0\n",
      "Calculating Recall: correct_by_relation= 3624.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.928%\n",
      "   Recall (micro): 56.108%\n",
      "       F1 (micro): 59.322%\n",
      "epoch 66: train_loss = 0.043409, dev_loss = 0.822264, dev_f1 = 0.5932\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_66.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:45:07.544346: step 99200/300200 (epoch 67/200), loss = 0.046002 (0.159 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:46:17.041220: step 99600/300200 (epoch 67/200), loss = 0.237222 (0.155 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:47:24.404418: step 100000/300200 (epoch 67/200), loss = 0.037235 (0.195 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:48:34.021612: step 100400/300200 (epoch 67/200), loss = 0.029382 (0.181 sec/batch), lr: 0.075103\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3726.0 / guessed_by_relation= 5995.0\n",
      "Calculating Recall: correct_by_relation= 3726.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.152%\n",
      "   Recall (micro): 57.687%\n",
      "       F1 (micro): 59.836%\n",
      "epoch 67: train_loss = 0.043140, dev_loss = 0.843007, dev_f1 = 0.5984\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_67.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:50:07.384975: step 100800/300200 (epoch 68/200), loss = 0.068022 (0.173 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:51:16.330381: step 101200/300200 (epoch 68/200), loss = 0.042412 (0.151 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:52:24.440567: step 101600/300200 (epoch 68/200), loss = 0.014455 (0.161 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:53:34.103883: step 102000/300200 (epoch 68/200), loss = 0.008966 (0.132 sec/batch), lr: 0.075103\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3695.0 / guessed_by_relation= 5884.0\n",
      "Calculating Recall: correct_by_relation= 3695.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.797%\n",
      "   Recall (micro): 57.207%\n",
      "       F1 (micro): 59.872%\n",
      "epoch 68: train_loss = 0.042380, dev_loss = 0.840736, dev_f1 = 0.5987\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_68.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:55:07.007022: step 102400/300200 (epoch 69/200), loss = 0.180437 (0.162 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:56:15.245548: step 102800/300200 (epoch 69/200), loss = 0.053103 (0.166 sec/batch), lr: 0.075103\n",
      "2018-03-22 12:57:24.203989: step 103200/300200 (epoch 69/200), loss = 0.016619 (0.170 sec/batch), lr: 0.075103\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3632.0 / guessed_by_relation= 5806.0\n",
      "Calculating Recall: correct_by_relation= 3632.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.556%\n",
      "   Recall (micro): 56.232%\n",
      "       F1 (micro): 59.225%\n",
      "epoch 69: train_loss = 0.042516, dev_loss = 0.854462, dev_f1 = 0.5923\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_69.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 12:58:57.613475: step 103600/300200 (epoch 70/200), loss = 0.018649 (0.181 sec/batch), lr: 0.071348\n",
      "2018-03-22 13:00:06.868706: step 104000/300200 (epoch 70/200), loss = 0.125481 (0.186 sec/batch), lr: 0.071348\n",
      "2018-03-22 13:01:14.157706: step 104400/300200 (epoch 70/200), loss = 0.039461 (0.174 sec/batch), lr: 0.071348\n",
      "2018-03-22 13:02:23.305652: step 104800/300200 (epoch 70/200), loss = 0.030444 (0.162 sec/batch), lr: 0.071348\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3587.0 / guessed_by_relation= 5662.0\n",
      "Calculating Recall: correct_by_relation= 3587.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.352%\n",
      "   Recall (micro): 55.535%\n",
      "       F1 (micro): 59.187%\n",
      "epoch 70: train_loss = 0.041467, dev_loss = 0.824523, dev_f1 = 0.5919\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_70.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:03:56.125569: step 105200/300200 (epoch 71/200), loss = 0.020180 (0.175 sec/batch), lr: 0.067781\n",
      "2018-03-22 13:05:05.420907: step 105600/300200 (epoch 71/200), loss = 0.024029 (0.182 sec/batch), lr: 0.067781\n",
      "2018-03-22 13:06:12.366995: step 106000/300200 (epoch 71/200), loss = 0.108407 (0.172 sec/batch), lr: 0.067781\n",
      "2018-03-22 13:07:21.755580: step 106400/300200 (epoch 71/200), loss = 0.012602 (0.175 sec/batch), lr: 0.067781\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3550.0 / guessed_by_relation= 5615.0\n",
      "Calculating Recall: correct_by_relation= 3550.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.224%\n",
      "   Recall (micro): 54.962%\n",
      "       F1 (micro): 58.804%\n",
      "epoch 71: train_loss = 0.040976, dev_loss = 0.856647, dev_f1 = 0.5880\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_71.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:08:55.511989: step 106800/300200 (epoch 72/200), loss = 0.012685 (0.190 sec/batch), lr: 0.064392\n",
      "2018-03-22 13:10:04.224777: step 107200/300200 (epoch 72/200), loss = 0.068101 (0.175 sec/batch), lr: 0.064392\n",
      "2018-03-22 13:11:11.669190: step 107600/300200 (epoch 72/200), loss = 0.010409 (0.158 sec/batch), lr: 0.064392\n",
      "2018-03-22 13:12:21.040731: step 108000/300200 (epoch 72/200), loss = 0.062885 (0.157 sec/batch), lr: 0.064392\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3682.0 / guessed_by_relation= 5790.0\n",
      "Calculating Recall: correct_by_relation= 3682.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.592%\n",
      "   Recall (micro): 57.006%\n",
      "       F1 (micro): 60.119%\n",
      "epoch 72: train_loss = 0.037092, dev_loss = 0.851661, dev_f1 = 0.6012\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_72.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:13:53.623016: step 108400/300200 (epoch 73/200), loss = 0.003388 (0.161 sec/batch), lr: 0.064392\n",
      "2018-03-22 13:15:02.265617: step 108800/300200 (epoch 73/200), loss = 0.009450 (0.162 sec/batch), lr: 0.064392\n",
      "2018-03-22 13:16:09.734095: step 109200/300200 (epoch 73/200), loss = 0.079238 (0.185 sec/batch), lr: 0.064392\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3644.0 / guessed_by_relation= 5721.0\n",
      "Calculating Recall: correct_by_relation= 3644.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.695%\n",
      "   Recall (micro): 56.417%\n",
      "       F1 (micro): 59.836%\n",
      "epoch 73: train_loss = 0.035655, dev_loss = 0.861334, dev_f1 = 0.5984\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_73.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:17:42.783623: step 109600/300200 (epoch 74/200), loss = 0.039907 (0.179 sec/batch), lr: 0.061172\n",
      "2018-03-22 13:18:51.756102: step 110000/300200 (epoch 74/200), loss = 0.021700 (0.166 sec/batch), lr: 0.061172\n",
      "2018-03-22 13:20:00.604250: step 110400/300200 (epoch 74/200), loss = 0.018982 (0.175 sec/batch), lr: 0.061172\n",
      "2018-03-22 13:21:08.590104: step 110800/300200 (epoch 74/200), loss = 0.047421 (0.165 sec/batch), lr: 0.061172\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3620.0 / guessed_by_relation= 5720.0\n",
      "Calculating Recall: correct_by_relation= 3620.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.287%\n",
      "   Recall (micro): 56.046%\n",
      "       F1 (micro): 59.447%\n",
      "epoch 74: train_loss = 0.036403, dev_loss = 0.882507, dev_f1 = 0.5945\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_74.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:22:41.433082: step 111200/300200 (epoch 75/200), loss = 0.027961 (0.173 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:23:50.220067: step 111600/300200 (epoch 75/200), loss = 0.021352 (0.157 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:24:57.902114: step 112000/300200 (epoch 75/200), loss = 0.005739 (0.097 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:26:06.830475: step 112400/300200 (epoch 75/200), loss = 0.065555 (0.176 sec/batch), lr: 0.058113\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3778.0 / guessed_by_relation= 6028.0\n",
      "Calculating Recall: correct_by_relation= 3778.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.674%\n",
      "   Recall (micro): 58.492%\n",
      "       F1 (micro): 60.511%\n",
      "epoch 75: train_loss = 0.034974, dev_loss = 0.882646, dev_f1 = 0.6051\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_75.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:27:39.225261: step 112800/300200 (epoch 76/200), loss = 0.030855 (0.168 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:28:48.243864: step 113200/300200 (epoch 76/200), loss = 0.036866 (0.178 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:29:55.427583: step 113600/300200 (epoch 76/200), loss = 0.037373 (0.190 sec/batch), lr: 0.058113\n",
      "2018-03-22 13:31:04.785086: step 114000/300200 (epoch 76/200), loss = 0.085950 (0.152 sec/batch), lr: 0.058113\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3765.0 / guessed_by_relation= 6009.0\n",
      "Calculating Recall: correct_by_relation= 3765.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.656%\n",
      "   Recall (micro): 58.291%\n",
      "       F1 (micro): 60.395%\n",
      "epoch 76: train_loss = 0.033547, dev_loss = 0.893217, dev_f1 = 0.6039\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_76.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:32:36.874059: step 114400/300200 (epoch 77/200), loss = 0.034502 (0.161 sec/batch), lr: 0.055208\n",
      "2018-03-22 13:33:45.716191: step 114800/300200 (epoch 77/200), loss = 0.018174 (0.192 sec/batch), lr: 0.055208\n",
      "2018-03-22 13:34:52.644231: step 115200/300200 (epoch 77/200), loss = 0.002753 (0.186 sec/batch), lr: 0.055208\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3703.0 / guessed_by_relation= 5877.0\n",
      "Calculating Recall: correct_by_relation= 3703.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.008%\n",
      "   Recall (micro): 57.331%\n",
      "       F1 (micro): 60.036%\n",
      "epoch 77: train_loss = 0.034322, dev_loss = 0.895870, dev_f1 = 0.6004\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_77.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:36:24.296041: step 115600/300200 (epoch 78/200), loss = 0.009386 (0.173 sec/batch), lr: 0.052447\n",
      "2018-03-22 13:37:33.367784: step 116000/300200 (epoch 78/200), loss = 0.010475 (0.177 sec/batch), lr: 0.052447\n",
      "2018-03-22 13:38:42.654097: step 116400/300200 (epoch 78/200), loss = 0.134008 (0.175 sec/batch), lr: 0.052447\n",
      "2018-03-22 13:39:49.107876: step 116800/300200 (epoch 78/200), loss = 0.071524 (0.167 sec/batch), lr: 0.052447\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3665.0 / guessed_by_relation= 5761.0\n",
      "Calculating Recall: correct_by_relation= 3665.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.617%\n",
      "   Recall (micro): 56.743%\n",
      "       F1 (micro): 59.984%\n",
      "epoch 78: train_loss = 0.031268, dev_loss = 0.886644, dev_f1 = 0.5998\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_78.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:41:21.676123: step 117200/300200 (epoch 79/200), loss = 0.009426 (0.192 sec/batch), lr: 0.049825\n",
      "2018-03-22 13:42:30.734832: step 117600/300200 (epoch 79/200), loss = 0.008343 (0.165 sec/batch), lr: 0.049825\n",
      "2018-03-22 13:43:39.653166: step 118000/300200 (epoch 79/200), loss = 0.000345 (0.170 sec/batch), lr: 0.049825\n",
      "2018-03-22 13:44:46.392705: step 118400/300200 (epoch 79/200), loss = 0.026672 (0.117 sec/batch), lr: 0.049825\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3635.0 / guessed_by_relation= 5711.0\n",
      "Calculating Recall: correct_by_relation= 3635.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.649%\n",
      "   Recall (micro): 56.278%\n",
      "       F1 (micro): 59.737%\n",
      "epoch 79: train_loss = 0.031302, dev_loss = 0.891481, dev_f1 = 0.5974\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_79.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:46:19.495374: step 118800/300200 (epoch 80/200), loss = 0.034899 (0.157 sec/batch), lr: 0.047334\n",
      "2018-03-22 13:47:28.756621: step 119200/300200 (epoch 80/200), loss = 0.016090 (0.168 sec/batch), lr: 0.047334\n",
      "2018-03-22 13:48:37.261857: step 119600/300200 (epoch 80/200), loss = 0.008400 (0.180 sec/batch), lr: 0.047334\n",
      "2018-03-22 13:49:45.214623: step 120000/300200 (epoch 80/200), loss = 0.023403 (0.163 sec/batch), lr: 0.047334\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3715.0 / guessed_by_relation= 5852.0\n",
      "Calculating Recall: correct_by_relation= 3715.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.483%\n",
      "   Recall (micro): 57.517%\n",
      "       F1 (micro): 60.353%\n",
      "epoch 80: train_loss = 0.030405, dev_loss = 0.888250, dev_f1 = 0.6035\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_80.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:51:18.602050: step 120400/300200 (epoch 81/200), loss = 0.005819 (0.164 sec/batch), lr: 0.047334\n",
      "2018-03-22 13:52:27.554476: step 120800/300200 (epoch 81/200), loss = 0.083042 (0.183 sec/batch), lr: 0.047334\n",
      "2018-03-22 13:53:33.845822: step 121200/300200 (epoch 81/200), loss = 0.016337 (0.066 sec/batch), lr: 0.047334\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3604.0 / guessed_by_relation= 5591.0\n",
      "Calculating Recall: correct_by_relation= 3604.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.461%\n",
      "   Recall (micro): 55.798%\n",
      "       F1 (micro): 59.817%\n",
      "epoch 81: train_loss = 0.030148, dev_loss = 0.894215, dev_f1 = 0.5982\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_81.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 13:55:04.958197: step 121600/300200 (epoch 82/200), loss = 0.050717 (0.176 sec/batch), lr: 0.044967\n",
      "2018-03-22 13:56:15.200052: step 122000/300200 (epoch 82/200), loss = 0.009222 (0.179 sec/batch), lr: 0.044967\n",
      "2018-03-22 13:57:24.217651: step 122400/300200 (epoch 82/200), loss = 0.018046 (0.200 sec/batch), lr: 0.044967\n",
      "2018-03-22 13:58:31.176774: step 122800/300200 (epoch 82/200), loss = 0.077363 (0.209 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3701.0 / guessed_by_relation= 5883.0\n",
      "Calculating Recall: correct_by_relation= 3701.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.910%\n",
      "   Recall (micro): 57.300%\n",
      "       F1 (micro): 59.974%\n",
      "epoch 82: train_loss = 0.029011, dev_loss = 0.910061, dev_f1 = 0.5997\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_82.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:00:04.429843: step 123200/300200 (epoch 83/200), loss = 0.002217 (0.172 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:01:13.588818: step 123600/300200 (epoch 83/200), loss = 0.025058 (0.167 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:02:22.755815: step 124000/300200 (epoch 83/200), loss = 0.006575 (0.173 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:03:29.224633: step 124400/300200 (epoch 83/200), loss = 0.001354 (0.176 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3725.0 / guessed_by_relation= 5961.0\n",
      "Calculating Recall: correct_by_relation= 3725.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.490%\n",
      "   Recall (micro): 57.671%\n",
      "       F1 (micro): 59.984%\n",
      "epoch 83: train_loss = 0.028626, dev_loss = 0.930017, dev_f1 = 0.5998\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_83.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:05:02.576967: step 124800/300200 (epoch 84/200), loss = 0.047050 (0.174 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:06:11.524378: step 125200/300200 (epoch 84/200), loss = 0.106489 (0.153 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:07:20.620186: step 125600/300200 (epoch 84/200), loss = 0.074001 (0.155 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:08:27.173228: step 126000/300200 (epoch 84/200), loss = 0.003896 (0.167 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3663.0 / guessed_by_relation= 5724.0\n",
      "Calculating Recall: correct_by_relation= 3663.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.994%\n",
      "   Recall (micro): 56.712%\n",
      "       F1 (micro): 60.133%\n",
      "epoch 84: train_loss = 0.026547, dev_loss = 0.910007, dev_f1 = 0.6013\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_84.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:10:01.281573: step 126400/300200 (epoch 85/200), loss = 0.029586 (0.187 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:11:10.167823: step 126800/300200 (epoch 85/200), loss = 0.028035 (0.138 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:12:19.016973: step 127200/300200 (epoch 85/200), loss = 0.008178 (0.153 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3696.0 / guessed_by_relation= 5801.0\n",
      "Calculating Recall: correct_by_relation= 3696.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.713%\n",
      "   Recall (micro): 57.222%\n",
      "       F1 (micro): 60.294%\n",
      "epoch 85: train_loss = 0.027946, dev_loss = 0.918801, dev_f1 = 0.6029\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_85.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:13:49.913774: step 127600/300200 (epoch 86/200), loss = 0.006082 (0.151 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:14:59.515928: step 128000/300200 (epoch 86/200), loss = 0.012583 (0.180 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:16:08.544556: step 128400/300200 (epoch 86/200), loss = 0.028154 (0.154 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:17:16.463232: step 128800/300200 (epoch 86/200), loss = 0.002864 (0.159 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3643.0 / guessed_by_relation= 5611.0\n",
      "Calculating Recall: correct_by_relation= 3643.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.926%\n",
      "   Recall (micro): 56.402%\n",
      "       F1 (micro): 60.365%\n",
      "epoch 86: train_loss = 0.027282, dev_loss = 0.903417, dev_f1 = 0.6036\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_86.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:18:48.522124: step 129200/300200 (epoch 87/200), loss = 0.108233 (0.192 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:19:58.060107: step 129600/300200 (epoch 87/200), loss = 0.090431 (0.174 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:21:07.051637: step 130000/300200 (epoch 87/200), loss = 0.004567 (0.147 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:22:14.611358: step 130400/300200 (epoch 87/200), loss = 0.007010 (0.113 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3690.0 / guessed_by_relation= 5642.0\n",
      "Calculating Recall: correct_by_relation= 3690.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.402%\n",
      "   Recall (micro): 57.130%\n",
      "       F1 (micro): 60.987%\n",
      "epoch 87: train_loss = 0.026545, dev_loss = 0.904420, dev_f1 = 0.6099\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_87.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:23:47.386155: step 130800/300200 (epoch 88/200), loss = 0.041792 (0.174 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:24:56.500010: step 131200/300200 (epoch 88/200), loss = 0.018919 (0.180 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:26:05.697086: step 131600/300200 (epoch 88/200), loss = 0.011831 (0.151 sec/batch), lr: 0.044967\n",
      "2018-03-22 14:27:12.759484: step 132000/300200 (epoch 88/200), loss = 0.034535 (0.168 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3632.0 / guessed_by_relation= 5692.0\n",
      "Calculating Recall: correct_by_relation= 3632.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.809%\n",
      "   Recall (micro): 56.232%\n",
      "       F1 (micro): 59.781%\n",
      "epoch 88: train_loss = 0.027193, dev_loss = 0.920448, dev_f1 = 0.5978\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_88.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:28:46.242164: step 132400/300200 (epoch 89/200), loss = 0.008581 (0.139 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:29:55.402141: step 132800/300200 (epoch 89/200), loss = 0.039295 (0.162 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:31:04.392668: step 133200/300200 (epoch 89/200), loss = 0.002633 (0.170 sec/batch), lr: 0.042719\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3630.0 / guessed_by_relation= 5676.0\n",
      "Calculating Recall: correct_by_relation= 3630.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.953%\n",
      "   Recall (micro): 56.201%\n",
      "       F1 (micro): 59.827%\n",
      "epoch 89: train_loss = 0.025768, dev_loss = 0.938184, dev_f1 = 0.5983\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_89.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:32:34.869352: step 133600/300200 (epoch 90/200), loss = 0.007970 (0.159 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:33:43.997244: step 134000/300200 (epoch 90/200), loss = 0.010409 (0.163 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:34:53.427942: step 134400/300200 (epoch 90/200), loss = 0.019793 (0.166 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:36:02.455567: step 134800/300200 (epoch 90/200), loss = 0.005596 (0.170 sec/batch), lr: 0.042719\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3645.0 / guessed_by_relation= 5580.0\n",
      "Calculating Recall: correct_by_relation= 3645.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.323%\n",
      "   Recall (micro): 56.433%\n",
      "       F1 (micro): 60.553%\n",
      "epoch 90: train_loss = 0.026045, dev_loss = 0.909237, dev_f1 = 0.6055\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_90.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:37:32.942277: step 135200/300200 (epoch 91/200), loss = 0.013499 (0.172 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:38:42.637680: step 135600/300200 (epoch 91/200), loss = 0.024831 (0.169 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:39:51.573059: step 136000/300200 (epoch 91/200), loss = 0.109303 (0.193 sec/batch), lr: 0.042719\n",
      "2018-03-22 14:41:00.349015: step 136400/300200 (epoch 91/200), loss = 0.002939 (0.164 sec/batch), lr: 0.042719\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3666.0 / guessed_by_relation= 5774.0\n",
      "Calculating Recall: correct_by_relation= 3666.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.492%\n",
      "   Recall (micro): 56.758%\n",
      "       F1 (micro): 59.936%\n",
      "epoch 91: train_loss = 0.025538, dev_loss = 0.921131, dev_f1 = 0.5994\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_91.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:42:29.510200: step 136800/300200 (epoch 92/200), loss = 0.036948 (0.153 sec/batch), lr: 0.040583\n",
      "2018-03-22 14:43:38.977996: step 137200/300200 (epoch 92/200), loss = 0.010793 (0.166 sec/batch), lr: 0.040583\n",
      "2018-03-22 14:44:48.401675: step 137600/300200 (epoch 92/200), loss = 0.008096 (0.194 sec/batch), lr: 0.040583\n",
      "2018-03-22 14:45:56.747486: step 138000/300200 (epoch 92/200), loss = 0.007283 (0.146 sec/batch), lr: 0.040583\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3601.0 / guessed_by_relation= 5652.0\n",
      "Calculating Recall: correct_by_relation= 3601.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.712%\n",
      "   Recall (micro): 55.752%\n",
      "       F1 (micro): 59.467%\n",
      "epoch 92: train_loss = 0.024500, dev_loss = 0.932648, dev_f1 = 0.5947\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_92.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:47:26.417023: step 138400/300200 (epoch 93/200), loss = 0.005617 (0.169 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:48:35.862761: step 138800/300200 (epoch 93/200), loss = 0.019572 (0.165 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:49:44.882365: step 139200/300200 (epoch 93/200), loss = 0.003545 (0.162 sec/batch), lr: 0.038554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3589.0 / guessed_by_relation= 5557.0\n",
      "Calculating Recall: correct_by_relation= 3589.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.585%\n",
      "   Recall (micro): 55.566%\n",
      "       F1 (micro): 59.737%\n",
      "epoch 93: train_loss = 0.023688, dev_loss = 0.947604, dev_f1 = 0.5974\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_93.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:51:13.758792: step 139600/300200 (epoch 94/200), loss = 0.004128 (0.159 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:52:23.529394: step 140000/300200 (epoch 94/200), loss = 0.000940 (0.179 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:53:32.618183: step 140400/300200 (epoch 94/200), loss = 0.120105 (0.154 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:54:41.388122: step 140800/300200 (epoch 94/200), loss = 0.010900 (0.187 sec/batch), lr: 0.038554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3674.0 / guessed_by_relation= 5763.0\n",
      "Calculating Recall: correct_by_relation= 3674.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.752%\n",
      "   Recall (micro): 56.882%\n",
      "       F1 (micro): 60.121%\n",
      "epoch 94: train_loss = 0.025186, dev_loss = 0.933845, dev_f1 = 0.6012\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_94.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 14:56:12.027238: step 141200/300200 (epoch 95/200), loss = 0.020169 (0.181 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:57:21.546170: step 141600/300200 (epoch 95/200), loss = 0.006728 (0.147 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:58:30.906681: step 142000/300200 (epoch 95/200), loss = 0.004640 (0.153 sec/batch), lr: 0.038554\n",
      "2018-03-22 14:59:39.690659: step 142400/300200 (epoch 95/200), loss = 0.021988 (0.181 sec/batch), lr: 0.038554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3673.0 / guessed_by_relation= 5767.0\n",
      "Calculating Recall: correct_by_relation= 3673.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.690%\n",
      "   Recall (micro): 56.866%\n",
      "       F1 (micro): 60.085%\n",
      "epoch 95: train_loss = 0.022341, dev_loss = 0.946630, dev_f1 = 0.6009\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_95.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:01:10.227502: step 142800/300200 (epoch 96/200), loss = 0.032640 (0.173 sec/batch), lr: 0.036626\n",
      "2018-03-22 15:02:19.554926: step 143200/300200 (epoch 96/200), loss = 0.005860 (0.154 sec/batch), lr: 0.036626\n",
      "2018-03-22 15:03:28.649730: step 143600/300200 (epoch 96/200), loss = 0.003640 (0.164 sec/batch), lr: 0.036626\n",
      "2018-03-22 15:04:37.578091: step 144000/300200 (epoch 96/200), loss = 0.008698 (0.169 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3686.0 / guessed_by_relation= 5742.0\n",
      "Calculating Recall: correct_by_relation= 3686.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.194%\n",
      "   Recall (micro): 57.068%\n",
      "       F1 (micro): 60.421%\n",
      "epoch 96: train_loss = 0.023334, dev_loss = 0.950211, dev_f1 = 0.6042\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_96.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:06:08.783714: step 144400/300200 (epoch 97/200), loss = 0.039061 (0.161 sec/batch), lr: 0.036626\n",
      "2018-03-22 15:07:18.354785: step 144800/300200 (epoch 97/200), loss = 0.027348 (0.193 sec/batch), lr: 0.036626\n",
      "2018-03-22 15:08:27.610016: step 145200/300200 (epoch 97/200), loss = 0.001407 (0.184 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3620.0 / guessed_by_relation= 5712.0\n",
      "Calculating Recall: correct_by_relation= 3620.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.375%\n",
      "   Recall (micro): 56.046%\n",
      "       F1 (micro): 59.486%\n",
      "epoch 97: train_loss = 0.022650, dev_loss = 0.963218, dev_f1 = 0.5949\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_97.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:09:58.857751: step 145600/300200 (epoch 98/200), loss = 0.005109 (0.159 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:11:07.990656: step 146000/300200 (epoch 98/200), loss = 0.022208 (0.160 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:12:16.986196: step 146400/300200 (epoch 98/200), loss = 0.011387 (0.175 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:13:25.046248: step 146800/300200 (epoch 98/200), loss = 0.037264 (0.185 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3602.0 / guessed_by_relation= 5591.0\n",
      "Calculating Recall: correct_by_relation= 3602.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.425%\n",
      "   Recall (micro): 55.767%\n",
      "       F1 (micro): 59.784%\n",
      "epoch 98: train_loss = 0.022578, dev_loss = 0.943695, dev_f1 = 0.5978\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_98.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:14:55.902943: step 147200/300200 (epoch 99/200), loss = 0.001015 (0.174 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:16:05.447945: step 147600/300200 (epoch 99/200), loss = 0.019500 (0.191 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:17:15.067143: step 148000/300200 (epoch 99/200), loss = 0.009863 (0.162 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:18:23.432006: step 148400/300200 (epoch 99/200), loss = 0.008261 (0.166 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3650.0 / guessed_by_relation= 5711.0\n",
      "Calculating Recall: correct_by_relation= 3650.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.912%\n",
      "   Recall (micro): 56.510%\n",
      "       F1 (micro): 59.984%\n",
      "epoch 99: train_loss = 0.021249, dev_loss = 0.948999, dev_f1 = 0.5998\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_99.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:19:54.874258: step 148800/300200 (epoch 100/200), loss = 0.022170 (0.161 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:21:04.175612: step 149200/300200 (epoch 100/200), loss = 0.118436 (0.161 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:22:13.248357: step 149600/300200 (epoch 100/200), loss = 0.005993 (0.167 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:23:22.111545: step 150000/300200 (epoch 100/200), loss = 0.004729 (0.178 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3651.0 / guessed_by_relation= 5698.0\n",
      "Calculating Recall: correct_by_relation= 3651.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.075%\n",
      "   Recall (micro): 56.526%\n",
      "       F1 (micro): 60.064%\n",
      "epoch 100: train_loss = 0.022357, dev_loss = 0.967757, dev_f1 = 0.6006\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_100.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:24:53.713222: step 150400/300200 (epoch 101/200), loss = 0.002689 (0.175 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:26:02.967450: step 150800/300200 (epoch 101/200), loss = 0.019047 (0.177 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:27:11.508782: step 151200/300200 (epoch 101/200), loss = 0.065912 (0.165 sec/batch), lr: 0.034795\n",
      "2018-03-22 15:28:20.508333: step 151600/300200 (epoch 101/200), loss = 0.098258 (0.173 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3678.0 / guessed_by_relation= 5856.0\n",
      "Calculating Recall: correct_by_relation= 3678.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.807%\n",
      "   Recall (micro): 56.944%\n",
      "       F1 (micro): 59.732%\n",
      "epoch 101: train_loss = 0.021513, dev_loss = 0.983126, dev_f1 = 0.5973\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_101.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:29:51.708942: step 152000/300200 (epoch 102/200), loss = 0.001570 (0.163 sec/batch), lr: 0.033055\n",
      "2018-03-22 15:31:00.781688: step 152400/300200 (epoch 102/200), loss = 0.014092 (0.152 sec/batch), lr: 0.033055\n",
      "2018-03-22 15:32:09.905569: step 152800/300200 (epoch 102/200), loss = 0.042568 (0.178 sec/batch), lr: 0.033055\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3572.0 / guessed_by_relation= 5506.0\n",
      "Calculating Recall: correct_by_relation= 3572.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.875%\n",
      "   Recall (micro): 55.303%\n",
      "       F1 (micro): 59.707%\n",
      "epoch 102: train_loss = 0.020762, dev_loss = 0.951742, dev_f1 = 0.5971\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_102.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:33:41.910318: step 153200/300200 (epoch 103/200), loss = 0.037062 (0.173 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:34:50.465687: step 153600/300200 (epoch 103/200), loss = 0.010327 (0.161 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:35:59.841238: step 154000/300200 (epoch 103/200), loss = 0.034777 (0.151 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:37:08.730496: step 154400/300200 (epoch 103/200), loss = 0.021430 (0.184 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3671.0 / guessed_by_relation= 5747.0\n",
      "Calculating Recall: correct_by_relation= 3671.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.877%\n",
      "   Recall (micro): 56.835%\n",
      "       F1 (micro): 60.151%\n",
      "epoch 103: train_loss = 0.020511, dev_loss = 0.978732, dev_f1 = 0.6015\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_103.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:38:40.168737: step 154800/300200 (epoch 104/200), loss = 0.024158 (0.148 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:39:49.358794: step 155200/300200 (epoch 104/200), loss = 0.036632 (0.179 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:40:59.267765: step 155600/300200 (epoch 104/200), loss = 0.002915 (0.173 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:42:08.339508: step 156000/300200 (epoch 104/200), loss = 0.066703 (0.157 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3673.0 / guessed_by_relation= 5671.0\n",
      "Calculating Recall: correct_by_relation= 3673.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.768%\n",
      "   Recall (micro): 56.866%\n",
      "       F1 (micro): 60.561%\n",
      "epoch 104: train_loss = 0.019302, dev_loss = 0.971300, dev_f1 = 0.6056\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_104.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:43:39.487978: step 156400/300200 (epoch 105/200), loss = 0.008065 (0.179 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:44:48.446420: step 156800/300200 (epoch 105/200), loss = 0.001297 (0.198 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:45:58.183934: step 157200/300200 (epoch 105/200), loss = 0.017074 (0.167 sec/batch), lr: 0.031402\n",
      "2018-03-22 15:47:07.265703: step 157600/300200 (epoch 105/200), loss = 0.002089 (0.163 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3618.0 / guessed_by_relation= 5607.0\n",
      "Calculating Recall: correct_by_relation= 3618.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.526%\n",
      "   Recall (micro): 56.015%\n",
      "       F1 (micro): 59.970%\n",
      "epoch 105: train_loss = 0.020257, dev_loss = 0.968282, dev_f1 = 0.5997\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_105.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:48:38.526472: step 158000/300200 (epoch 106/200), loss = 0.020506 (0.170 sec/batch), lr: 0.029832\n",
      "2018-03-22 15:49:47.917064: step 158400/300200 (epoch 106/200), loss = 0.022911 (0.165 sec/batch), lr: 0.029832\n",
      "2018-03-22 15:50:57.351772: step 158800/300200 (epoch 106/200), loss = 0.016582 (0.177 sec/batch), lr: 0.029832\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3746.0 / guessed_by_relation= 5936.0\n",
      "Calculating Recall: correct_by_relation= 3746.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.106%\n",
      "   Recall (micro): 57.997%\n",
      "       F1 (micro): 60.444%\n",
      "epoch 106: train_loss = 0.019663, dev_loss = 0.981697, dev_f1 = 0.6044\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_106.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:52:29.957118: step 159200/300200 (epoch 107/200), loss = 0.060083 (0.172 sec/batch), lr: 0.029832\n",
      "2018-03-22 15:53:37.510823: step 159600/300200 (epoch 107/200), loss = 0.003885 (0.170 sec/batch), lr: 0.029832\n",
      "2018-03-22 15:54:47.277414: step 160000/300200 (epoch 107/200), loss = 0.065472 (0.194 sec/batch), lr: 0.029832\n",
      "2018-03-22 15:55:56.503568: step 160400/300200 (epoch 107/200), loss = 0.007439 (0.195 sec/batch), lr: 0.029832\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3666.0 / guessed_by_relation= 5749.0\n",
      "Calculating Recall: correct_by_relation= 3666.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.768%\n",
      "   Recall (micro): 56.758%\n",
      "       F1 (micro): 60.059%\n",
      "epoch 107: train_loss = 0.019315, dev_loss = 0.969347, dev_f1 = 0.6006\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_107.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 15:57:27.906716: step 160800/300200 (epoch 108/200), loss = 0.021858 (0.161 sec/batch), lr: 0.028340\n",
      "2018-03-22 15:58:36.365829: step 161200/300200 (epoch 108/200), loss = 0.000599 (0.163 sec/batch), lr: 0.028340\n",
      "2018-03-22 15:59:46.056218: step 161600/300200 (epoch 108/200), loss = 0.008128 (0.166 sec/batch), lr: 0.028340\n",
      "2018-03-22 16:00:55.253294: step 162000/300200 (epoch 108/200), loss = 0.021982 (0.169 sec/batch), lr: 0.028340\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3681.0 / guessed_by_relation= 5739.0\n",
      "Calculating Recall: correct_by_relation= 3681.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.140%\n",
      "   Recall (micro): 56.990%\n",
      "       F1 (micro): 60.354%\n",
      "epoch 108: train_loss = 0.019563, dev_loss = 0.984953, dev_f1 = 0.6035\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_108.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:02:26.151098: step 162400/300200 (epoch 109/200), loss = 0.014459 (0.152 sec/batch), lr: 0.028340\n",
      "2018-03-22 16:03:35.705124: step 162800/300200 (epoch 109/200), loss = 0.007037 (0.174 sec/batch), lr: 0.028340\n",
      "2018-03-22 16:04:45.357411: step 163200/300200 (epoch 109/200), loss = 0.005223 (0.149 sec/batch), lr: 0.028340\n",
      "2018-03-22 16:05:54.598605: step 163600/300200 (epoch 109/200), loss = 0.008614 (0.192 sec/batch), lr: 0.028340\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3735.0 / guessed_by_relation= 5925.0\n",
      "Calculating Recall: correct_by_relation= 3735.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.038%\n",
      "   Recall (micro): 57.826%\n",
      "       F1 (micro): 60.320%\n",
      "epoch 109: train_loss = 0.019169, dev_loss = 1.010737, dev_f1 = 0.6032\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_109.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:07:24.595010: step 164000/300200 (epoch 110/200), loss = 0.004802 (0.174 sec/batch), lr: 0.026923\n",
      "2018-03-22 16:08:34.221229: step 164400/300200 (epoch 110/200), loss = 0.069596 (0.184 sec/batch), lr: 0.026923\n",
      "2018-03-22 16:09:43.535617: step 164800/300200 (epoch 110/200), loss = 0.004865 (0.169 sec/batch), lr: 0.026923\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3688.0 / guessed_by_relation= 5794.0\n",
      "Calculating Recall: correct_by_relation= 3688.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.652%\n",
      "   Recall (micro): 57.099%\n",
      "       F1 (micro): 60.198%\n",
      "epoch 110: train_loss = 0.018041, dev_loss = 0.997579, dev_f1 = 0.6020\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_110.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:11:15.062093: step 165200/300200 (epoch 111/200), loss = 0.007504 (0.178 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:12:22.275893: step 165600/300200 (epoch 111/200), loss = 0.001129 (0.195 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:13:32.207925: step 166000/300200 (epoch 111/200), loss = 0.029066 (0.172 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:14:41.232542: step 166400/300200 (epoch 111/200), loss = 0.009316 (0.161 sec/batch), lr: 0.025577\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3707.0 / guessed_by_relation= 5816.0\n",
      "Calculating Recall: correct_by_relation= 3707.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.738%\n",
      "   Recall (micro): 57.393%\n",
      "       F1 (micro): 60.399%\n",
      "epoch 111: train_loss = 0.018853, dev_loss = 0.995660, dev_f1 = 0.6040\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_111.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:16:13.688491: step 166800/300200 (epoch 112/200), loss = 0.015463 (0.181 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:17:20.960447: step 167200/300200 (epoch 112/200), loss = 0.018312 (0.176 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:18:30.611731: step 167600/300200 (epoch 112/200), loss = 0.002682 (0.168 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:19:40.071506: step 168000/300200 (epoch 112/200), loss = 0.002757 (0.156 sec/batch), lr: 0.025577\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3677.0 / guessed_by_relation= 5711.0\n",
      "Calculating Recall: correct_by_relation= 3677.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.385%\n",
      "   Recall (micro): 56.928%\n",
      "       F1 (micro): 60.427%\n",
      "epoch 112: train_loss = 0.018090, dev_loss = 0.981256, dev_f1 = 0.6043\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_112.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:21:11.450591: step 168400/300200 (epoch 113/200), loss = 0.005619 (0.159 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:22:18.877959: step 168800/300200 (epoch 113/200), loss = 0.001643 (0.178 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:23:28.379846: step 169200/300200 (epoch 113/200), loss = 0.012535 (0.177 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:24:37.299184: step 169600/300200 (epoch 113/200), loss = 0.028840 (0.163 sec/batch), lr: 0.025577\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3697.0 / guessed_by_relation= 5670.0\n",
      "Calculating Recall: correct_by_relation= 3697.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.203%\n",
      "   Recall (micro): 57.238%\n",
      "       F1 (micro): 60.961%\n",
      "epoch 113: train_loss = 0.017848, dev_loss = 0.980028, dev_f1 = 0.6096\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_113.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:26:08.568977: step 170000/300200 (epoch 114/200), loss = 0.002356 (0.131 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:27:16.499684: step 170400/300200 (epoch 114/200), loss = 0.010910 (0.172 sec/batch), lr: 0.025577\n",
      "2018-03-22 16:28:25.775971: step 170800/300200 (epoch 114/200), loss = 0.035535 (0.175 sec/batch), lr: 0.025577\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3712.0 / guessed_by_relation= 5797.0\n",
      "Calculating Recall: correct_by_relation= 3712.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.033%\n",
      "   Recall (micro): 57.470%\n",
      "       F1 (micro): 60.574%\n",
      "epoch 114: train_loss = 0.016474, dev_loss = 0.995273, dev_f1 = 0.6057\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_114.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:29:57.166085: step 171200/300200 (epoch 115/200), loss = 0.013977 (0.167 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:31:04.515245: step 171600/300200 (epoch 115/200), loss = 0.003847 (0.152 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:32:13.391468: step 172000/300200 (epoch 115/200), loss = 0.045711 (0.196 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:33:22.351915: step 172400/300200 (epoch 115/200), loss = 0.000328 (0.163 sec/batch), lr: 0.024298\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3717.0 / guessed_by_relation= 5795.0\n",
      "Calculating Recall: correct_by_relation= 3717.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.142%\n",
      "   Recall (micro): 57.548%\n",
      "       F1 (micro): 60.666%\n",
      "epoch 115: train_loss = 0.017206, dev_loss = 1.003646, dev_f1 = 0.6067\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_115.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:34:55.149773: step 172800/300200 (epoch 116/200), loss = 0.059557 (0.172 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:36:02.071798: step 173200/300200 (epoch 116/200), loss = 0.001493 (0.178 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:37:11.403231: step 173600/300200 (epoch 116/200), loss = 0.000410 (0.136 sec/batch), lr: 0.024298\n",
      "2018-03-22 16:38:20.721630: step 174000/300200 (epoch 116/200), loss = 0.008769 (0.163 sec/batch), lr: 0.024298\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3679.0 / guessed_by_relation= 5773.0\n",
      "Calculating Recall: correct_by_relation= 3679.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.728%\n",
      "   Recall (micro): 56.959%\n",
      "       F1 (micro): 60.154%\n",
      "epoch 116: train_loss = 0.016731, dev_loss = 1.004193, dev_f1 = 0.6015\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_116.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:39:53.394155: step 174400/300200 (epoch 117/200), loss = 0.044651 (0.180 sec/batch), lr: 0.023083\n",
      "2018-03-22 16:41:00.374334: step 174800/300200 (epoch 117/200), loss = 0.001326 (0.165 sec/batch), lr: 0.023083\n",
      "2018-03-22 16:42:09.582440: step 175200/300200 (epoch 117/200), loss = 0.014814 (0.166 sec/batch), lr: 0.023083\n",
      "2018-03-22 16:43:19.048231: step 175600/300200 (epoch 117/200), loss = 0.012718 (0.164 sec/batch), lr: 0.023083\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3666.0 / guessed_by_relation= 5733.0\n",
      "Calculating Recall: correct_by_relation= 3666.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.946%\n",
      "   Recall (micro): 56.758%\n",
      "       F1 (micro): 60.138%\n",
      "epoch 117: train_loss = 0.017329, dev_loss = 0.991719, dev_f1 = 0.6014\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_117.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:44:51.955380: step 176000/300200 (epoch 118/200), loss = 0.010884 (0.173 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:45:59.557212: step 176400/300200 (epoch 118/200), loss = 0.002558 (0.180 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:47:08.882630: step 176800/300200 (epoch 118/200), loss = 0.014706 (0.154 sec/batch), lr: 0.021929\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3629.0 / guessed_by_relation= 5605.0\n",
      "Calculating Recall: correct_by_relation= 3629.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.746%\n",
      "   Recall (micro): 56.185%\n",
      "       F1 (micro): 60.162%\n",
      "epoch 118: train_loss = 0.017175, dev_loss = 0.999377, dev_f1 = 0.6016\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_118.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:48:41.692521: step 177200/300200 (epoch 119/200), loss = 0.004460 (0.173 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:49:51.491197: step 177600/300200 (epoch 119/200), loss = 0.002299 (0.185 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:50:58.716027: step 178000/300200 (epoch 119/200), loss = 0.000526 (0.167 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:52:08.347258: step 178400/300200 (epoch 119/200), loss = 0.007135 (0.182 sec/batch), lr: 0.021929\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3743.0 / guessed_by_relation= 5781.0\n",
      "Calculating Recall: correct_by_relation= 3743.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.747%\n",
      "   Recall (micro): 57.950%\n",
      "       F1 (micro): 61.160%\n",
      "epoch 119: train_loss = 0.016573, dev_loss = 0.993998, dev_f1 = 0.6116\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_119.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:53:41.128072: step 178800/300200 (epoch 120/200), loss = 0.043985 (0.198 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:54:49.664390: step 179200/300200 (epoch 120/200), loss = 0.022231 (0.137 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:55:57.567023: step 179600/300200 (epoch 120/200), loss = 0.039322 (0.182 sec/batch), lr: 0.021929\n",
      "2018-03-22 16:57:07.054872: step 180000/300200 (epoch 120/200), loss = 0.003423 (0.161 sec/batch), lr: 0.021929\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3687.0 / guessed_by_relation= 5738.0\n",
      "Calculating Recall: correct_by_relation= 3687.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.256%\n",
      "   Recall (micro): 57.083%\n",
      "       F1 (micro): 60.457%\n",
      "epoch 120: train_loss = 0.016397, dev_loss = 1.006038, dev_f1 = 0.6046\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_120.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 16:58:39.946981: step 180400/300200 (epoch 121/200), loss = 0.002589 (0.171 sec/batch), lr: 0.020833\n",
      "2018-03-22 16:59:47.883705: step 180800/300200 (epoch 121/200), loss = 0.002997 (0.128 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:00:55.066423: step 181200/300200 (epoch 121/200), loss = 0.000363 (0.175 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:02:04.903201: step 181600/300200 (epoch 121/200), loss = 0.006327 (0.173 sec/batch), lr: 0.020833\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3633.0 / guessed_by_relation= 5556.0\n",
      "Calculating Recall: correct_by_relation= 3633.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.389%\n",
      "   Recall (micro): 56.247%\n",
      "       F1 (micro): 60.474%\n",
      "epoch 121: train_loss = 0.016358, dev_loss = 0.990803, dev_f1 = 0.6047\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_121.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:03:37.729134: step 182000/300200 (epoch 122/200), loss = 0.000397 (0.164 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:04:44.542870: step 182400/300200 (epoch 122/200), loss = 0.023112 (0.154 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:05:54.012672: step 182800/300200 (epoch 122/200), loss = 0.008742 (0.182 sec/batch), lr: 0.020833\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3725.0 / guessed_by_relation= 5814.0\n",
      "Calculating Recall: correct_by_relation= 3725.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.069%\n",
      "   Recall (micro): 57.671%\n",
      "       F1 (micro): 60.702%\n",
      "epoch 122: train_loss = 0.016098, dev_loss = 1.018671, dev_f1 = 0.6070\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_122.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:07:27.004045: step 183200/300200 (epoch 123/200), loss = 0.007193 (0.159 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:08:36.321441: step 183600/300200 (epoch 123/200), loss = 0.000539 (0.161 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:09:43.446004: step 184000/300200 (epoch 123/200), loss = 0.004575 (0.174 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:10:53.227636: step 184400/300200 (epoch 123/200), loss = 0.002238 (0.133 sec/batch), lr: 0.020833\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3744.0 / guessed_by_relation= 5867.0\n",
      "Calculating Recall: correct_by_relation= 3744.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.815%\n",
      "   Recall (micro): 57.966%\n",
      "       F1 (micro): 60.750%\n",
      "epoch 123: train_loss = 0.016093, dev_loss = 1.019921, dev_f1 = 0.6075\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_123.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:12:26.455638: step 184800/300200 (epoch 124/200), loss = 0.008460 (0.166 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:13:35.441151: step 185200/300200 (epoch 124/200), loss = 0.021939 (0.166 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:14:42.663976: step 185600/300200 (epoch 124/200), loss = 0.000918 (0.182 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:15:52.283175: step 186000/300200 (epoch 124/200), loss = 0.004696 (0.181 sec/batch), lr: 0.020833\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3666.0 / guessed_by_relation= 5574.0\n",
      "Calculating Recall: correct_by_relation= 3666.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.770%\n",
      "   Recall (micro): 56.758%\n",
      "       F1 (micro): 60.932%\n",
      "epoch 124: train_loss = 0.015518, dev_loss = 0.987317, dev_f1 = 0.6093\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_124.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:17:24.960714: step 186400/300200 (epoch 125/200), loss = 0.001879 (0.160 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:18:33.397768: step 186800/300200 (epoch 125/200), loss = 0.085518 (0.163 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:19:40.643654: step 187200/300200 (epoch 125/200), loss = 0.008212 (0.158 sec/batch), lr: 0.020833\n",
      "2018-03-22 17:20:50.382170: step 187600/300200 (epoch 125/200), loss = 0.005093 (0.165 sec/batch), lr: 0.020833\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3740.0 / guessed_by_relation= 5840.0\n",
      "Calculating Recall: correct_by_relation= 3740.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.041%\n",
      "   Recall (micro): 57.904%\n",
      "       F1 (micro): 60.818%\n",
      "epoch 125: train_loss = 0.016629, dev_loss = 1.028516, dev_f1 = 0.6082\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_125.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:22:22.256572: step 188000/300200 (epoch 126/200), loss = 0.006487 (0.159 sec/batch), lr: 0.019791\n",
      "2018-03-22 17:23:30.784870: step 188400/300200 (epoch 126/200), loss = 0.014717 (0.150 sec/batch), lr: 0.019791\n",
      "2018-03-22 17:24:38.469923: step 188800/300200 (epoch 126/200), loss = 0.004903 (0.155 sec/batch), lr: 0.019791\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3700.0 / guessed_by_relation= 5732.0\n",
      "Calculating Recall: correct_by_relation= 3700.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.550%\n",
      "   Recall (micro): 57.284%\n",
      "       F1 (micro): 60.701%\n",
      "epoch 126: train_loss = 0.014895, dev_loss = 1.029568, dev_f1 = 0.6070\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_126.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:26:12.127068: step 189200/300200 (epoch 127/200), loss = 0.064640 (0.184 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:27:20.944133: step 189600/300200 (epoch 127/200), loss = 0.007198 (0.176 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:28:29.001177: step 190000/300200 (epoch 127/200), loss = 0.096547 (0.205 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:29:37.597655: step 190400/300200 (epoch 127/200), loss = 0.000307 (0.205 sec/batch), lr: 0.018802\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3696.0 / guessed_by_relation= 5717.0\n",
      "Calculating Recall: correct_by_relation= 3696.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.649%\n",
      "   Recall (micro): 57.222%\n",
      "       F1 (micro): 60.710%\n",
      "epoch 127: train_loss = 0.014849, dev_loss = 1.017217, dev_f1 = 0.6071\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_127.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:31:10.583012: step 190800/300200 (epoch 128/200), loss = 0.000176 (0.160 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:32:19.627683: step 191200/300200 (epoch 128/200), loss = 0.002603 (0.169 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:33:26.876577: step 191600/300200 (epoch 128/200), loss = 0.002252 (0.183 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:34:36.391499: step 192000/300200 (epoch 128/200), loss = 0.003093 (0.162 sec/batch), lr: 0.018802\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3711.0 / guessed_by_relation= 5762.0\n",
      "Calculating Recall: correct_by_relation= 3711.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.405%\n",
      "   Recall (micro): 57.455%\n",
      "       F1 (micro): 60.732%\n",
      "epoch 128: train_loss = 0.015183, dev_loss = 1.029744, dev_f1 = 0.6073\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_128.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:36:08.607810: step 192400/300200 (epoch 129/200), loss = 0.005631 (0.168 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:37:17.391787: step 192800/300200 (epoch 129/200), loss = 0.007612 (0.159 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:38:24.281726: step 193200/300200 (epoch 129/200), loss = 0.045769 (0.173 sec/batch), lr: 0.018802\n",
      "2018-03-22 17:39:34.249854: step 193600/300200 (epoch 129/200), loss = 0.027596 (0.171 sec/batch), lr: 0.018802\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3698.0 / guessed_by_relation= 5733.0\n",
      "Calculating Recall: correct_by_relation= 3698.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.504%\n",
      "   Recall (micro): 57.253%\n",
      "       F1 (micro): 60.663%\n",
      "epoch 129: train_loss = 0.014984, dev_loss = 1.022597, dev_f1 = 0.6066\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_129.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:41:05.750261: step 194000/300200 (epoch 130/200), loss = 0.010829 (0.156 sec/batch), lr: 0.017862\n",
      "2018-03-22 17:42:15.260169: step 194400/300200 (epoch 130/200), loss = 0.116659 (0.180 sec/batch), lr: 0.017862\n",
      "2018-03-22 17:43:22.347633: step 194800/300200 (epoch 130/200), loss = 0.017615 (0.174 sec/batch), lr: 0.017862\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3702.0 / guessed_by_relation= 5679.0\n",
      "Calculating Recall: correct_by_relation= 3702.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.188%\n",
      "   Recall (micro): 57.315%\n",
      "       F1 (micro): 60.999%\n",
      "epoch 130: train_loss = 0.015086, dev_loss = 1.007682, dev_f1 = 0.6100\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_130.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:44:54.763476: step 195200/300200 (epoch 131/200), loss = 0.006581 (0.177 sec/batch), lr: 0.017862\n",
      "2018-03-22 17:46:04.602259: step 195600/300200 (epoch 131/200), loss = 0.001244 (0.156 sec/batch), lr: 0.017862\n",
      "2018-03-22 17:47:14.002877: step 196000/300200 (epoch 131/200), loss = 0.000850 (0.195 sec/batch), lr: 0.017862\n",
      "2018-03-22 17:48:21.159525: step 196400/300200 (epoch 131/200), loss = 0.002116 (0.177 sec/batch), lr: 0.017862\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3679.0 / guessed_by_relation= 5693.0\n",
      "Calculating Recall: correct_by_relation= 3679.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.623%\n",
      "   Recall (micro): 56.959%\n",
      "       F1 (micro): 60.550%\n",
      "epoch 131: train_loss = 0.014855, dev_loss = 1.016414, dev_f1 = 0.6055\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_131.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:49:53.727773: step 196800/300200 (epoch 132/200), loss = 0.014823 (0.175 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:51:02.936881: step 197200/300200 (epoch 132/200), loss = 0.002721 (0.158 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:52:11.950469: step 197600/300200 (epoch 132/200), loss = 0.001687 (0.140 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:53:19.588398: step 198000/300200 (epoch 132/200), loss = 0.019908 (0.117 sec/batch), lr: 0.016968\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3687.0 / guessed_by_relation= 5692.0\n",
      "Calculating Recall: correct_by_relation= 3687.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.775%\n",
      "   Recall (micro): 57.083%\n",
      "       F1 (micro): 60.686%\n",
      "epoch 132: train_loss = 0.014568, dev_loss = 1.003690, dev_f1 = 0.6069\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_132.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:54:52.419344: step 198400/300200 (epoch 133/200), loss = 0.001782 (0.167 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:56:01.358736: step 198800/300200 (epoch 133/200), loss = 0.017110 (0.183 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:57:09.612301: step 199200/300200 (epoch 133/200), loss = 0.001905 (0.158 sec/batch), lr: 0.016968\n",
      "2018-03-22 17:58:17.914999: step 199600/300200 (epoch 133/200), loss = 0.006463 (0.182 sec/batch), lr: 0.016968\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3659.0 / guessed_by_relation= 5583.0\n",
      "Calculating Recall: correct_by_relation= 3659.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.538%\n",
      "   Recall (micro): 56.650%\n",
      "       F1 (micro): 60.771%\n",
      "epoch 133: train_loss = 0.014623, dev_loss = 1.006166, dev_f1 = 0.6077\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_133.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 17:59:51.254297: step 200000/300200 (epoch 134/200), loss = 0.000711 (0.175 sec/batch), lr: 0.016968\n",
      "2018-03-22 18:01:00.382189: step 200400/300200 (epoch 134/200), loss = 0.004546 (0.155 sec/batch), lr: 0.016968\n",
      "2018-03-22 18:02:06.706624: step 200800/300200 (epoch 134/200), loss = 0.008995 (0.156 sec/batch), lr: 0.016968\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3712.0 / guessed_by_relation= 5781.0\n",
      "Calculating Recall: correct_by_relation= 3712.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.210%\n",
      "   Recall (micro): 57.470%\n",
      "       F1 (micro): 60.654%\n",
      "epoch 134: train_loss = 0.013763, dev_loss = 1.032180, dev_f1 = 0.6065\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_134.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:03:38.993122: step 201200/300200 (epoch 135/200), loss = 0.000685 (0.193 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:04:48.411788: step 201600/300200 (epoch 135/200), loss = 0.004780 (0.153 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:05:57.406325: step 202000/300200 (epoch 135/200), loss = 0.072878 (0.176 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:07:03.652552: step 202400/300200 (epoch 135/200), loss = 0.222932 (0.179 sec/batch), lr: 0.016120\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3710.0 / guessed_by_relation= 5706.0\n",
      "Calculating Recall: correct_by_relation= 3710.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.019%\n",
      "   Recall (micro): 57.439%\n",
      "       F1 (micro): 60.995%\n",
      "epoch 135: train_loss = 0.014620, dev_loss = 1.024444, dev_f1 = 0.6099\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_135.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:08:37.086101: step 202800/300200 (epoch 136/200), loss = 0.061534 (0.177 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:09:46.472681: step 203200/300200 (epoch 136/200), loss = 0.008337 (0.173 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:10:55.826173: step 203600/300200 (epoch 136/200), loss = 0.001666 (0.163 sec/batch), lr: 0.016120\n",
      "2018-03-22 18:12:02.249872: step 204000/300200 (epoch 136/200), loss = 0.036973 (0.163 sec/batch), lr: 0.016120\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3674.0 / guessed_by_relation= 5680.0\n",
      "Calculating Recall: correct_by_relation= 3674.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.683%\n",
      "   Recall (micro): 56.882%\n",
      "       F1 (micro): 60.532%\n",
      "epoch 136: train_loss = 0.015166, dev_loss = 1.016559, dev_f1 = 0.6053\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_136.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:13:35.603208: step 204400/300200 (epoch 137/200), loss = 0.028918 (0.146 sec/batch), lr: 0.015314\n",
      "2018-03-22 18:14:44.441330: step 204800/300200 (epoch 137/200), loss = 0.000211 (0.154 sec/batch), lr: 0.015314\n",
      "2018-03-22 18:15:53.147099: step 205200/300200 (epoch 137/200), loss = 0.002699 (0.162 sec/batch), lr: 0.015314\n",
      "2018-03-22 18:17:00.246595: step 205600/300200 (epoch 137/200), loss = 0.024280 (0.172 sec/batch), lr: 0.015314\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3723.0 / guessed_by_relation= 5756.0\n",
      "Calculating Recall: correct_by_relation= 3723.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.680%\n",
      "   Recall (micro): 57.641%\n",
      "       F1 (micro): 60.958%\n",
      "epoch 137: train_loss = 0.014175, dev_loss = 1.026310, dev_f1 = 0.6096\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_137.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:18:32.953212: step 206000/300200 (epoch 138/200), loss = 0.000870 (0.175 sec/batch), lr: 0.015314\n",
      "2018-03-22 18:19:42.123216: step 206400/300200 (epoch 138/200), loss = 0.004186 (0.176 sec/batch), lr: 0.015314\n",
      "2018-03-22 18:20:50.549241: step 206800/300200 (epoch 138/200), loss = 0.004969 (0.159 sec/batch), lr: 0.015314\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3717.0 / guessed_by_relation= 5762.0\n",
      "Calculating Recall: correct_by_relation= 3717.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.509%\n",
      "   Recall (micro): 57.548%\n",
      "       F1 (micro): 60.830%\n",
      "epoch 138: train_loss = 0.014609, dev_loss = 1.027214, dev_f1 = 0.6083\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_138.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:22:21.688688: step 207200/300200 (epoch 139/200), loss = 0.021187 (0.176 sec/batch), lr: 0.014548\n",
      "2018-03-22 18:23:31.396121: step 207600/300200 (epoch 139/200), loss = 0.039640 (0.191 sec/batch), lr: 0.014548\n",
      "2018-03-22 18:24:40.300418: step 208000/300200 (epoch 139/200), loss = 0.002311 (0.154 sec/batch), lr: 0.014548\n",
      "2018-03-22 18:25:48.124843: step 208400/300200 (epoch 139/200), loss = 0.001904 (0.155 sec/batch), lr: 0.014548\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3709.0 / guessed_by_relation= 5774.0\n",
      "Calculating Recall: correct_by_relation= 3709.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.236%\n",
      "   Recall (micro): 57.424%\n",
      "       F1 (micro): 60.639%\n",
      "epoch 139: train_loss = 0.013952, dev_loss = 1.035390, dev_f1 = 0.6064\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_139.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:27:20.113550: step 208800/300200 (epoch 140/200), loss = 0.000113 (0.167 sec/batch), lr: 0.013821\n",
      "2018-03-22 18:28:29.536226: step 209200/300200 (epoch 140/200), loss = 0.090632 (0.216 sec/batch), lr: 0.013821\n",
      "2018-03-22 18:29:38.464587: step 209600/300200 (epoch 140/200), loss = 0.002594 (0.178 sec/batch), lr: 0.013821\n",
      "2018-03-22 18:30:45.724510: step 210000/300200 (epoch 140/200), loss = 0.007078 (0.176 sec/batch), lr: 0.013821\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3652.0 / guessed_by_relation= 5639.0\n",
      "Calculating Recall: correct_by_relation= 3652.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.763%\n",
      "   Recall (micro): 56.541%\n",
      "       F1 (micro): 60.374%\n",
      "epoch 140: train_loss = 0.012773, dev_loss = 1.027531, dev_f1 = 0.6037\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_140.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:32:19.284067: step 210400/300200 (epoch 141/200), loss = 0.003770 (0.179 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:33:24.182033: step 210800/300200 (epoch 141/200), loss = 0.000615 (0.128 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:34:32.260879: step 211200/300200 (epoch 141/200), loss = 0.002047 (0.110 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:35:16.236875: step 211600/300200 (epoch 141/200), loss = 0.008939 (0.123 sec/batch), lr: 0.013130\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3682.0 / guessed_by_relation= 5717.0\n",
      "Calculating Recall: correct_by_relation= 3682.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.404%\n",
      "   Recall (micro): 57.006%\n",
      "       F1 (micro): 60.480%\n",
      "epoch 141: train_loss = 0.013697, dev_loss = 1.035777, dev_f1 = 0.6048\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_141.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:36:19.682923: step 212000/300200 (epoch 142/200), loss = 0.017435 (0.108 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:37:05.791960: step 212400/300200 (epoch 142/200), loss = 0.001516 (0.124 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:37:53.339828: step 212800/300200 (epoch 142/200), loss = 0.003224 (0.096 sec/batch), lr: 0.013130\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3739.0 / guessed_by_relation= 5827.0\n",
      "Calculating Recall: correct_by_relation= 3739.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.167%\n",
      "   Recall (micro): 57.888%\n",
      "       F1 (micro): 60.866%\n",
      "epoch 142: train_loss = 0.013514, dev_loss = 1.045559, dev_f1 = 0.6087\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_142.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:38:53.014573: step 213200/300200 (epoch 143/200), loss = 0.001651 (0.120 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:39:38.554717: step 213600/300200 (epoch 143/200), loss = 0.005462 (0.116 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:40:24.165049: step 214000/300200 (epoch 143/200), loss = 0.005523 (0.115 sec/batch), lr: 0.013130\n",
      "2018-03-22 18:41:09.577855: step 214400/300200 (epoch 143/200), loss = 0.003699 (0.102 sec/batch), lr: 0.013130\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3697.0 / guessed_by_relation= 5747.0\n",
      "Calculating Recall: correct_by_relation= 3697.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.329%\n",
      "   Recall (micro): 57.238%\n",
      "       F1 (micro): 60.577%\n",
      "epoch 143: train_loss = 0.013913, dev_loss = 1.046162, dev_f1 = 0.6058\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_143.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:42:08.951800: step 214800/300200 (epoch 144/200), loss = 0.026801 (0.101 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:42:54.530047: step 215200/300200 (epoch 144/200), loss = 0.008736 (0.126 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:43:40.064175: step 215600/300200 (epoch 144/200), loss = 0.003621 (0.117 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:44:24.701919: step 216000/300200 (epoch 144/200), loss = 0.068810 (0.094 sec/batch), lr: 0.012473\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3723.0 / guessed_by_relation= 5751.0\n",
      "Calculating Recall: correct_by_relation= 3723.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.737%\n",
      "   Recall (micro): 57.641%\n",
      "       F1 (micro): 60.983%\n",
      "epoch 144: train_loss = 0.013420, dev_loss = 1.039334, dev_f1 = 0.6098\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_144.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:45:24.882008: step 216400/300200 (epoch 145/200), loss = 0.003128 (0.118 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:46:10.522420: step 216800/300200 (epoch 145/200), loss = 0.010528 (0.123 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:46:56.250063: step 217200/300200 (epoch 145/200), loss = 0.001270 (0.088 sec/batch), lr: 0.012473\n",
      "2018-03-22 18:47:40.089684: step 217600/300200 (epoch 145/200), loss = 0.017185 (0.108 sec/batch), lr: 0.012473\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3700.0 / guessed_by_relation= 5697.0\n",
      "Calculating Recall: correct_by_relation= 3700.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.946%\n",
      "   Recall (micro): 57.284%\n",
      "       F1 (micro): 60.875%\n",
      "epoch 145: train_loss = 0.012712, dev_loss = 1.040066, dev_f1 = 0.6088\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_145.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:48:41.897103: step 218000/300200 (epoch 146/200), loss = 0.064838 (0.106 sec/batch), lr: 0.011850\n",
      "2018-03-22 18:49:28.034857: step 218400/300200 (epoch 146/200), loss = 0.010658 (0.123 sec/batch), lr: 0.011850\n",
      "2018-03-22 18:50:13.892365: step 218800/300200 (epoch 146/200), loss = 0.001006 (0.113 sec/batch), lr: 0.011850\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3733.0 / guessed_by_relation= 5836.0\n",
      "Calculating Recall: correct_by_relation= 3733.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.965%\n",
      "   Recall (micro): 57.795%\n",
      "       F1 (micro): 60.724%\n",
      "epoch 146: train_loss = 0.013913, dev_loss = 1.046730, dev_f1 = 0.6072\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_146.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:51:13.669981: step 219200/300200 (epoch 147/200), loss = 0.009489 (0.110 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:51:59.371212: step 219600/300200 (epoch 147/200), loss = 0.003366 (0.108 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:52:45.292424: step 220000/300200 (epoch 147/200), loss = 0.000456 (0.119 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:53:30.950114: step 220400/300200 (epoch 147/200), loss = 0.009745 (0.108 sec/batch), lr: 0.011257\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3709.0 / guessed_by_relation= 5732.0\n",
      "Calculating Recall: correct_by_relation= 3709.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.707%\n",
      "   Recall (micro): 57.424%\n",
      "       F1 (micro): 60.848%\n",
      "epoch 147: train_loss = 0.012488, dev_loss = 1.043889, dev_f1 = 0.6085\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_147.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:54:30.679528: step 220800/300200 (epoch 148/200), loss = 0.001287 (0.119 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:55:16.413580: step 221200/300200 (epoch 148/200), loss = 0.004381 (0.103 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:56:02.644781: step 221600/300200 (epoch 148/200), loss = 0.003033 (0.116 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:56:47.687451: step 222000/300200 (epoch 148/200), loss = 0.012734 (0.104 sec/batch), lr: 0.011257\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3747.0 / guessed_by_relation= 5813.0\n",
      "Calculating Recall: correct_by_relation= 3747.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.459%\n",
      "   Recall (micro): 58.012%\n",
      "       F1 (micro): 61.066%\n",
      "epoch 148: train_loss = 0.013994, dev_loss = 1.055570, dev_f1 = 0.6107\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_148.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 18:57:49.438474: step 222400/300200 (epoch 149/200), loss = 0.000530 (0.108 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:58:36.371421: step 222800/300200 (epoch 149/200), loss = 0.002733 (0.114 sec/batch), lr: 0.011257\n",
      "2018-03-22 18:59:23.559337: step 223200/300200 (epoch 149/200), loss = 0.005808 (0.112 sec/batch), lr: 0.011257\n",
      "2018-03-22 19:00:08.294382: step 223600/300200 (epoch 149/200), loss = 0.000994 (0.109 sec/batch), lr: 0.011257\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3703.0 / guessed_by_relation= 5710.0\n",
      "Calculating Recall: correct_by_relation= 3703.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.851%\n",
      "   Recall (micro): 57.331%\n",
      "       F1 (micro): 60.860%\n",
      "epoch 149: train_loss = 0.012564, dev_loss = 1.045049, dev_f1 = 0.6086\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_149.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:01:11.595438: step 224000/300200 (epoch 150/200), loss = 0.001519 (0.123 sec/batch), lr: 0.010694\n",
      "2018-03-22 19:01:57.084849: step 224400/300200 (epoch 150/200), loss = 0.067663 (0.113 sec/batch), lr: 0.010694\n",
      "2018-03-22 19:02:42.700280: step 224800/300200 (epoch 150/200), loss = 0.010838 (0.120 sec/batch), lr: 0.010694\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3754.0 / guessed_by_relation= 5930.0\n",
      "Calculating Recall: correct_by_relation= 3754.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.305%\n",
      "   Recall (micro): 58.120%\n",
      "       F1 (micro): 60.602%\n",
      "epoch 150: train_loss = 0.012781, dev_loss = 1.079106, dev_f1 = 0.6060\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_150.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:03:24.776689: step 225200/300200 (epoch 151/200), loss = 0.004582 (0.056 sec/batch), lr: 0.010160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-82670023530b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    \n",
    "    print(\n",
    "        \"Current params: \"+ \" heads-\"+ str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "        \" drop-\"+ str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\"+ str(opt[\"lr\"]),\n",
    "        \" lr_decay-\"+ str(opt[\"lr_decay\"]) + \" grad_norm-\"+ str(opt[\"max_grad_norm\"])\n",
    "    )\n",
    "    print(\n",
    "        \" weight_no_rel-\"+ str(opt[\"weight_no_rel\"]) +\n",
    "        \" weight_rest-\"+ str(opt[\"weight_rest\"]) + \" attn-\"+ str(opt[\"attn\"]) +\" attn_dim-\"+ str(opt[\"attn_dim\"]),\n",
    "        \" obj_sub_pos-\"+ str(opt[\"obj_sub_pos\"])\n",
    "    )\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam', 'nadam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout\n",
    "\n",
    "# prev\n",
    "# max norm = 3.0, 3.0 weights, 3 encoders, new residual, !batch norm!\n",
    "# prev\n",
    "# max norm = 1.0, 0.3 weights, 2 encoders, new residual, !batch norm!\n",
    "# prev\n",
    "# max norm = 1.0, 2.0 weights, 3 encoders, new residual, !batch norm!, scaled=0.1, dropout 0.6\n",
    "# prev, 61.8 in test\n",
    "# max norm = 1.0, 2.0 weights, 3 encoders, new residual, !batch norm!, scaled=0.1, dropout 0.6, 400 epochs, 0.5 lr\n",
    "\n",
    "# prev, test 60.5, 65% rec\n",
    "# lr 0.3, drop 0.7, max 0.1, batchn, scaled 0.1, 3 enc, w 0.2\n",
    "\n",
    "# prev\n",
    "# lr 0.3, drop 0.6, max 0.1, batchn, scaled 0.3, 4 enc, w 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mindel 63% training\n",
    "# 4 encoders, 0.3 lr, 0.95 decay, 1.0 maxnorm, drop 0.6, scaled 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
