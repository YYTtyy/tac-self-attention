{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurrent params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\\n weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
    " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=3, help='Num of lstm layers.')\n",
    "# encoder layers\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=2, help='Num of self-attention encoders.')\n",
    "parser.add_argument('--dropout', type=float, default=0.4, help='Input and attn dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')        # 0.1 original\n",
    "\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=1.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=0.3, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "parser.add_argument('--obj_sub_pos', dest='obj_sub_pos', action='store_true', \n",
    "    help='In self-attention add obj/subg positional vectors.', default=False)\n",
    "parser.add_argument('--use_batch_norm', dest='use_batch_norm', action='store_true', \n",
    "    help='BatchNorm if true, else LayerNorm in self-attention.', default=False)\n",
    "parser.add_argument('--relative_positions', dest='relative_positions', action='store_true', \n",
    "    help='Use relative positions for subj/obj positional vectors.', default=True)\n",
    "parser.add_argument('--new_residual', dest='new_residual', action='store_true', \n",
    "    help='Use a different residual connection than in usual self-attention.', default=True)\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=3, help='Number of self-attention heads.')\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.3, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=10, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='39_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/39_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/39_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 3\n",
      "\tnum_layers_encoder : 2\n",
      "\tdropout : 0.4\n",
      "\tscaled_dropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 1.0\n",
      "\tweight_rest : 0.3\n",
      "\tself_att : True\n",
      "\tobj_sub_pos : False\n",
      "\tuse_batch_norm : False\n",
      "\trelative_positions : True\n",
      "\tnew_residual : True\n",
      "\tn_head : 3\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.3\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 1.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 10\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 39_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/39_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  3\n",
      "d_v and d_k:  120.0\n",
      "Finetune all embeddings.\n",
      "Using weights [1.0, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-24 17:05:54.061803: step 400/300200 (epoch 1/200), loss = 0.310703 (0.086 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:06:30.639455: step 800/300200 (epoch 1/200), loss = 0.313410 (0.092 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:07:05.342750: step 1200/300200 (epoch 1/200), loss = 0.289224 (0.074 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2571.0 / guessed_by_relation= 5821.0\n",
      "Calculating Recall: correct_by_relation= 2571.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 44.168%\n",
      "   Recall (micro): 39.805%\n",
      "       F1 (micro): 41.873%\n",
      "epoch 1: train_loss = 0.351838, dev_loss = 0.473196, dev_f1 = 0.4187\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:07:54.177636: step 1600/300200 (epoch 2/200), loss = 0.263046 (0.085 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:08:27.737026: step 2000/300200 (epoch 2/200), loss = 0.351319 (0.087 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:09:02.513663: step 2400/300200 (epoch 2/200), loss = 0.274323 (0.091 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:09:37.100619: step 2800/300200 (epoch 2/200), loss = 0.373924 (0.089 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3880.0 / guessed_by_relation= 9676.0\n",
      "Calculating Recall: correct_by_relation= 3880.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 40.099%\n",
      "   Recall (micro): 60.071%\n",
      "       F1 (micro): 48.094%\n",
      "epoch 2: train_loss = 0.272841, dev_loss = 0.615794, dev_f1 = 0.4809\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:10:24.995866: step 3200/300200 (epoch 3/200), loss = 0.263642 (0.040 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:11:00.392975: step 3600/300200 (epoch 3/200), loss = 0.231999 (0.087 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:11:35.239621: step 4000/300200 (epoch 3/200), loss = 0.176991 (0.085 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:12:10.086626: step 4400/300200 (epoch 3/200), loss = 0.274830 (0.079 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3795.0 / guessed_by_relation= 8579.0\n",
      "Calculating Recall: correct_by_relation= 3795.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 44.236%\n",
      "   Recall (micro): 58.755%\n",
      "       F1 (micro): 50.472%\n",
      "epoch 3: train_loss = 0.256389, dev_loss = 0.513090, dev_f1 = 0.5047\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:12:58.827753: step 4800/300200 (epoch 4/200), loss = 0.195133 (0.084 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:13:34.441846: step 5200/300200 (epoch 4/200), loss = 0.329487 (0.083 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:14:09.779924: step 5600/300200 (epoch 4/200), loss = 0.415046 (0.093 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:14:44.945248: step 6000/300200 (epoch 4/200), loss = 0.169902 (0.096 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3999.0 / guessed_by_relation= 8461.0\n",
      "Calculating Recall: correct_by_relation= 3999.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.264%\n",
      "   Recall (micro): 61.914%\n",
      "       F1 (micro): 53.606%\n",
      "epoch 4: train_loss = 0.245620, dev_loss = 0.512363, dev_f1 = 0.5361\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:15:32.972576: step 6400/300200 (epoch 5/200), loss = 0.242945 (0.093 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:16:07.749035: step 6800/300200 (epoch 5/200), loss = 0.209514 (0.089 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:16:42.420215: step 7200/300200 (epoch 5/200), loss = 0.247713 (0.083 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3757.0 / guessed_by_relation= 7080.0\n",
      "Calculating Recall: correct_by_relation= 3757.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.065%\n",
      "   Recall (micro): 58.167%\n",
      "       F1 (micro): 55.499%\n",
      "epoch 5: train_loss = 0.235081, dev_loss = 0.432391, dev_f1 = 0.5550\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:17:29.791881: step 7600/300200 (epoch 6/200), loss = 0.206266 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:18:04.522219: step 8000/300200 (epoch 6/200), loss = 0.138790 (0.091 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:18:39.348811: step 8400/300200 (epoch 6/200), loss = 0.288489 (0.086 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:19:14.010966: step 8800/300200 (epoch 6/200), loss = 0.212064 (0.076 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3967.0 / guessed_by_relation= 7484.0\n",
      "Calculating Recall: correct_by_relation= 3967.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.006%\n",
      "   Recall (micro): 61.418%\n",
      "       F1 (micro): 56.903%\n",
      "epoch 6: train_loss = 0.226259, dev_loss = 0.452962, dev_f1 = 0.5690\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:20:01.476927: step 9200/300200 (epoch 7/200), loss = 0.283416 (0.095 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:20:36.271433: step 9600/300200 (epoch 7/200), loss = 0.118616 (0.087 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:21:11.102477: step 10000/300200 (epoch 7/200), loss = 0.190118 (0.084 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:21:45.865901: step 10400/300200 (epoch 7/200), loss = 0.095710 (0.081 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3685.0 / guessed_by_relation= 6176.0\n",
      "Calculating Recall: correct_by_relation= 3685.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.666%\n",
      "   Recall (micro): 57.052%\n",
      "       F1 (micro): 58.330%\n",
      "epoch 7: train_loss = 0.220460, dev_loss = 0.371800, dev_f1 = 0.5833\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:22:33.188821: step 10800/300200 (epoch 8/200), loss = 0.167874 (0.084 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:23:07.983328: step 11200/300200 (epoch 8/200), loss = 0.345335 (0.091 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:23:42.915794: step 11600/300200 (epoch 8/200), loss = 0.199604 (0.082 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:24:17.802546: step 12000/300200 (epoch 8/200), loss = 0.145701 (0.081 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3947.0 / guessed_by_relation= 6939.0\n",
      "Calculating Recall: correct_by_relation= 3947.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.881%\n",
      "   Recall (micro): 61.109%\n",
      "       F1 (micro): 58.919%\n",
      "epoch 8: train_loss = 0.216442, dev_loss = 0.410816, dev_f1 = 0.5892\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:25:05.445212: step 12400/300200 (epoch 9/200), loss = 0.207444 (0.085 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:25:40.318930: step 12800/300200 (epoch 9/200), loss = 0.150426 (0.088 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:26:14.942984: step 13200/300200 (epoch 9/200), loss = 0.263760 (0.085 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3929.0 / guessed_by_relation= 6703.0\n",
      "Calculating Recall: correct_by_relation= 3929.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.616%\n",
      "   Recall (micro): 60.830%\n",
      "       F1 (micro): 59.702%\n",
      "epoch 9: train_loss = 0.212844, dev_loss = 0.386762, dev_f1 = 0.5970\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:27:03.203552: step 13600/300200 (epoch 10/200), loss = 0.256253 (0.087 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:27:38.106413: step 14000/300200 (epoch 10/200), loss = 0.259756 (0.098 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:28:13.021241: step 14400/300200 (epoch 10/200), loss = 0.149238 (0.090 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:28:47.792687: step 14800/300200 (epoch 10/200), loss = 0.266825 (0.073 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3804.0 / guessed_by_relation= 6281.0\n",
      "Calculating Recall: correct_by_relation= 3804.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.564%\n",
      "   Recall (micro): 58.895%\n",
      "       F1 (micro): 59.717%\n",
      "epoch 10: train_loss = 0.208764, dev_loss = 0.362043, dev_f1 = 0.5972\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:29:35.038297: step 15200/300200 (epoch 11/200), loss = 0.301536 (0.074 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:30:09.766629: step 15600/300200 (epoch 11/200), loss = 0.253198 (0.093 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:30:44.574959: step 16000/300200 (epoch 11/200), loss = 0.178117 (0.090 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:31:19.395536: step 16400/300200 (epoch 11/200), loss = 0.155022 (0.089 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4329.0 / guessed_by_relation= 7872.0\n",
      "Calculating Recall: correct_by_relation= 4329.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.992%\n",
      "   Recall (micro): 67.023%\n",
      "       F1 (micro): 60.414%\n",
      "epoch 11: train_loss = 0.206837, dev_loss = 0.468408, dev_f1 = 0.6041\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:32:06.562644: step 16800/300200 (epoch 12/200), loss = 0.309794 (0.086 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:32:41.345119: step 17200/300200 (epoch 12/200), loss = 0.164391 (0.082 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:33:16.119503: step 17600/300200 (epoch 12/200), loss = 0.151043 (0.090 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:33:50.845202: step 18000/300200 (epoch 12/200), loss = 0.176995 (0.081 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3952.0 / guessed_by_relation= 6485.0\n",
      "Calculating Recall: correct_by_relation= 3952.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.941%\n",
      "   Recall (micro): 61.186%\n",
      "       F1 (micro): 61.063%\n",
      "epoch 12: train_loss = 0.204094, dev_loss = 0.371385, dev_f1 = 0.6106\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:34:37.474175: step 18400/300200 (epoch 13/200), loss = 0.298635 (0.087 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:35:12.342879: step 18800/300200 (epoch 13/200), loss = 0.148554 (0.081 sec/batch), lr: 0.300000\n",
      "2018-03-24 17:35:46.934847: step 19200/300200 (epoch 13/200), loss = 0.135328 (0.083 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4284.0 / guessed_by_relation= 7629.0\n",
      "Calculating Recall: correct_by_relation= 4284.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.154%\n",
      "   Recall (micro): 66.326%\n",
      "       F1 (micro): 60.818%\n",
      "epoch 13: train_loss = 0.201394, dev_loss = 0.445231, dev_f1 = 0.6082\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:36:33.629094: step 19600/300200 (epoch 14/200), loss = 0.184498 (0.077 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:37:08.403151: step 20000/300200 (epoch 14/200), loss = 0.169156 (0.087 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:37:43.235760: step 20400/300200 (epoch 14/200), loss = 0.220522 (0.092 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:38:17.883877: step 20800/300200 (epoch 14/200), loss = 0.168512 (0.082 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3780.0 / guessed_by_relation= 5787.0\n",
      "Calculating Recall: correct_by_relation= 3780.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.319%\n",
      "   Recall (micro): 58.523%\n",
      "       F1 (micro): 61.734%\n",
      "epoch 14: train_loss = 0.198788, dev_loss = 0.337960, dev_f1 = 0.6173\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:39:04.858326: step 21200/300200 (epoch 15/200), loss = 0.223863 (0.082 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:39:39.614732: step 21600/300200 (epoch 15/200), loss = 0.169151 (0.079 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:40:14.508503: step 22000/300200 (epoch 15/200), loss = 0.156632 (0.069 sec/batch), lr: 0.285000\n",
      "2018-03-24 17:40:49.324066: step 22400/300200 (epoch 15/200), loss = 0.254492 (0.089 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3925.0 / guessed_by_relation= 6329.0\n",
      "Calculating Recall: correct_by_relation= 3925.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.016%\n",
      "   Recall (micro): 60.768%\n",
      "       F1 (micro): 61.386%\n",
      "epoch 15: train_loss = 0.197633, dev_loss = 0.360396, dev_f1 = 0.6139\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:41:35.813667: step 22800/300200 (epoch 16/200), loss = 0.147373 (0.077 sec/batch), lr: 0.270750\n",
      "2018-03-24 17:42:10.581103: step 23200/300200 (epoch 16/200), loss = 0.178586 (0.085 sec/batch), lr: 0.270750\n",
      "2018-03-24 17:42:45.370597: step 23600/300200 (epoch 16/200), loss = 0.205440 (0.077 sec/batch), lr: 0.270750\n",
      "2018-03-24 17:43:19.875465: step 24000/300200 (epoch 16/200), loss = 0.192878 (0.086 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4161.0 / guessed_by_relation= 6768.0\n",
      "Calculating Recall: correct_by_relation= 4161.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.480%\n",
      "   Recall (micro): 64.422%\n",
      "       F1 (micro): 62.917%\n",
      "epoch 16: train_loss = 0.195102, dev_loss = 0.383116, dev_f1 = 0.6292\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:44:07.620806: step 24400/300200 (epoch 17/200), loss = 0.143661 (0.082 sec/batch), lr: 0.270750\n",
      "2018-03-24 17:44:42.454047: step 24800/300200 (epoch 17/200), loss = 0.174602 (0.088 sec/batch), lr: 0.270750\n",
      "2018-03-24 17:45:17.119211: step 25200/300200 (epoch 17/200), loss = 0.222350 (0.080 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3838.0 / guessed_by_relation= 5852.0\n",
      "Calculating Recall: correct_by_relation= 3838.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 65.584%\n",
      "   Recall (micro): 59.421%\n",
      "       F1 (micro): 62.351%\n",
      "epoch 17: train_loss = 0.193056, dev_loss = 0.336903, dev_f1 = 0.6235\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:46:04.302905: step 25600/300200 (epoch 18/200), loss = 0.163648 (0.090 sec/batch), lr: 0.257212\n",
      "2018-03-24 17:46:39.083375: step 26000/300200 (epoch 18/200), loss = 0.156726 (0.080 sec/batch), lr: 0.257212\n",
      "2018-03-24 17:47:13.922000: step 26400/300200 (epoch 18/200), loss = 0.200118 (0.082 sec/batch), lr: 0.257212\n",
      "2018-03-24 17:47:48.663942: step 26800/300200 (epoch 18/200), loss = 0.217768 (0.076 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4323.0 / guessed_by_relation= 7434.0\n",
      "Calculating Recall: correct_by_relation= 4323.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.152%\n",
      "   Recall (micro): 66.930%\n",
      "       F1 (micro): 62.233%\n",
      "epoch 18: train_loss = 0.191124, dev_loss = 0.423662, dev_f1 = 0.6223\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:48:35.459376: step 27200/300200 (epoch 19/200), loss = 0.276483 (0.080 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:49:10.156624: step 27600/300200 (epoch 19/200), loss = 0.081736 (0.072 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:49:44.979207: step 28000/300200 (epoch 19/200), loss = 0.092494 (0.077 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:50:19.706535: step 28400/300200 (epoch 19/200), loss = 0.224684 (0.074 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4149.0 / guessed_by_relation= 6789.0\n",
      "Calculating Recall: correct_by_relation= 4149.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.114%\n",
      "   Recall (micro): 64.236%\n",
      "       F1 (micro): 62.636%\n",
      "epoch 19: train_loss = 0.188672, dev_loss = 0.384483, dev_f1 = 0.6264\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:51:06.939633: step 28800/300200 (epoch 20/200), loss = 0.297452 (0.073 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:51:41.726119: step 29200/300200 (epoch 20/200), loss = 0.131329 (0.091 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:52:16.513310: step 29600/300200 (epoch 20/200), loss = 0.151029 (0.073 sec/batch), lr: 0.244352\n",
      "2018-03-24 17:52:50.322735: step 30000/300200 (epoch 20/200), loss = 0.176918 (0.046 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4227.0 / guessed_by_relation= 7168.0\n",
      "Calculating Recall: correct_by_relation= 4227.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.970%\n",
      "   Recall (micro): 65.444%\n",
      "       F1 (micro): 62.039%\n",
      "epoch 20: train_loss = 0.187561, dev_loss = 0.411698, dev_f1 = 0.6204\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:53:37.860122: step 30400/300200 (epoch 21/200), loss = 0.110197 (0.091 sec/batch), lr: 0.232134\n",
      "2018-03-24 17:54:12.824079: step 30800/300200 (epoch 21/200), loss = 0.217084 (0.087 sec/batch), lr: 0.232134\n",
      "2018-03-24 17:54:47.676741: step 31200/300200 (epoch 21/200), loss = 0.108461 (0.090 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4230.0 / guessed_by_relation= 7087.0\n",
      "Calculating Recall: correct_by_relation= 4230.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.687%\n",
      "   Recall (micro): 65.490%\n",
      "       F1 (micro): 62.454%\n",
      "epoch 21: train_loss = 0.186039, dev_loss = 0.406238, dev_f1 = 0.6245\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:55:34.801029: step 31600/300200 (epoch 22/200), loss = 0.215190 (0.079 sec/batch), lr: 0.232134\n",
      "2018-03-24 17:56:09.616593: step 32000/300200 (epoch 22/200), loss = 0.116990 (0.080 sec/batch), lr: 0.232134\n",
      "2018-03-24 17:56:44.442183: step 32400/300200 (epoch 22/200), loss = 0.181349 (0.083 sec/batch), lr: 0.232134\n",
      "2018-03-24 17:57:19.214631: step 32800/300200 (epoch 22/200), loss = 0.108980 (0.082 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4115.0 / guessed_by_relation= 6759.0\n",
      "Calculating Recall: correct_by_relation= 4115.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.882%\n",
      "   Recall (micro): 63.710%\n",
      "       F1 (micro): 62.264%\n",
      "epoch 22: train_loss = 0.183944, dev_loss = 0.395028, dev_f1 = 0.6226\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 17:58:06.622448: step 33200/300200 (epoch 23/200), loss = 0.139837 (0.086 sec/batch), lr: 0.220528\n",
      "2018-03-24 17:58:41.547302: step 33600/300200 (epoch 23/200), loss = 0.147380 (0.095 sec/batch), lr: 0.220528\n",
      "2018-03-24 17:59:16.417009: step 34000/300200 (epoch 23/200), loss = 0.208629 (0.089 sec/batch), lr: 0.220528\n",
      "2018-03-24 17:59:50.360357: step 34400/300200 (epoch 23/200), loss = 0.112194 (0.086 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4085.0 / guessed_by_relation= 6605.0\n",
      "Calculating Recall: correct_by_relation= 4085.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.847%\n",
      "   Recall (micro): 63.245%\n",
      "       F1 (micro): 62.538%\n",
      "epoch 23: train_loss = 0.182568, dev_loss = 0.378109, dev_f1 = 0.6254\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:00:38.495332: step 34800/300200 (epoch 24/200), loss = 0.510255 (0.090 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:01:13.225669: step 35200/300200 (epoch 24/200), loss = 0.165286 (0.086 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:01:47.999120: step 35600/300200 (epoch 24/200), loss = 0.171129 (0.085 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:02:21.093088: step 36000/300200 (epoch 24/200), loss = 0.215889 (0.079 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4442.0 / guessed_by_relation= 7709.0\n",
      "Calculating Recall: correct_by_relation= 4442.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.621%\n",
      "   Recall (micro): 68.772%\n",
      "       F1 (micro): 62.705%\n",
      "epoch 24: train_loss = 0.182163, dev_loss = 0.453805, dev_f1 = 0.6270\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:03:09.833674: step 36400/300200 (epoch 25/200), loss = 0.133345 (0.085 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:03:44.627178: step 36800/300200 (epoch 25/200), loss = 0.180846 (0.093 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:04:19.357515: step 37200/300200 (epoch 25/200), loss = 0.150207 (0.083 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4188.0 / guessed_by_relation= 6855.0\n",
      "Calculating Recall: correct_by_relation= 4188.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.094%\n",
      "   Recall (micro): 64.840%\n",
      "       F1 (micro): 62.911%\n",
      "epoch 25: train_loss = 0.181051, dev_loss = 0.390036, dev_f1 = 0.6291\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:05:06.353462: step 37600/300200 (epoch 26/200), loss = 0.159115 (0.079 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:05:41.222166: step 38000/300200 (epoch 26/200), loss = 0.128669 (0.069 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:06:15.941051: step 38400/300200 (epoch 26/200), loss = 0.098412 (0.087 sec/batch), lr: 0.220528\n",
      "2018-03-24 18:06:49.969522: step 38800/300200 (epoch 26/200), loss = 0.165698 (0.087 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4081.0 / guessed_by_relation= 6543.0\n",
      "Calculating Recall: correct_by_relation= 4081.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.372%\n",
      "   Recall (micro): 63.183%\n",
      "       F1 (micro): 62.775%\n",
      "epoch 26: train_loss = 0.179358, dev_loss = 0.376914, dev_f1 = 0.6277\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:07:37.716260: step 39200/300200 (epoch 27/200), loss = 0.075512 (0.088 sec/batch), lr: 0.209501\n",
      "2018-03-24 18:08:12.449555: step 39600/300200 (epoch 27/200), loss = 0.365987 (0.084 sec/batch), lr: 0.209501\n",
      "2018-03-24 18:08:47.182899: step 40000/300200 (epoch 27/200), loss = 0.256952 (0.081 sec/batch), lr: 0.209501\n",
      "2018-03-24 18:09:19.856454: step 40400/300200 (epoch 27/200), loss = 0.240983 (0.093 sec/batch), lr: 0.209501\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4205.0 / guessed_by_relation= 6975.0\n",
      "Calculating Recall: correct_by_relation= 4205.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.287%\n",
      "   Recall (micro): 65.103%\n",
      "       F1 (micro): 62.602%\n",
      "epoch 27: train_loss = 0.178275, dev_loss = 0.415348, dev_f1 = 0.6260\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:10:08.651183: step 40800/300200 (epoch 28/200), loss = 0.207700 (0.077 sec/batch), lr: 0.199026\n",
      "2018-03-24 18:10:43.339604: step 41200/300200 (epoch 28/200), loss = 0.179982 (0.090 sec/batch), lr: 0.199026\n",
      "2018-03-24 18:11:18.208087: step 41600/300200 (epoch 28/200), loss = 0.122581 (0.090 sec/batch), lr: 0.199026\n",
      "2018-03-24 18:11:51.620608: step 42000/300200 (epoch 28/200), loss = 0.206651 (0.092 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4187.0 / guessed_by_relation= 6777.0\n",
      "Calculating Recall: correct_by_relation= 4187.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.782%\n",
      "   Recall (micro): 64.824%\n",
      "       F1 (micro): 63.267%\n",
      "epoch 28: train_loss = 0.176680, dev_loss = 0.393116, dev_f1 = 0.6327\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:12:40.453268: step 42400/300200 (epoch 29/200), loss = 0.242090 (0.077 sec/batch), lr: 0.199026\n",
      "2018-03-24 18:13:15.138484: step 42800/300200 (epoch 29/200), loss = 0.262340 (0.081 sec/batch), lr: 0.199026\n",
      "2018-03-24 18:13:49.247168: step 43200/300200 (epoch 29/200), loss = 0.089511 (0.084 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4186.0 / guessed_by_relation= 6900.0\n",
      "Calculating Recall: correct_by_relation= 4186.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.667%\n",
      "   Recall (micro): 64.809%\n",
      "       F1 (micro): 62.669%\n",
      "epoch 29: train_loss = 0.176458, dev_loss = 0.406212, dev_f1 = 0.6267\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:14:37.526137: step 43600/300200 (epoch 30/200), loss = 0.223360 (0.076 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:15:12.500853: step 44000/300200 (epoch 30/200), loss = 0.226382 (0.082 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:15:47.263725: step 44400/300200 (epoch 30/200), loss = 0.208574 (0.082 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:16:19.970387: step 44800/300200 (epoch 30/200), loss = 0.284184 (0.089 sec/batch), lr: 0.189075\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4037.0 / guessed_by_relation= 6414.0\n",
      "Calculating Recall: correct_by_relation= 4037.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.940%\n",
      "   Recall (micro): 62.502%\n",
      "       F1 (micro): 62.720%\n",
      "epoch 30: train_loss = 0.174711, dev_loss = 0.379831, dev_f1 = 0.6272\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:17:09.394289: step 45200/300200 (epoch 31/200), loss = 0.166000 (0.093 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:17:44.107580: step 45600/300200 (epoch 31/200), loss = 0.205471 (0.086 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:18:18.934906: step 46000/300200 (epoch 31/200), loss = 0.186076 (0.086 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:18:51.813319: step 46400/300200 (epoch 31/200), loss = 0.112950 (0.089 sec/batch), lr: 0.189075\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4268.0 / guessed_by_relation= 7006.0\n",
      "Calculating Recall: correct_by_relation= 4268.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.919%\n",
      "   Recall (micro): 66.078%\n",
      "       F1 (micro): 63.394%\n",
      "epoch 31: train_loss = 0.173619, dev_loss = 0.413042, dev_f1 = 0.6339\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:19:41.032677: step 46800/300200 (epoch 32/200), loss = 0.157101 (0.098 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:20:15.598118: step 47200/300200 (epoch 32/200), loss = 0.255765 (0.090 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:20:49.761445: step 47600/300200 (epoch 32/200), loss = 0.123675 (0.086 sec/batch), lr: 0.189075\n",
      "2018-03-24 18:21:23.534236: step 48000/300200 (epoch 32/200), loss = 0.143294 (0.097 sec/batch), lr: 0.189075\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4274.0 / guessed_by_relation= 7171.0\n",
      "Calculating Recall: correct_by_relation= 4274.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.601%\n",
      "   Recall (micro): 66.171%\n",
      "       F1 (micro): 62.715%\n",
      "epoch 32: train_loss = 0.172409, dev_loss = 0.427697, dev_f1 = 0.6271\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:22:11.966000: step 48400/300200 (epoch 33/200), loss = 0.222543 (0.078 sec/batch), lr: 0.179621\n",
      "2018-03-24 18:22:46.705419: step 48800/300200 (epoch 33/200), loss = 0.326515 (0.084 sec/batch), lr: 0.179621\n",
      "2018-03-24 18:23:19.723139: step 49200/300200 (epoch 33/200), loss = 0.093962 (0.097 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4162.0 / guessed_by_relation= 6752.0\n",
      "Calculating Recall: correct_by_relation= 4162.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.641%\n",
      "   Recall (micro): 64.437%\n",
      "       F1 (micro): 63.008%\n",
      "epoch 33: train_loss = 0.171117, dev_loss = 0.403938, dev_f1 = 0.6301\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:24:08.471290: step 49600/300200 (epoch 34/200), loss = 0.335659 (0.078 sec/batch), lr: 0.179621\n",
      "2018-03-24 18:24:43.332414: step 50000/300200 (epoch 34/200), loss = 0.155488 (0.087 sec/batch), lr: 0.179621\n",
      "2018-03-24 18:25:18.077791: step 50400/300200 (epoch 34/200), loss = 0.113799 (0.080 sec/batch), lr: 0.179621\n",
      "2018-03-24 18:25:51.504662: step 50800/300200 (epoch 34/200), loss = 0.189239 (0.091 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4263.0 / guessed_by_relation= 7089.0\n",
      "Calculating Recall: correct_by_relation= 4263.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.135%\n",
      "   Recall (micro): 66.001%\n",
      "       F1 (micro): 62.932%\n",
      "epoch 34: train_loss = 0.169899, dev_loss = 0.427477, dev_f1 = 0.6293\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:26:40.288275: step 51200/300200 (epoch 35/200), loss = 0.147669 (0.095 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:27:14.976129: step 51600/300200 (epoch 35/200), loss = 0.076195 (0.089 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:27:49.218167: step 52000/300200 (epoch 35/200), loss = 0.145985 (0.069 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:28:23.380995: step 52400/300200 (epoch 35/200), loss = 0.109971 (0.089 sec/batch), lr: 0.170640\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4134.0 / guessed_by_relation= 6580.0\n",
      "Calculating Recall: correct_by_relation= 4134.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.827%\n",
      "   Recall (micro): 64.004%\n",
      "       F1 (micro): 63.410%\n",
      "epoch 35: train_loss = 0.168657, dev_loss = 0.384036, dev_f1 = 0.6341\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:29:12.598849: step 52800/300200 (epoch 36/200), loss = 0.180584 (0.075 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:29:47.301764: step 53200/300200 (epoch 36/200), loss = 0.138414 (0.090 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:30:20.393745: step 53600/300200 (epoch 36/200), loss = 0.172489 (0.071 sec/batch), lr: 0.170640\n",
      "2018-03-24 18:30:55.226354: step 54000/300200 (epoch 36/200), loss = 0.154402 (0.084 sec/batch), lr: 0.170640\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4201.0 / guessed_by_relation= 6811.0\n",
      "Calculating Recall: correct_by_relation= 4201.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.680%\n",
      "   Recall (micro): 65.041%\n",
      "       F1 (micro): 63.316%\n",
      "epoch 36: train_loss = 0.168368, dev_loss = 0.399913, dev_f1 = 0.6332\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:31:44.285736: step 54400/300200 (epoch 37/200), loss = 0.126147 (0.087 sec/batch), lr: 0.162108\n",
      "2018-03-24 18:32:19.116624: step 54800/300200 (epoch 37/200), loss = 0.133484 (0.080 sec/batch), lr: 0.162108\n",
      "2018-03-24 18:32:52.031194: step 55200/300200 (epoch 37/200), loss = 0.224475 (0.083 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4219.0 / guessed_by_relation= 7082.0\n",
      "Calculating Recall: correct_by_relation= 4219.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.574%\n",
      "   Recall (micro): 65.320%\n",
      "       F1 (micro): 62.314%\n",
      "epoch 37: train_loss = 0.166768, dev_loss = 0.427438, dev_f1 = 0.6231\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:33:40.628398: step 55600/300200 (epoch 38/200), loss = 0.095712 (0.078 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:34:15.387813: step 56000/300200 (epoch 38/200), loss = 0.121540 (0.077 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:34:49.568688: step 56400/300200 (epoch 38/200), loss = 0.263213 (0.074 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:35:23.161000: step 56800/300200 (epoch 38/200), loss = 0.104305 (0.093 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4363.0 / guessed_by_relation= 7527.0\n",
      "Calculating Recall: correct_by_relation= 4363.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.965%\n",
      "   Recall (micro): 67.549%\n",
      "       F1 (micro): 62.391%\n",
      "epoch 38: train_loss = 0.165997, dev_loss = 0.473816, dev_f1 = 0.6239\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:36:11.933670: step 57200/300200 (epoch 39/200), loss = 0.123082 (0.082 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:36:46.660999: step 57600/300200 (epoch 39/200), loss = 0.263754 (0.091 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:37:19.632462: step 58000/300200 (epoch 39/200), loss = 0.311618 (0.085 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:37:54.421956: step 58400/300200 (epoch 39/200), loss = 0.193967 (0.087 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4158.0 / guessed_by_relation= 6848.0\n",
      "Calculating Recall: correct_by_relation= 4158.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.718%\n",
      "   Recall (micro): 64.375%\n",
      "       F1 (micro): 62.493%\n",
      "epoch 39: train_loss = 0.165048, dev_loss = 0.414231, dev_f1 = 0.6249\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:38:43.670894: step 58800/300200 (epoch 40/200), loss = 0.192505 (0.079 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:39:18.378169: step 59200/300200 (epoch 40/200), loss = 0.155408 (0.079 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:39:51.644981: step 59600/300200 (epoch 40/200), loss = 0.211092 (0.079 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:40:26.435092: step 60000/300200 (epoch 40/200), loss = 0.246365 (0.086 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4054.0 / guessed_by_relation= 6403.0\n",
      "Calculating Recall: correct_by_relation= 4054.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.314%\n",
      "   Recall (micro): 62.765%\n",
      "       F1 (micro): 63.038%\n",
      "epoch 40: train_loss = 0.165048, dev_loss = 0.384534, dev_f1 = 0.6304\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:41:15.002605: step 60400/300200 (epoch 41/200), loss = 0.201642 (0.086 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:41:49.260686: step 60800/300200 (epoch 41/200), loss = 0.098818 (0.082 sec/batch), lr: 0.154003\n",
      "2018-03-24 18:42:22.904133: step 61200/300200 (epoch 41/200), loss = 0.129962 (0.086 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4261.0 / guessed_by_relation= 7179.0\n",
      "Calculating Recall: correct_by_relation= 4261.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.354%\n",
      "   Recall (micro): 65.970%\n",
      "       F1 (micro): 62.487%\n",
      "epoch 41: train_loss = 0.163757, dev_loss = 0.451583, dev_f1 = 0.6249\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_41.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:43:12.025732: step 61600/300200 (epoch 42/200), loss = 0.133921 (0.087 sec/batch), lr: 0.146302\n",
      "2018-03-24 18:43:46.811215: step 62000/300200 (epoch 42/200), loss = 0.291463 (0.081 sec/batch), lr: 0.146302\n",
      "2018-03-24 18:44:19.843036: step 62400/300200 (epoch 42/200), loss = 0.158381 (0.079 sec/batch), lr: 0.146302\n",
      "2018-03-24 18:44:54.559990: step 62800/300200 (epoch 42/200), loss = 0.122627 (0.087 sec/batch), lr: 0.146302\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4120.0 / guessed_by_relation= 6830.0\n",
      "Calculating Recall: correct_by_relation= 4120.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.322%\n",
      "   Recall (micro): 63.787%\n",
      "       F1 (micro): 62.006%\n",
      "epoch 42: train_loss = 0.162715, dev_loss = 0.422575, dev_f1 = 0.6201\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_42.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:45:43.239413: step 63200/300200 (epoch 43/200), loss = 0.136142 (0.083 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:46:18.052971: step 63600/300200 (epoch 43/200), loss = 0.115325 (0.086 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:46:51.091382: step 64000/300200 (epoch 43/200), loss = 0.217157 (0.095 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:47:25.841772: step 64400/300200 (epoch 43/200), loss = 0.130616 (0.097 sec/batch), lr: 0.138987\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4302.0 / guessed_by_relation= 7221.0\n",
      "Calculating Recall: correct_by_relation= 4302.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.576%\n",
      "   Recall (micro): 66.605%\n",
      "       F1 (micro): 62.895%\n",
      "epoch 43: train_loss = 0.160932, dev_loss = 0.445465, dev_f1 = 0.6289\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_43.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:48:15.452775: step 64800/300200 (epoch 44/200), loss = 0.129788 (0.092 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:48:49.615315: step 65200/300200 (epoch 44/200), loss = 0.144343 (0.079 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:49:23.367050: step 65600/300200 (epoch 44/200), loss = 0.347261 (0.088 sec/batch), lr: 0.138987\n",
      "2018-03-24 18:49:58.241770: step 66000/300200 (epoch 44/200), loss = 0.261814 (0.080 sec/batch), lr: 0.138987\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4396.0 / guessed_by_relation= 7629.0\n",
      "Calculating Recall: correct_by_relation= 4396.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.622%\n",
      "   Recall (micro): 68.060%\n",
      "       F1 (micro): 62.408%\n",
      "epoch 44: train_loss = 0.159507, dev_loss = 0.488064, dev_f1 = 0.6241\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_44.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:50:46.789847: step 66400/300200 (epoch 45/200), loss = 0.175970 (0.095 sec/batch), lr: 0.132038\n",
      "2018-03-24 18:51:19.580025: step 66800/300200 (epoch 45/200), loss = 0.167746 (0.080 sec/batch), lr: 0.132038\n",
      "2018-03-24 18:51:54.448308: step 67200/300200 (epoch 45/200), loss = 0.275036 (0.079 sec/batch), lr: 0.132038\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4039.0 / guessed_by_relation= 6411.0\n",
      "Calculating Recall: correct_by_relation= 4039.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.001%\n",
      "   Recall (micro): 62.533%\n",
      "       F1 (micro): 62.766%\n",
      "epoch 45: train_loss = 0.160840, dev_loss = 0.398527, dev_f1 = 0.6277\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_45.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:52:43.636083: step 67600/300200 (epoch 46/200), loss = 0.161259 (0.091 sec/batch), lr: 0.132038\n",
      "2018-03-24 18:53:18.489747: step 68000/300200 (epoch 46/200), loss = 0.231200 (0.075 sec/batch), lr: 0.132038\n",
      "2018-03-24 18:53:51.973997: step 68400/300200 (epoch 46/200), loss = 0.184003 (0.080 sec/batch), lr: 0.132038\n",
      "2018-03-24 18:54:26.632349: step 68800/300200 (epoch 46/200), loss = 0.224797 (0.079 sec/batch), lr: 0.132038\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4220.0 / guessed_by_relation= 7122.0\n",
      "Calculating Recall: correct_by_relation= 4220.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.253%\n",
      "   Recall (micro): 65.335%\n",
      "       F1 (micro): 62.146%\n",
      "epoch 46: train_loss = 0.157941, dev_loss = 0.456544, dev_f1 = 0.6215\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_46.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:55:15.972528: step 69200/300200 (epoch 47/200), loss = 0.206036 (0.087 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:55:50.132348: step 69600/300200 (epoch 47/200), loss = 0.144850 (0.079 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:56:24.316954: step 70000/300200 (epoch 47/200), loss = 0.203233 (0.082 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:56:59.065237: step 70400/300200 (epoch 47/200), loss = 0.097817 (0.087 sec/batch), lr: 0.125436\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3903.0 / guessed_by_relation= 6033.0\n",
      "Calculating Recall: correct_by_relation= 3903.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.694%\n",
      "   Recall (micro): 60.427%\n",
      "       F1 (micro): 62.488%\n",
      "epoch 47: train_loss = 0.156803, dev_loss = 0.380028, dev_f1 = 0.6249\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_47.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 18:57:47.794541: step 70800/300200 (epoch 48/200), loss = 0.125503 (0.087 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:58:21.605853: step 71200/300200 (epoch 48/200), loss = 0.277173 (0.095 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:58:56.937549: step 71600/300200 (epoch 48/200), loss = 0.035129 (0.086 sec/batch), lr: 0.125436\n",
      "2018-03-24 18:59:31.867700: step 72000/300200 (epoch 48/200), loss = 0.055494 (0.079 sec/batch), lr: 0.125436\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4228.0 / guessed_by_relation= 7060.0\n",
      "Calculating Recall: correct_by_relation= 4228.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.887%\n",
      "   Recall (micro): 65.459%\n",
      "       F1 (micro): 62.549%\n",
      "epoch 48: train_loss = 0.157288, dev_loss = 0.457618, dev_f1 = 0.6255\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_48.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:00:21.136362: step 72400/300200 (epoch 49/200), loss = 0.188524 (0.080 sec/batch), lr: 0.125436\n",
      "2018-03-24 19:00:55.555873: step 72800/300200 (epoch 49/200), loss = 0.295059 (0.094 sec/batch), lr: 0.125436\n",
      "2018-03-24 19:01:31.249135: step 73200/300200 (epoch 49/200), loss = 0.073331 (0.075 sec/batch), lr: 0.125436\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4271.0 / guessed_by_relation= 7281.0\n",
      "Calculating Recall: correct_by_relation= 4271.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.660%\n",
      "   Recall (micro): 66.125%\n",
      "       F1 (micro): 62.169%\n",
      "epoch 49: train_loss = 0.156789, dev_loss = 0.478054, dev_f1 = 0.6217\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_49.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:02:20.952827: step 73600/300200 (epoch 50/200), loss = 0.188178 (0.091 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:02:55.529620: step 74000/300200 (epoch 50/200), loss = 0.138596 (0.067 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:03:29.428913: step 74400/300200 (epoch 50/200), loss = 0.156567 (0.096 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:04:04.330706: step 74800/300200 (epoch 50/200), loss = 0.119195 (0.082 sec/batch), lr: 0.119164\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4199.0 / guessed_by_relation= 6860.0\n",
      "Calculating Recall: correct_by_relation= 4199.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.210%\n",
      "   Recall (micro): 65.010%\n",
      "       F1 (micro): 63.053%\n",
      "epoch 50: train_loss = 0.155204, dev_loss = 0.433515, dev_f1 = 0.6305\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_50.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:04:53.468347: step 75200/300200 (epoch 51/200), loss = 0.083059 (0.089 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:05:27.260759: step 75600/300200 (epoch 51/200), loss = 0.225116 (0.090 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:06:03.075990: step 76000/300200 (epoch 51/200), loss = 0.193754 (0.091 sec/batch), lr: 0.119164\n",
      "2018-03-24 19:06:38.464075: step 76400/300200 (epoch 51/200), loss = 0.169228 (0.084 sec/batch), lr: 0.119164\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4489.0 / guessed_by_relation= 8394.0\n",
      "Calculating Recall: correct_by_relation= 4489.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.479%\n",
      "   Recall (micro): 69.500%\n",
      "       F1 (micro): 60.446%\n",
      "epoch 51: train_loss = 0.154791, dev_loss = 0.588496, dev_f1 = 0.6045\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_51.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:07:28.115632: step 76800/300200 (epoch 52/200), loss = 0.231167 (0.092 sec/batch), lr: 0.113206\n",
      "2018-03-24 19:08:01.550524: step 77200/300200 (epoch 52/200), loss = 0.120614 (0.073 sec/batch), lr: 0.113206\n",
      "2018-03-24 19:08:36.366088: step 77600/300200 (epoch 52/200), loss = 0.163046 (0.079 sec/batch), lr: 0.113206\n",
      "2018-03-24 19:09:11.224766: step 78000/300200 (epoch 52/200), loss = 0.135165 (0.090 sec/batch), lr: 0.113206\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4195.0 / guessed_by_relation= 6977.0\n",
      "Calculating Recall: correct_by_relation= 4195.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.126%\n",
      "   Recall (micro): 64.948%\n",
      "       F1 (micro): 62.444%\n",
      "epoch 52: train_loss = 0.154237, dev_loss = 0.451253, dev_f1 = 0.6244\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_52.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:09:59.468551: step 78400/300200 (epoch 53/200), loss = 0.248975 (0.074 sec/batch), lr: 0.113206\n",
      "2018-03-24 19:10:34.186627: step 78800/300200 (epoch 53/200), loss = 0.160284 (0.094 sec/batch), lr: 0.113206\n",
      "2018-03-24 19:11:10.407278: step 79200/300200 (epoch 53/200), loss = 0.081378 (0.099 sec/batch), lr: 0.113206\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4194.0 / guessed_by_relation= 7026.0\n",
      "Calculating Recall: correct_by_relation= 4194.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.693%\n",
      "   Recall (micro): 64.933%\n",
      "       F1 (micro): 62.202%\n",
      "epoch 53: train_loss = 0.153444, dev_loss = 0.454816, dev_f1 = 0.6220\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_53.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:12:00.692069: step 79600/300200 (epoch 54/200), loss = 0.091569 (0.099 sec/batch), lr: 0.107546\n",
      "2018-03-24 19:12:34.906033: step 80000/300200 (epoch 54/200), loss = 0.175527 (0.081 sec/batch), lr: 0.107546\n",
      "2018-03-24 19:13:10.267046: step 80400/300200 (epoch 54/200), loss = 0.182679 (0.081 sec/batch), lr: 0.107546\n",
      "2018-03-24 19:13:45.798868: step 80800/300200 (epoch 54/200), loss = 0.067761 (0.086 sec/batch), lr: 0.107546\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4425.0 / guessed_by_relation= 8007.0\n",
      "Calculating Recall: correct_by_relation= 4425.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.264%\n",
      "   Recall (micro): 68.509%\n",
      "       F1 (micro): 61.178%\n",
      "epoch 54: train_loss = 0.152151, dev_loss = 0.545779, dev_f1 = 0.6118\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_54.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:14:34.757032: step 81200/300200 (epoch 55/200), loss = 0.077374 (0.073 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:15:08.259253: step 81600/300200 (epoch 55/200), loss = 0.220293 (0.090 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:15:43.116929: step 82000/300200 (epoch 55/200), loss = 0.160647 (0.094 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:16:18.202613: step 82400/300200 (epoch 55/200), loss = 0.152558 (0.078 sec/batch), lr: 0.102168\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4285.0 / guessed_by_relation= 7270.0\n",
      "Calculating Recall: correct_by_relation= 4285.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.941%\n",
      "   Recall (micro): 66.342%\n",
      "       F1 (micro): 62.423%\n",
      "epoch 55: train_loss = 0.152015, dev_loss = 0.483611, dev_f1 = 0.6242\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_55.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:17:07.164759: step 82800/300200 (epoch 56/200), loss = 0.139181 (0.071 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:17:42.011150: step 83200/300200 (epoch 56/200), loss = 0.127466 (0.091 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:18:18.190685: step 83600/300200 (epoch 56/200), loss = 0.121174 (0.084 sec/batch), lr: 0.102168\n",
      "2018-03-24 19:18:53.356179: step 84000/300200 (epoch 56/200), loss = 0.171811 (0.083 sec/batch), lr: 0.102168\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4501.0 / guessed_by_relation= 8182.0\n",
      "Calculating Recall: correct_by_relation= 4501.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.011%\n",
      "   Recall (micro): 69.686%\n",
      "       F1 (micro): 61.485%\n",
      "epoch 56: train_loss = 0.150664, dev_loss = 0.553934, dev_f1 = 0.6148\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_56.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:19:41.874674: step 84400/300200 (epoch 57/200), loss = 0.157275 (0.083 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:20:17.712957: step 84800/300200 (epoch 57/200), loss = 0.258079 (0.074 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:20:53.188277: step 85200/300200 (epoch 57/200), loss = 0.182139 (0.095 sec/batch), lr: 0.097060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4391.0 / guessed_by_relation= 7708.0\n",
      "Calculating Recall: correct_by_relation= 4391.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.967%\n",
      "   Recall (micro): 67.983%\n",
      "       F1 (micro): 61.989%\n",
      "epoch 57: train_loss = 0.149682, dev_loss = 0.527498, dev_f1 = 0.6199\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_57.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:21:42.351991: step 85600/300200 (epoch 58/200), loss = 0.108858 (0.067 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:22:17.116422: step 86000/300200 (epoch 58/200), loss = 0.131998 (0.091 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:22:52.562662: step 86400/300200 (epoch 58/200), loss = 0.185174 (0.089 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:23:27.545670: step 86800/300200 (epoch 58/200), loss = 0.139485 (0.090 sec/batch), lr: 0.097060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4174.0 / guessed_by_relation= 6850.0\n",
      "Calculating Recall: correct_by_relation= 4174.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.934%\n",
      "   Recall (micro): 64.623%\n",
      "       F1 (micro): 62.724%\n",
      "epoch 58: train_loss = 0.150120, dev_loss = 0.450841, dev_f1 = 0.6272\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_58.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:24:16.051131: step 87200/300200 (epoch 59/200), loss = 0.190648 (0.053 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:24:50.963451: step 87600/300200 (epoch 59/200), loss = 0.081653 (0.088 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:25:26.880450: step 88000/300200 (epoch 59/200), loss = 0.258619 (0.086 sec/batch), lr: 0.097060\n",
      "2018-03-24 19:26:02.456536: step 88400/300200 (epoch 59/200), loss = 0.208414 (0.090 sec/batch), lr: 0.097060\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4340.0 / guessed_by_relation= 7449.0\n",
      "Calculating Recall: correct_by_relation= 4340.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.263%\n",
      "   Recall (micro): 67.193%\n",
      "       F1 (micro): 62.410%\n",
      "epoch 59: train_loss = 0.148883, dev_loss = 0.494772, dev_f1 = 0.6241\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_59.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:26:50.550405: step 88800/300200 (epoch 60/200), loss = 0.213958 (0.087 sec/batch), lr: 0.092207\n",
      "2018-03-24 19:27:25.679806: step 89200/300200 (epoch 60/200), loss = 0.238568 (0.085 sec/batch), lr: 0.092207\n",
      "2018-03-24 19:28:01.549176: step 89600/300200 (epoch 60/200), loss = 0.164348 (0.090 sec/batch), lr: 0.092207\n",
      "2018-03-24 19:28:37.279170: step 90000/300200 (epoch 60/200), loss = 0.086276 (0.085 sec/batch), lr: 0.092207\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4406.0 / guessed_by_relation= 7724.0\n",
      "Calculating Recall: correct_by_relation= 4406.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.043%\n",
      "   Recall (micro): 68.215%\n",
      "       F1 (micro): 62.131%\n",
      "epoch 60: train_loss = 0.148086, dev_loss = 0.531063, dev_f1 = 0.6213\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_60.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:29:26.409304: step 90400/300200 (epoch 61/200), loss = 0.046771 (0.088 sec/batch), lr: 0.087597\n",
      "2018-03-24 19:30:01.254446: step 90800/300200 (epoch 61/200), loss = 0.147806 (0.088 sec/batch), lr: 0.087597\n",
      "2018-03-24 19:30:36.183815: step 91200/300200 (epoch 61/200), loss = 0.116788 (0.092 sec/batch), lr: 0.087597\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4192.0 / guessed_by_relation= 7155.0\n",
      "Calculating Recall: correct_by_relation= 4192.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.588%\n",
      "   Recall (micro): 64.902%\n",
      "       F1 (micro): 61.584%\n",
      "epoch 61: train_loss = 0.147411, dev_loss = 0.486733, dev_f1 = 0.6158\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_61.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:31:23.833500: step 91600/300200 (epoch 62/200), loss = 0.171863 (0.050 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:31:58.594920: step 92000/300200 (epoch 62/200), loss = 0.124088 (0.091 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:32:33.876730: step 92400/300200 (epoch 62/200), loss = 0.275045 (0.096 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:33:09.080325: step 92800/300200 (epoch 62/200), loss = 0.159381 (0.094 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4352.0 / guessed_by_relation= 7454.0\n",
      "Calculating Recall: correct_by_relation= 4352.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.385%\n",
      "   Recall (micro): 67.379%\n",
      "       F1 (micro): 62.560%\n",
      "epoch 62: train_loss = 0.145910, dev_loss = 0.497437, dev_f1 = 0.6256\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_62.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:33:56.955112: step 93200/300200 (epoch 63/200), loss = 0.115069 (0.091 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:34:31.819304: step 93600/300200 (epoch 63/200), loss = 0.147698 (0.079 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:35:06.780755: step 94000/300200 (epoch 63/200), loss = 0.171273 (0.085 sec/batch), lr: 0.083217\n",
      "2018-03-24 19:35:41.572760: step 94400/300200 (epoch 63/200), loss = 0.124041 (0.083 sec/batch), lr: 0.083217\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4438.0 / guessed_by_relation= 7865.0\n",
      "Calculating Recall: correct_by_relation= 4438.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.427%\n",
      "   Recall (micro): 68.710%\n",
      "       F1 (micro): 61.966%\n",
      "epoch 63: train_loss = 0.145542, dev_loss = 0.542704, dev_f1 = 0.6197\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_63.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:36:29.460077: step 94800/300200 (epoch 64/200), loss = 0.130683 (0.086 sec/batch), lr: 0.079056\n",
      "2018-03-24 19:37:04.380920: step 95200/300200 (epoch 64/200), loss = 0.197061 (0.080 sec/batch), lr: 0.079056\n",
      "2018-03-24 19:37:39.470716: step 95600/300200 (epoch 64/200), loss = 0.064662 (0.089 sec/batch), lr: 0.079056\n",
      "2018-03-24 19:38:14.609140: step 96000/300200 (epoch 64/200), loss = 0.116602 (0.088 sec/batch), lr: 0.079056\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4386.0 / guessed_by_relation= 7632.0\n",
      "Calculating Recall: correct_by_relation= 4386.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.469%\n",
      "   Recall (micro): 67.905%\n",
      "       F1 (micro): 62.253%\n",
      "epoch 64: train_loss = 0.143360, dev_loss = 0.523591, dev_f1 = 0.6225\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_64.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:39:01.888344: step 96400/300200 (epoch 65/200), loss = 0.098610 (0.074 sec/batch), lr: 0.079056\n",
      "2018-03-24 19:39:36.786627: step 96800/300200 (epoch 65/200), loss = 0.223349 (0.081 sec/batch), lr: 0.079056\n",
      "2018-03-24 19:40:11.673380: step 97200/300200 (epoch 65/200), loss = 0.193793 (0.088 sec/batch), lr: 0.079056\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4245.0 / guessed_by_relation= 7181.0\n",
      "Calculating Recall: correct_by_relation= 4245.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.114%\n",
      "   Recall (micro): 65.722%\n",
      "       F1 (micro): 62.243%\n",
      "epoch 65: train_loss = 0.144489, dev_loss = 0.493297, dev_f1 = 0.6224\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_65.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:40:58.529956: step 97600/300200 (epoch 66/200), loss = 0.255302 (0.093 sec/batch), lr: 0.075103\n",
      "2018-03-24 19:41:33.541042: step 98000/300200 (epoch 66/200), loss = 0.120515 (0.083 sec/batch), lr: 0.075103\n",
      "2018-03-24 19:42:08.582206: step 98400/300200 (epoch 66/200), loss = 0.134733 (0.085 sec/batch), lr: 0.075103\n",
      "2018-03-24 19:42:43.346140: step 98800/300200 (epoch 66/200), loss = 0.088609 (0.076 sec/batch), lr: 0.075103\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4411.0 / guessed_by_relation= 7756.0\n",
      "Calculating Recall: correct_by_relation= 4411.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.872%\n",
      "   Recall (micro): 68.292%\n",
      "       F1 (micro): 62.061%\n",
      "epoch 66: train_loss = 0.143215, dev_loss = 0.545206, dev_f1 = 0.6206\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_66.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:43:31.128687: step 99200/300200 (epoch 67/200), loss = 0.170466 (0.078 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:44:06.517782: step 99600/300200 (epoch 67/200), loss = 0.206970 (0.075 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:44:41.992598: step 100000/300200 (epoch 67/200), loss = 0.134617 (0.090 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:45:17.356626: step 100400/300200 (epoch 67/200), loss = 0.065514 (0.089 sec/batch), lr: 0.071348\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4404.0 / guessed_by_relation= 7650.0\n",
      "Calculating Recall: correct_by_relation= 4404.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.569%\n",
      "   Recall (micro): 68.184%\n",
      "       F1 (micro): 62.428%\n",
      "epoch 67: train_loss = 0.142776, dev_loss = 0.523656, dev_f1 = 0.6243\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_67.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:46:04.942642: step 100800/300200 (epoch 68/200), loss = 0.212130 (0.082 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:46:39.764221: step 101200/300200 (epoch 68/200), loss = 0.170709 (0.081 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:47:14.685566: step 101600/300200 (epoch 68/200), loss = 0.096110 (0.080 sec/batch), lr: 0.071348\n",
      "2018-03-24 19:47:49.724226: step 102000/300200 (epoch 68/200), loss = 0.108553 (0.075 sec/batch), lr: 0.071348\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4378.0 / guessed_by_relation= 7639.0\n",
      "Calculating Recall: correct_by_relation= 4378.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.311%\n",
      "   Recall (micro): 67.781%\n",
      "       F1 (micro): 62.108%\n",
      "epoch 68: train_loss = 0.141743, dev_loss = 0.529646, dev_f1 = 0.6211\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_68.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:48:37.118732: step 102400/300200 (epoch 69/200), loss = 0.143775 (0.083 sec/batch), lr: 0.067781\n",
      "2018-03-24 19:49:12.086704: step 102800/300200 (epoch 69/200), loss = 0.111719 (0.083 sec/batch), lr: 0.067781\n",
      "2018-03-24 19:49:47.158451: step 103200/300200 (epoch 69/200), loss = 0.175044 (0.086 sec/batch), lr: 0.067781\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4326.0 / guessed_by_relation= 7518.0\n",
      "Calculating Recall: correct_by_relation= 4326.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.542%\n",
      "   Recall (micro): 66.976%\n",
      "       F1 (micro): 61.902%\n",
      "epoch 69: train_loss = 0.141152, dev_loss = 0.531773, dev_f1 = 0.6190\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_69.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:50:34.814156: step 103600/300200 (epoch 70/200), loss = 0.257497 (0.090 sec/batch), lr: 0.064392\n",
      "2018-03-24 19:51:09.790149: step 104000/300200 (epoch 70/200), loss = 0.294206 (0.085 sec/batch), lr: 0.064392\n",
      "2018-03-24 19:51:44.878437: step 104400/300200 (epoch 70/200), loss = 0.126398 (0.090 sec/batch), lr: 0.064392\n",
      "2018-03-24 19:52:19.744635: step 104800/300200 (epoch 70/200), loss = 0.158560 (0.089 sec/batch), lr: 0.064392\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4452.0 / guessed_by_relation= 8033.0\n",
      "Calculating Recall: correct_by_relation= 4452.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.421%\n",
      "   Recall (micro): 68.927%\n",
      "       F1 (micro): 61.441%\n",
      "epoch 70: train_loss = 0.140766, dev_loss = 0.568936, dev_f1 = 0.6144\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_70.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:53:06.871433: step 105200/300200 (epoch 71/200), loss = 0.091543 (0.089 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:53:41.815338: step 105600/300200 (epoch 71/200), loss = 0.140373 (0.087 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:54:16.832437: step 106000/300200 (epoch 71/200), loss = 0.113067 (0.082 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:54:51.767318: step 106400/300200 (epoch 71/200), loss = 0.163150 (0.088 sec/batch), lr: 0.061172\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4330.0 / guessed_by_relation= 7552.0\n",
      "Calculating Recall: correct_by_relation= 4330.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.336%\n",
      "   Recall (micro): 67.038%\n",
      "       F1 (micro): 61.809%\n",
      "epoch 71: train_loss = 0.140627, dev_loss = 0.539234, dev_f1 = 0.6181\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_71.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:55:38.792349: step 106800/300200 (epoch 72/200), loss = 0.110305 (0.093 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:56:13.792402: step 107200/300200 (epoch 72/200), loss = 0.286144 (0.090 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:56:48.759372: step 107600/300200 (epoch 72/200), loss = 0.098343 (0.076 sec/batch), lr: 0.061172\n",
      "2018-03-24 19:57:23.231522: step 108000/300200 (epoch 72/200), loss = 0.170538 (0.071 sec/batch), lr: 0.061172\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4461.0 / guessed_by_relation= 8009.0\n",
      "Calculating Recall: correct_by_relation= 4461.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.700%\n",
      "   Recall (micro): 69.066%\n",
      "       F1 (micro): 61.667%\n",
      "epoch 72: train_loss = 0.139072, dev_loss = 0.574043, dev_f1 = 0.6167\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_72.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 19:58:10.938363: step 108400/300200 (epoch 73/200), loss = 0.148582 (0.080 sec/batch), lr: 0.058113\n",
      "2018-03-24 19:58:45.916361: step 108800/300200 (epoch 73/200), loss = 0.102839 (0.085 sec/batch), lr: 0.058113\n",
      "2018-03-24 19:59:20.783561: step 109200/300200 (epoch 73/200), loss = 0.125361 (0.085 sec/batch), lr: 0.058113\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4424.0 / guessed_by_relation= 7790.0\n",
      "Calculating Recall: correct_by_relation= 4424.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.791%\n",
      "   Recall (micro): 68.494%\n",
      "       F1 (micro): 62.096%\n",
      "epoch 73: train_loss = 0.138429, dev_loss = 0.551508, dev_f1 = 0.6210\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_73.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:00:08.325961: step 109600/300200 (epoch 74/200), loss = 0.118177 (0.079 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:00:43.306463: step 110000/300200 (epoch 74/200), loss = 0.108960 (0.091 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:01:18.433855: step 110400/300200 (epoch 74/200), loss = 0.176946 (0.086 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:01:53.449450: step 110800/300200 (epoch 74/200), loss = 0.164840 (0.081 sec/batch), lr: 0.058113\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4261.0 / guessed_by_relation= 7186.0\n",
      "Calculating Recall: correct_by_relation= 4261.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.296%\n",
      "   Recall (micro): 65.970%\n",
      "       F1 (micro): 62.455%\n",
      "epoch 74: train_loss = 0.138110, dev_loss = 0.500038, dev_f1 = 0.6246\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_74.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:02:40.422339: step 111200/300200 (epoch 75/200), loss = 0.127193 (0.085 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:03:15.497601: step 111600/300200 (epoch 75/200), loss = 0.122637 (0.089 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:03:50.354778: step 112000/300200 (epoch 75/200), loss = 0.186854 (0.083 sec/batch), lr: 0.058113\n",
      "2018-03-24 20:04:24.717638: step 112400/300200 (epoch 75/200), loss = 0.174848 (0.083 sec/batch), lr: 0.058113\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4487.0 / guessed_by_relation= 8259.0\n",
      "Calculating Recall: correct_by_relation= 4487.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.329%\n",
      "   Recall (micro): 69.469%\n",
      "       F1 (micro): 60.973%\n",
      "epoch 75: train_loss = 0.137735, dev_loss = 0.608655, dev_f1 = 0.6097\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_75.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:05:12.446034: step 112800/300200 (epoch 76/200), loss = 0.226047 (0.086 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:05:47.366376: step 113200/300200 (epoch 76/200), loss = 0.095211 (0.080 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:06:22.406038: step 113600/300200 (epoch 76/200), loss = 0.186284 (0.091 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:06:55.598285: step 114000/300200 (epoch 76/200), loss = 0.061870 (0.085 sec/batch), lr: 0.055208\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4289.0 / guessed_by_relation= 7429.0\n",
      "Calculating Recall: correct_by_relation= 4289.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.733%\n",
      "   Recall (micro): 66.403%\n",
      "       F1 (micro): 61.766%\n",
      "epoch 76: train_loss = 0.136982, dev_loss = 0.526845, dev_f1 = 0.6177\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_76.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:07:44.170427: step 114400/300200 (epoch 77/200), loss = 0.177041 (0.090 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:08:18.896251: step 114800/300200 (epoch 77/200), loss = 0.113145 (0.093 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:08:53.682236: step 115200/300200 (epoch 77/200), loss = 0.132675 (0.093 sec/batch), lr: 0.055208\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4299.0 / guessed_by_relation= 7397.0\n",
      "Calculating Recall: correct_by_relation= 4299.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.118%\n",
      "   Recall (micro): 66.558%\n",
      "       F1 (micro): 62.053%\n",
      "epoch 77: train_loss = 0.137428, dev_loss = 0.518587, dev_f1 = 0.6205\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_77.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:09:40.600480: step 115600/300200 (epoch 78/200), loss = 0.089299 (0.088 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:10:15.526336: step 116000/300200 (epoch 78/200), loss = 0.124473 (0.079 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:10:50.417100: step 116400/300200 (epoch 78/200), loss = 0.153880 (0.083 sec/batch), lr: 0.055208\n",
      "2018-03-24 20:11:24.802018: step 116800/300200 (epoch 78/200), loss = 0.148139 (0.080 sec/batch), lr: 0.055208\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4365.0 / guessed_by_relation= 7716.0\n",
      "Calculating Recall: correct_by_relation= 4365.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.571%\n",
      "   Recall (micro): 67.580%\n",
      "       F1 (micro): 61.587%\n",
      "epoch 78: train_loss = 0.136042, dev_loss = 0.561305, dev_f1 = 0.6159\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_78.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:12:12.139373: step 117200/300200 (epoch 79/200), loss = 0.131291 (0.098 sec/batch), lr: 0.052447\n",
      "2018-03-24 20:12:47.040167: step 117600/300200 (epoch 79/200), loss = 0.136557 (0.081 sec/batch), lr: 0.052447\n",
      "2018-03-24 20:13:21.960011: step 118000/300200 (epoch 79/200), loss = 0.089495 (0.088 sec/batch), lr: 0.052447\n",
      "2018-03-24 20:13:55.488154: step 118400/300200 (epoch 79/200), loss = 0.100792 (0.092 sec/batch), lr: 0.052447\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4589.0 / guessed_by_relation= 8761.0\n",
      "Calculating Recall: correct_by_relation= 4589.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.380%\n",
      "   Recall (micro): 71.048%\n",
      "       F1 (micro): 60.302%\n",
      "epoch 79: train_loss = 0.135803, dev_loss = 0.674436, dev_f1 = 0.6030\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_79.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:14:44.602744: step 118800/300200 (epoch 80/200), loss = 0.180793 (0.075 sec/batch), lr: 0.049825\n",
      "2018-03-24 20:15:19.490500: step 119200/300200 (epoch 80/200), loss = 0.133935 (0.085 sec/batch), lr: 0.049825\n",
      "2018-03-24 20:15:54.459475: step 119600/300200 (epoch 80/200), loss = 0.110339 (0.083 sec/batch), lr: 0.049825\n",
      "2018-03-24 20:16:27.900885: step 120000/300200 (epoch 80/200), loss = 0.091129 (0.083 sec/batch), lr: 0.049825\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4308.0 / guessed_by_relation= 7483.0\n",
      "Calculating Recall: correct_by_relation= 4308.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.570%\n",
      "   Recall (micro): 66.698%\n",
      "       F1 (micro): 61.799%\n",
      "epoch 80: train_loss = 0.134738, dev_loss = 0.539985, dev_f1 = 0.6180\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_80.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:17:16.665534: step 120400/300200 (epoch 81/200), loss = 0.173474 (0.087 sec/batch), lr: 0.049825\n",
      "2018-03-24 20:17:51.567326: step 120800/300200 (epoch 81/200), loss = 0.216544 (0.089 sec/batch), lr: 0.049825\n",
      "2018-03-24 20:18:26.177845: step 121200/300200 (epoch 81/200), loss = 0.099939 (0.086 sec/batch), lr: 0.049825\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4441.0 / guessed_by_relation= 8005.0\n",
      "Calculating Recall: correct_by_relation= 4441.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.478%\n",
      "   Recall (micro): 68.757%\n",
      "       F1 (micro): 61.408%\n",
      "epoch 81: train_loss = 0.134719, dev_loss = 0.596294, dev_f1 = 0.6141\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_81.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:19:13.407914: step 121600/300200 (epoch 82/200), loss = 0.145457 (0.083 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:19:48.480160: step 122000/300200 (epoch 82/200), loss = 0.130799 (0.089 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:20:23.400502: step 122400/300200 (epoch 82/200), loss = 0.158642 (0.093 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:20:56.758192: step 122800/300200 (epoch 82/200), loss = 0.106456 (0.049 sec/batch), lr: 0.047334\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4407.0 / guessed_by_relation= 7777.0\n",
      "Calculating Recall: correct_by_relation= 4407.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.667%\n",
      "   Recall (micro): 68.230%\n",
      "       F1 (micro): 61.913%\n",
      "epoch 82: train_loss = 0.134521, dev_loss = 0.573816, dev_f1 = 0.6191\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_82.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:21:45.169910: step 123200/300200 (epoch 83/200), loss = 0.132688 (0.082 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:22:20.186508: step 123600/300200 (epoch 83/200), loss = 0.147395 (0.089 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:22:55.111362: step 124000/300200 (epoch 83/200), loss = 0.074071 (0.088 sec/batch), lr: 0.047334\n",
      "2018-03-24 20:23:28.485593: step 124400/300200 (epoch 83/200), loss = 0.058490 (0.089 sec/batch), lr: 0.047334\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4462.0 / guessed_by_relation= 8023.0\n",
      "Calculating Recall: correct_by_relation= 4462.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.615%\n",
      "   Recall (micro): 69.082%\n",
      "       F1 (micro): 61.621%\n",
      "epoch 83: train_loss = 0.133832, dev_loss = 0.597231, dev_f1 = 0.6162\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_83.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:24:17.580625: step 124800/300200 (epoch 84/200), loss = 0.141239 (0.089 sec/batch), lr: 0.044967\n",
      "2018-03-24 20:24:52.415740: step 125200/300200 (epoch 84/200), loss = 0.344014 (0.079 sec/batch), lr: 0.044967\n",
      "2018-03-24 20:25:27.356640: step 125600/300200 (epoch 84/200), loss = 0.057046 (0.062 sec/batch), lr: 0.044967\n",
      "2018-03-24 20:26:00.460653: step 126000/300200 (epoch 84/200), loss = 0.120032 (0.096 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4375.0 / guessed_by_relation= 7640.0\n",
      "Calculating Recall: correct_by_relation= 4375.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.264%\n",
      "   Recall (micro): 67.735%\n",
      "       F1 (micro): 62.061%\n",
      "epoch 84: train_loss = 0.133225, dev_loss = 0.559847, dev_f1 = 0.6206\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_84.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:26:49.266415: step 126400/300200 (epoch 85/200), loss = 0.191557 (0.082 sec/batch), lr: 0.044967\n",
      "2018-03-24 20:27:24.076969: step 126800/300200 (epoch 85/200), loss = 0.225904 (0.075 sec/batch), lr: 0.044967\n",
      "2018-03-24 20:27:58.125998: step 127200/300200 (epoch 85/200), loss = 0.070735 (0.079 sec/batch), lr: 0.044967\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4300.0 / guessed_by_relation= 7402.0\n",
      "Calculating Recall: correct_by_relation= 4300.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.092%\n",
      "   Recall (micro): 66.574%\n",
      "       F1 (micro): 62.045%\n",
      "epoch 85: train_loss = 0.132935, dev_loss = 0.541734, dev_f1 = 0.6204\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_85.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:28:46.346200: step 127600/300200 (epoch 86/200), loss = 0.076880 (0.082 sec/batch), lr: 0.042719\n",
      "2018-03-24 20:29:21.436003: step 128000/300200 (epoch 86/200), loss = 0.071574 (0.093 sec/batch), lr: 0.042719\n",
      "2018-03-24 20:29:56.408486: step 128400/300200 (epoch 86/200), loss = 0.263124 (0.083 sec/batch), lr: 0.042719\n",
      "2018-03-24 20:30:29.780214: step 128800/300200 (epoch 86/200), loss = 0.098067 (0.089 sec/batch), lr: 0.042719\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4345.0 / guessed_by_relation= 7558.0\n",
      "Calculating Recall: correct_by_relation= 4345.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.489%\n",
      "   Recall (micro): 67.270%\n",
      "       F1 (micro): 61.996%\n",
      "epoch 86: train_loss = 0.132202, dev_loss = 0.551848, dev_f1 = 0.6200\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_86.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:31:19.318425: step 129200/300200 (epoch 87/200), loss = 0.095537 (0.091 sec/batch), lr: 0.040583\n",
      "2018-03-24 20:31:54.217209: step 129600/300200 (epoch 87/200), loss = 0.342750 (0.092 sec/batch), lr: 0.040583\n",
      "2018-03-24 20:32:29.141075: step 130000/300200 (epoch 87/200), loss = 0.131998 (0.072 sec/batch), lr: 0.040583\n",
      "2018-03-24 20:33:02.122261: step 130400/300200 (epoch 87/200), loss = 0.055018 (0.087 sec/batch), lr: 0.040583\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4433.0 / guessed_by_relation= 7861.0\n",
      "Calculating Recall: correct_by_relation= 4433.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.392%\n",
      "   Recall (micro): 68.633%\n",
      "       F1 (micro): 61.913%\n",
      "epoch 87: train_loss = 0.132361, dev_loss = 0.581988, dev_f1 = 0.6191\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_87.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:33:51.057366: step 130800/300200 (epoch 88/200), loss = 0.161401 (0.084 sec/batch), lr: 0.038554\n",
      "2018-03-24 20:34:25.840346: step 131200/300200 (epoch 88/200), loss = 0.129900 (0.083 sec/batch), lr: 0.038554\n",
      "2018-03-24 20:35:00.037265: step 131600/300200 (epoch 88/200), loss = 0.116104 (0.069 sec/batch), lr: 0.038554\n",
      "2018-03-24 20:35:34.039172: step 132000/300200 (epoch 88/200), loss = 0.073039 (0.082 sec/batch), lr: 0.038554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4448.0 / guessed_by_relation= 7992.0\n",
      "Calculating Recall: correct_by_relation= 4448.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.656%\n",
      "   Recall (micro): 68.865%\n",
      "       F1 (micro): 61.560%\n",
      "epoch 88: train_loss = 0.130242, dev_loss = 0.595558, dev_f1 = 0.6156\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_88.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:36:22.919131: step 132400/300200 (epoch 89/200), loss = 0.096249 (0.080 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:36:57.778310: step 132800/300200 (epoch 89/200), loss = 0.180162 (0.085 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:37:30.800606: step 133200/300200 (epoch 89/200), loss = 0.083058 (0.086 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4476.0 / guessed_by_relation= 8008.0\n",
      "Calculating Recall: correct_by_relation= 4476.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.894%\n",
      "   Recall (micro): 69.299%\n",
      "       F1 (micro): 61.879%\n",
      "epoch 89: train_loss = 0.130567, dev_loss = 0.600216, dev_f1 = 0.6188\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_89.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:38:20.003921: step 133600/300200 (epoch 90/200), loss = 0.155693 (0.084 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:38:55.077671: step 134000/300200 (epoch 90/200), loss = 0.077183 (0.090 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:39:30.114325: step 134400/300200 (epoch 90/200), loss = 0.082929 (0.084 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:40:03.103038: step 134800/300200 (epoch 90/200), loss = 0.155714 (0.092 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4477.0 / guessed_by_relation= 7960.0\n",
      "Calculating Recall: correct_by_relation= 4477.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.244%\n",
      "   Recall (micro): 69.314%\n",
      "       F1 (micro): 62.099%\n",
      "epoch 90: train_loss = 0.131052, dev_loss = 0.596503, dev_f1 = 0.6210\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_90.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:40:52.524934: step 135200/300200 (epoch 91/200), loss = 0.166356 (0.088 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:41:27.388632: step 135600/300200 (epoch 91/200), loss = 0.116883 (0.088 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:42:01.725425: step 136000/300200 (epoch 91/200), loss = 0.132445 (0.076 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:42:35.768438: step 136400/300200 (epoch 91/200), loss = 0.106642 (0.090 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4367.0 / guessed_by_relation= 7504.0\n",
      "Calculating Recall: correct_by_relation= 4367.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.196%\n",
      "   Recall (micro): 67.611%\n",
      "       F1 (micro): 62.551%\n",
      "epoch 91: train_loss = 0.130170, dev_loss = 0.557444, dev_f1 = 0.6255\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_91.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:43:25.485125: step 136800/300200 (epoch 92/200), loss = 0.060899 (0.084 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:44:00.368869: step 137200/300200 (epoch 92/200), loss = 0.082611 (0.086 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:44:33.470880: step 137600/300200 (epoch 92/200), loss = 0.116475 (0.084 sec/batch), lr: 0.036626\n",
      "2018-03-24 20:45:08.509537: step 138000/300200 (epoch 92/200), loss = 0.151599 (0.088 sec/batch), lr: 0.036626\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4439.0 / guessed_by_relation= 7904.0\n",
      "Calculating Recall: correct_by_relation= 4439.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.161%\n",
      "   Recall (micro): 68.726%\n",
      "       F1 (micro): 61.812%\n",
      "epoch 92: train_loss = 0.129883, dev_loss = 0.597331, dev_f1 = 0.6181\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_92.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:45:58.180600: step 138400/300200 (epoch 93/200), loss = 0.159044 (0.086 sec/batch), lr: 0.034795\n",
      "2018-03-24 20:46:33.164610: step 138800/300200 (epoch 93/200), loss = 0.059492 (0.089 sec/batch), lr: 0.034795\n",
      "2018-03-24 20:47:06.151311: step 139200/300200 (epoch 93/200), loss = 0.041602 (0.082 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4459.0 / guessed_by_relation= 7853.0\n",
      "Calculating Recall: correct_by_relation= 4459.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.781%\n",
      "   Recall (micro): 69.035%\n",
      "       F1 (micro): 62.311%\n",
      "epoch 93: train_loss = 0.130396, dev_loss = 0.586280, dev_f1 = 0.6231\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_93.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:47:54.998685: step 139600/300200 (epoch 94/200), loss = 0.179955 (0.078 sec/batch), lr: 0.034795\n",
      "2018-03-24 20:48:29.975179: step 140000/300200 (epoch 94/200), loss = 0.216924 (0.088 sec/batch), lr: 0.034795\n",
      "2018-03-24 20:49:04.292921: step 140400/300200 (epoch 94/200), loss = 0.101216 (0.071 sec/batch), lr: 0.034795\n",
      "2018-03-24 20:49:38.048674: step 140800/300200 (epoch 94/200), loss = 0.118924 (0.085 sec/batch), lr: 0.034795\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4387.0 / guessed_by_relation= 7713.0\n",
      "Calculating Recall: correct_by_relation= 4387.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.878%\n",
      "   Recall (micro): 67.921%\n",
      "       F1 (micro): 61.911%\n",
      "epoch 94: train_loss = 0.129896, dev_loss = 0.579128, dev_f1 = 0.6191\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_94.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:50:27.069505: step 141200/300200 (epoch 95/200), loss = 0.211042 (0.090 sec/batch), lr: 0.033055\n",
      "2018-03-24 20:51:02.011912: step 141600/300200 (epoch 95/200), loss = 0.106514 (0.077 sec/batch), lr: 0.033055\n",
      "2018-03-24 20:51:35.006634: step 142000/300200 (epoch 95/200), loss = 0.177984 (0.086 sec/batch), lr: 0.033055\n",
      "2018-03-24 20:52:09.957056: step 142400/300200 (epoch 95/200), loss = 0.157877 (0.092 sec/batch), lr: 0.033055\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4405.0 / guessed_by_relation= 7907.0\n",
      "Calculating Recall: correct_by_relation= 4405.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.710%\n",
      "   Recall (micro): 68.199%\n",
      "       F1 (micro): 61.325%\n",
      "epoch 95: train_loss = 0.128389, dev_loss = 0.610393, dev_f1 = 0.6133\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_95.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:52:59.433127: step 142800/300200 (epoch 96/200), loss = 0.133781 (0.084 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:53:34.449727: step 143200/300200 (epoch 96/200), loss = 0.214809 (0.080 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:54:08.208481: step 143600/300200 (epoch 96/200), loss = 0.191862 (0.089 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:54:43.207031: step 144000/300200 (epoch 96/200), loss = 0.074497 (0.084 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4423.0 / guessed_by_relation= 7935.0\n",
      "Calculating Recall: correct_by_relation= 4423.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.740%\n",
      "   Recall (micro): 68.478%\n",
      "       F1 (micro): 61.456%\n",
      "epoch 96: train_loss = 0.128154, dev_loss = 0.608698, dev_f1 = 0.6146\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_96.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:55:32.465493: step 144400/300200 (epoch 97/200), loss = 0.117979 (0.089 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:56:06.675951: step 144800/300200 (epoch 97/200), loss = 0.125978 (0.084 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:56:40.969125: step 145200/300200 (epoch 97/200), loss = 0.108336 (0.094 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4482.0 / guessed_by_relation= 7992.0\n",
      "Calculating Recall: correct_by_relation= 4482.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.081%\n",
      "   Recall (micro): 69.392%\n",
      "       F1 (micro): 62.030%\n",
      "epoch 97: train_loss = 0.127348, dev_loss = 0.608080, dev_f1 = 0.6203\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_97.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 20:57:30.547441: step 145600/300200 (epoch 98/200), loss = 0.168769 (0.093 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:58:05.555014: step 146000/300200 (epoch 98/200), loss = 0.132675 (0.079 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:58:38.868083: step 146400/300200 (epoch 98/200), loss = 0.189409 (0.086 sec/batch), lr: 0.031402\n",
      "2018-03-24 20:59:13.813993: step 146800/300200 (epoch 98/200), loss = 0.138253 (0.087 sec/batch), lr: 0.031402\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4508.0 / guessed_by_relation= 8209.0\n",
      "Calculating Recall: correct_by_relation= 4508.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.915%\n",
      "   Recall (micro): 69.794%\n",
      "       F1 (micro): 61.467%\n",
      "epoch 98: train_loss = 0.127516, dev_loss = 0.639975, dev_f1 = 0.6147\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_98.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 21:00:02.826802: step 147200/300200 (epoch 99/200), loss = 0.036120 (0.092 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:00:37.952193: step 147600/300200 (epoch 99/200), loss = 0.257721 (0.088 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:01:11.150456: step 148000/300200 (epoch 99/200), loss = 0.088090 (0.092 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:01:46.062776: step 148400/300200 (epoch 99/200), loss = 0.117544 (0.089 sec/batch), lr: 0.029832\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4404.0 / guessed_by_relation= 7756.0\n",
      "Calculating Recall: correct_by_relation= 4404.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.782%\n",
      "   Recall (micro): 68.184%\n",
      "       F1 (micro): 61.963%\n",
      "epoch 99: train_loss = 0.126438, dev_loss = 0.596649, dev_f1 = 0.6196\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_99.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 21:02:35.184375: step 148800/300200 (epoch 100/200), loss = 0.332595 (0.086 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:03:09.330662: step 149200/300200 (epoch 100/200), loss = 0.194885 (0.075 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:03:43.573202: step 149600/300200 (epoch 100/200), loss = 0.095118 (0.080 sec/batch), lr: 0.029832\n",
      "2018-03-24 21:04:18.534657: step 150000/300200 (epoch 100/200), loss = 0.066703 (0.093 sec/batch), lr: 0.029832\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4473.0 / guessed_by_relation= 8186.0\n",
      "Calculating Recall: correct_by_relation= 4473.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.642%\n",
      "   Recall (micro): 69.252%\n",
      "       F1 (micro): 61.086%\n",
      "epoch 100: train_loss = 0.126392, dev_loss = 0.648781, dev_f1 = 0.6109\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_100.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-2  drop-0.4 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-1.0 weight_rest-0.3 attn-True attn_dim-200  obj_sub_pos-False new_residual-True  use_batch_norm-False relative_positions-True\n",
      "2018-03-24 21:05:08.208730: step 150400/300200 (epoch 101/200), loss = 0.086687 (0.088 sec/batch), lr: 0.028340\n",
      "2018-03-24 21:05:41.862720: step 150800/300200 (epoch 101/200), loss = 0.206895 (0.079 sec/batch), lr: 0.028340\n",
      "2018-03-24 21:06:17.683957: step 151200/300200 (epoch 101/200), loss = 0.208260 (0.093 sec/batch), lr: 0.028340\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2835baf890a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;31m# print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m# embedding lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mword_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         )\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\_functions\\thnn\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    \n",
    "    print(\n",
    "        \"Current params: \"+ \" heads-\"+ str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "        \" drop-\"+ str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\"+ str(opt[\"lr\"]),\n",
    "        \" lr_decay-\"+ str(opt[\"lr_decay\"]) + \" grad_norm-\"+ str(opt[\"max_grad_norm\"])\n",
    "    )\n",
    "    print(\n",
    "        \" weight_no_rel-\"+ str(opt[\"weight_no_rel\"]) +\n",
    "        \" weight_rest-\"+ str(opt[\"weight_rest\"]) + \" attn-\"+ str(opt[\"attn\"]) +\" attn_dim-\"+ str(opt[\"attn_dim\"]),\n",
    "        \" obj_sub_pos-\"+ str(opt[\"obj_sub_pos\"]) + \" new_residual-\"+str(opt[\"new_residual\"]),\n",
    "        \" use_batch_norm-\"+str(opt[\"use_batch_norm\"]) + \" relative_positions-\"+str(opt[\"relative_positions\"])\n",
    "    )\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam', 'nadam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
