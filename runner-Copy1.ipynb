{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of self-attention layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.2, help='Input and RNN dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=360, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.2, help='Applies to SGD and Adagrad.')            # lr 1.0 def\n",
    "parser.add_argument('--lr_decay', type=float, default=0.8)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=100)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='18_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/18_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/18_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.2\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 360\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.2\n",
      "\tlr_decay : 0.6\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 100\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 18_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/18_self_attention_dropout\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-06 23:49:51.970911: step 400/150100 (epoch 1/100), loss = 0.495513 (0.199 sec/batch), lr: 0.200000\n",
      "2018-03-06 23:51:10.818024: step 800/150100 (epoch 1/100), loss = 0.547903 (0.241 sec/batch), lr: 0.200000\n",
      "2018-03-06 23:52:35.072504: step 1200/150100 (epoch 1/100), loss = 0.581033 (0.165 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4123.0 / guessed_by_relation= 10870.0\n",
      "Calculating Recall: correct_by_relation= 4123.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.930%\n",
      "   Recall (micro): 63.833%\n",
      "       F1 (micro): 47.585%\n",
      "epoch 1: train_loss = 0.903048, dev_loss = 0.840968, dev_f1 = 0.4758\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-06 23:54:31.251363: step 1600/150100 (epoch 2/100), loss = 0.482220 (0.249 sec/batch), lr: 0.200000\n",
      "2018-03-06 23:55:59.596230: step 2000/150100 (epoch 2/100), loss = 0.693554 (0.203 sec/batch), lr: 0.200000\n",
      "2018-03-06 23:57:18.683476: step 2400/150100 (epoch 2/100), loss = 0.470421 (0.099 sec/batch), lr: 0.200000\n",
      "2018-03-06 23:58:41.029884: step 2800/150100 (epoch 2/100), loss = 0.664934 (0.208 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4420.0 / guessed_by_relation= 12014.0\n",
      "Calculating Recall: correct_by_relation= 4420.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 36.790%\n",
      "   Recall (micro): 68.432%\n",
      "       F1 (micro): 47.854%\n",
      "epoch 2: train_loss = 0.540921, dev_loss = 0.898277, dev_f1 = 0.4785\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:00:31.999404: step 3200/150100 (epoch 3/100), loss = 0.446588 (0.179 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:01:49.515479: step 3600/150100 (epoch 3/100), loss = 0.517308 (0.198 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:03:02.603301: step 4000/150100 (epoch 3/100), loss = 0.357740 (0.196 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:04:20.137416: step 4400/150100 (epoch 3/100), loss = 0.410647 (0.186 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4322.0 / guessed_by_relation= 9982.0\n",
      "Calculating Recall: correct_by_relation= 4322.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 43.298%\n",
      "   Recall (micro): 66.914%\n",
      "       F1 (micro): 52.576%\n",
      "epoch 3: train_loss = 0.497762, dev_loss = 0.705425, dev_f1 = 0.5258\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:06:13.496281: step 4800/150100 (epoch 4/100), loss = 0.403940 (0.180 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:07:33.938129: step 5200/150100 (epoch 4/100), loss = 0.665616 (0.197 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:08:49.425803: step 5600/150100 (epoch 4/100), loss = 0.883349 (0.231 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:10:09.288111: step 6000/150100 (epoch 4/100), loss = 0.287793 (0.236 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4291.0 / guessed_by_relation= 9075.0\n",
      "Calculating Recall: correct_by_relation= 4291.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.284%\n",
      "   Recall (micro): 66.434%\n",
      "       F1 (micro): 55.247%\n",
      "epoch 4: train_loss = 0.467005, dev_loss = 0.640928, dev_f1 = 0.5525\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:12:00.392468: step 6400/150100 (epoch 5/100), loss = 0.391776 (0.217 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:13:20.654842: step 6800/150100 (epoch 5/100), loss = 0.322289 (0.201 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:14:34.006840: step 7200/150100 (epoch 5/100), loss = 0.369216 (0.186 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4321.0 / guessed_by_relation= 8596.0\n",
      "Calculating Recall: correct_by_relation= 4321.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.268%\n",
      "   Recall (micro): 66.899%\n",
      "       F1 (micro): 57.403%\n",
      "epoch 5: train_loss = 0.445293, dev_loss = 0.588077, dev_f1 = 0.5740\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:16:24.772299: step 7600/150100 (epoch 6/100), loss = 0.485330 (0.223 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:17:44.951449: step 8000/150100 (epoch 6/100), loss = 0.359884 (0.211 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:19:05.445944: step 8400/150100 (epoch 6/100), loss = 0.462946 (0.203 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:20:20.058297: step 8800/150100 (epoch 6/100), loss = 0.530003 (0.185 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4276.0 / guessed_by_relation= 8081.0\n",
      "Calculating Recall: correct_by_relation= 4276.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.914%\n",
      "   Recall (micro): 66.202%\n",
      "       F1 (micro): 58.817%\n",
      "epoch 6: train_loss = 0.431701, dev_loss = 0.553976, dev_f1 = 0.5882\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:22:12.407976: step 9200/150100 (epoch 7/100), loss = 0.639185 (0.211 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:23:41.138366: step 9600/150100 (epoch 7/100), loss = 0.226405 (0.098 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:24:19.002527: step 10000/150100 (epoch 7/100), loss = 0.231458 (0.089 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:24:56.705756: step 10400/150100 (epoch 7/100), loss = 0.198824 (0.090 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4323.0 / guessed_by_relation= 8172.0\n",
      "Calculating Recall: correct_by_relation= 4323.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.900%\n",
      "   Recall (micro): 66.930%\n",
      "       F1 (micro): 59.094%\n",
      "epoch 7: train_loss = 0.416945, dev_loss = 0.556707, dev_f1 = 0.5909\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:26:10.896483: step 10800/150100 (epoch 8/100), loss = 0.364994 (0.170 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:27:24.025888: step 11200/150100 (epoch 8/100), loss = 0.533304 (0.196 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:28:34.575442: step 11600/150100 (epoch 8/100), loss = 0.454005 (0.175 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:29:42.972267: step 12000/150100 (epoch 8/100), loss = 0.194008 (0.168 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4256.0 / guessed_by_relation= 7865.0\n",
      "Calculating Recall: correct_by_relation= 4256.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.113%\n",
      "   Recall (micro): 65.893%\n",
      "       F1 (micro): 59.425%\n",
      "epoch 8: train_loss = 0.407270, dev_loss = 0.541348, dev_f1 = 0.5942\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:31:21.386899: step 12400/150100 (epoch 9/100), loss = 0.327130 (0.193 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:32:31.484252: step 12800/150100 (epoch 9/100), loss = 0.352118 (0.211 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:33:42.677020: step 13200/150100 (epoch 9/100), loss = 0.386588 (0.184 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4428.0 / guessed_by_relation= 8271.0\n",
      "Calculating Recall: correct_by_relation= 4428.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.536%\n",
      "   Recall (micro): 68.556%\n",
      "       F1 (micro): 60.122%\n",
      "epoch 9: train_loss = 0.399979, dev_loss = 0.546730, dev_f1 = 0.6012\n",
      "model saved to ./saved_models/18_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 00:35:26.754703: step 13600/150100 (epoch 10/100), loss = 0.481430 (0.160 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:36:35.889937: step 14000/150100 (epoch 10/100), loss = 0.412181 (0.170 sec/batch), lr: 0.200000\n",
      "2018-03-07 00:37:47.871125: step 14400/150100 (epoch 10/100), loss = 0.302777 (0.189 sec/batch), lr: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 5 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at + lstm h, lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
