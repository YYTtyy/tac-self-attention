{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCurrent params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\\n weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
    " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of lstm layers.')\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=2, help='Num of self-attention encoders.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Input and attn dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')        # 0.1 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=2.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=1.0, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "parser.add_argument('--obj_sub_pos', dest='obj_sub_pos', action='store_true', \n",
    "    help='In self-attention add obj/subg positional vectors.', default=True)\n",
    "parser.add_argument('--use_batch_norm', dest='use_batch_norm', action='store_true', \n",
    "    help='BatchNorm if true, else LayerNorm in self-attention.', default=True)\n",
    "parser.add_argument('--relative_positions', dest='relative_positions', action='store_true', \n",
    "    help='Use relative positions for subj/obj positional vectors.', default=True)\n",
    "parser.add_argument('--new_residual', dest='new_residual', action='store_true', \n",
    "    help='Use a different residual connection than in usual self-attention.', default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=1, help='Number of self-attention heads.')\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.3, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=10, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='40_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/39_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/39_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tnum_layers_encoder : 3\n",
      "\tdropout : 0.7\n",
      "\tscaled_dropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 2.0\n",
      "\tweight_rest : 1.0\n",
      "\tself_att : True\n",
      "\tobj_sub_pos : True\n",
      "\tuse_batch_norm : True\n",
      "\trelative_positions : False\n",
      "\tnew_residual : True\n",
      "\tn_head : 3\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.3\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 1.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 10\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 39_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/39_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  3\n",
      "d_v and d_k:  120.0\n",
      "Finetune all embeddings.\n",
      "Using weights [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-23 21:40:36.659678: step 400/300200 (epoch 1/200), loss = 0.508941 (0.054 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:41:02.673341: step 800/300200 (epoch 1/200), loss = 0.495421 (0.066 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:41:28.143558: step 1200/300200 (epoch 1/200), loss = 0.492163 (0.051 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2200.0 / guessed_by_relation= 4439.0\n",
      "Calculating Recall: correct_by_relation= 2200.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.561%\n",
      "   Recall (micro): 34.061%\n",
      "       F1 (micro): 40.374%\n",
      "epoch 1: train_loss = 0.553322, dev_loss = 0.613378, dev_f1 = 0.4037\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:42:03.900138: step 1600/300200 (epoch 2/200), loss = 0.372784 (0.068 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:42:30.159456: step 2000/300200 (epoch 2/200), loss = 0.554777 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:42:56.061321: step 2400/300200 (epoch 2/200), loss = 0.448437 (0.072 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:43:21.685948: step 2800/300200 (epoch 2/200), loss = 0.535886 (0.066 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3302.0 / guessed_by_relation= 7739.0\n",
      "Calculating Recall: correct_by_relation= 3302.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 42.667%\n",
      "   Recall (micro): 51.122%\n",
      "       F1 (micro): 46.514%\n",
      "epoch 2: train_loss = 0.407634, dev_loss = 0.645597, dev_f1 = 0.4651\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:43:57.461564: step 3200/300200 (epoch 3/200), loss = 0.368608 (0.052 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:44:22.951834: step 3600/300200 (epoch 3/200), loss = 0.422787 (0.062 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:44:48.791534: step 4000/300200 (epoch 3/200), loss = 0.226752 (0.067 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:45:14.326423: step 4400/300200 (epoch 3/200), loss = 0.415716 (0.060 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3942.0 / guessed_by_relation= 9186.0\n",
      "Calculating Recall: correct_by_relation= 3942.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 42.913%\n",
      "   Recall (micro): 61.031%\n",
      "       F1 (micro): 50.393%\n",
      "epoch 3: train_loss = 0.371594, dev_loss = 0.650135, dev_f1 = 0.5039\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:45:50.394316: step 4800/300200 (epoch 4/200), loss = 0.296717 (0.054 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:46:17.059711: step 5200/300200 (epoch 4/200), loss = 0.443388 (0.057 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:46:43.027756: step 5600/300200 (epoch 4/200), loss = 0.617046 (0.071 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:47:08.963210: step 6000/300200 (epoch 4/200), loss = 0.198041 (0.073 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3888.0 / guessed_by_relation= 8416.0\n",
      "Calculating Recall: correct_by_relation= 3888.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 46.198%\n",
      "   Recall (micro): 60.195%\n",
      "       F1 (micro): 52.276%\n",
      "epoch 4: train_loss = 0.352518, dev_loss = 0.590812, dev_f1 = 0.5228\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:47:44.929337: step 6400/300200 (epoch 5/200), loss = 0.366115 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:48:10.353431: step 6800/300200 (epoch 5/200), loss = 0.277083 (0.063 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:48:35.803079: step 7200/300200 (epoch 5/200), loss = 0.381785 (0.058 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3826.0 / guessed_by_relation= 7537.0\n",
      "Calculating Recall: correct_by_relation= 3826.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.763%\n",
      "   Recall (micro): 59.235%\n",
      "       F1 (micro): 54.673%\n",
      "epoch 5: train_loss = 0.339723, dev_loss = 0.533713, dev_f1 = 0.5467\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:49:11.455367: step 7600/300200 (epoch 6/200), loss = 0.307057 (0.068 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:49:36.950650: step 8000/300200 (epoch 6/200), loss = 0.182714 (0.066 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:50:02.478019: step 8400/300200 (epoch 6/200), loss = 0.305306 (0.062 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:50:27.957762: step 8800/300200 (epoch 6/200), loss = 0.303912 (0.053 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3947.0 / guessed_by_relation= 7910.0\n",
      "Calculating Recall: correct_by_relation= 3947.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.899%\n",
      "   Recall (micro): 61.109%\n",
      "       F1 (micro): 54.938%\n",
      "epoch 6: train_loss = 0.331407, dev_loss = 0.521870, dev_f1 = 0.5494\n",
      "model saved to ./saved_models/39_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-3  drop-0.7 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True\n",
      "2018-03-23 21:51:03.602529: step 9200/300200 (epoch 7/200), loss = 0.446449 (0.075 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:51:29.486349: step 9600/300200 (epoch 7/200), loss = 0.145661 (0.068 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:51:55.139553: step 10000/300200 (epoch 7/200), loss = 0.220528 (0.061 sec/batch), lr: 0.300000\n",
      "2018-03-23 21:52:20.777717: step 10400/300200 (epoch 7/200), loss = 0.133069 (0.061 sec/batch), lr: 0.300000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1e535d816291>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    \n",
    "    print(\n",
    "        \"Current params: \"+ \" heads-\"+ str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "        \" drop-\"+ str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\"+ str(opt[\"lr\"]),\n",
    "        \" lr_decay-\"+ str(opt[\"lr_decay\"]) + \" grad_norm-\"+ str(opt[\"max_grad_norm\"])\n",
    "    )\n",
    "    print(\n",
    "        \" weight_no_rel-\"+ str(opt[\"weight_no_rel\"]) +\n",
    "        \" weight_rest-\"+ str(opt[\"weight_rest\"]) + \" attn-\"+ str(opt[\"attn\"]) +\" attn_dim-\"+ str(opt[\"attn_dim\"]),\n",
    "        \" obj_sub_pos-\"+ str(opt[\"obj_sub_pos\"]) + \" new_residual\"+str(opt[\"new_residual\"]),\n",
    "        \" use_batch_norm\"+str(opt[\"use_batch_norm\"])\n",
    "    )\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam', 'nadam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is without relative positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout\n",
    "\n",
    "# prev\n",
    "# max norm = 3.0, 3.0 weights, 3 encoders, new residual, !batch norm!\n",
    "# prev\n",
    "# max norm = 1.0, 0.3 weights, 2 encoders, new residual, !batch norm!\n",
    "# prev\n",
    "# max norm = 1.0, 2.0 weights, 3 encoders, new residual, !batch norm!, scaled=0.1, dropout 0.6\n",
    "# prev, 61.8 in test\n",
    "# max norm = 1.0, 2.0 weights, 3 encoders, new residual, !batch norm!, scaled=0.1, dropout 0.6, 400 epochs, 0.5 lr\n",
    "\n",
    "# prev, test 60.5, 65% rec\n",
    "# lr 0.3, drop 0.7, max 0.1, batchn, scaled 0.1, 3 enc, w 0.2\n",
    "\n",
    "# prev\n",
    "# lr 0.3, drop 0.6, max 0.1, batchn, scaled 0.3, 4 enc, w 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mindel 63% training\n",
    "# 4 encoders, 0.3 lr, 0.95 decay, 1.0 maxnorm, drop 0.6, scaled 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
