{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0b0+591e73e\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')               # 200\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of RNN layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04, \n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=360, help='Attention size.')                      # 200\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1.0, help='Applies to SGD and Adagrad.')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.9)\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')\n",
    "parser.add_argument('--num_epoch', type=int, default=30)\n",
    "parser.add_argument('--batch_size', type=int, default=50)\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "parser.add_argument('--id', type=str, default='02_self_attention_q', help='Model ID under which to save models.')\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Directory ./saved_models/02_self_attention_q do not exist; creating...\n",
      "Config saved to file ./saved_models/02_self_attention_q/config.json\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.5\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 360\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.9\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 30\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 02_self_attention_q\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/02_self_attention_q\n",
      "\n",
      "\n",
      "using self-attention 2\n",
      "Finetune all embeddings.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:325: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n",
      "J:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py:187: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = F.softmax(scores)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-25 16:32:33.329265: step 400/45030 (epoch 1/30), loss = 0.971793 (0.039 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:32:49.922883: step 800/45030 (epoch 1/30), loss = 0.743485 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:33:06.462356: step 1200/45030 (epoch 1/30), loss = 0.672885 (0.032 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits).data.cpu().numpy().tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (micro): 31.709%\n",
      "   Recall (micro): 64.964%\n",
      "       F1 (micro): 42.616%\n",
      "epoch 1: train_loss = 1.130688, dev_loss = 1.409271, dev_f1 = 0.4262\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:33:30.947464: step 1600/45030 (epoch 2/30), loss = 0.678784 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:33:47.564644: step 2000/45030 (epoch 2/30), loss = 0.752038 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:34:04.111638: step 2400/45030 (epoch 2/30), loss = 0.594352 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:34:20.761905: step 2800/45030 (epoch 2/30), loss = 0.659498 (0.042 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 38.264%\n",
      "   Recall (micro): 67.007%\n",
      "       F1 (micro): 48.711%\n",
      "epoch 2: train_loss = 0.593566, dev_loss = 0.924629, dev_f1 = 0.4871\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:34:45.063013: step 3200/45030 (epoch 3/30), loss = 0.547714 (0.034 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:35:01.467127: step 3600/45030 (epoch 3/30), loss = 0.701323 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:35:17.913355: step 4000/45030 (epoch 3/30), loss = 0.423320 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:35:34.364593: step 4400/45030 (epoch 3/30), loss = 0.651540 (0.039 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 36.330%\n",
      "   Recall (micro): 72.457%\n",
      "       F1 (micro): 48.395%\n",
      "epoch 3: train_loss = 0.532541, dev_loss = 1.068310, dev_f1 = 0.4839\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_3.pt\n",
      "\n",
      "2018-02-25 16:35:58.977031: step 4800/45030 (epoch 4/30), loss = 0.387279 (0.036 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:36:15.374124: step 5200/45030 (epoch 4/30), loss = 0.721803 (0.037 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:36:31.803806: step 5600/45030 (epoch 4/30), loss = 0.946913 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:36:48.283119: step 6000/45030 (epoch 4/30), loss = 0.310581 (0.044 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 39.017%\n",
      "   Recall (micro): 73.479%\n",
      "       F1 (micro): 50.969%\n",
      "epoch 4: train_loss = 0.504843, dev_loss = 1.000068, dev_f1 = 0.5097\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:37:12.966246: step 6400/45030 (epoch 5/30), loss = 0.501473 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:37:29.389410: step 6800/45030 (epoch 5/30), loss = 0.309109 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:37:45.791518: step 7200/45030 (epoch 5/30), loss = 0.601413 (0.038 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 38.317%\n",
      "   Recall (micro): 75.863%\n",
      "       F1 (micro): 50.917%\n",
      "epoch 5: train_loss = 0.484133, dev_loss = 1.034544, dev_f1 = 0.5092\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_5.pt\n",
      "\n",
      "2018-02-25 16:38:09.907140: step 7600/45030 (epoch 6/30), loss = 0.386000 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:38:26.369910: step 8000/45030 (epoch 6/30), loss = 0.358648 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:38:42.831676: step 8400/45030 (epoch 6/30), loss = 0.436720 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:38:59.214739: step 8800/45030 (epoch 6/30), loss = 0.451632 (0.036 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 40.874%\n",
      "   Recall (micro): 71.110%\n",
      "       F1 (micro): 51.910%\n",
      "epoch 6: train_loss = 0.471636, dev_loss = 0.910036, dev_f1 = 0.5191\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:39:24.189639: step 9200/45030 (epoch 7/30), loss = 0.673565 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:39:40.583224: step 9600/45030 (epoch 7/30), loss = 0.189134 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:39:57.041982: step 10000/45030 (epoch 7/30), loss = 0.306967 (0.039 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:40:13.530320: step 10400/45030 (epoch 7/30), loss = 0.194783 (0.041 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 41.425%\n",
      "   Recall (micro): 72.999%\n",
      "       F1 (micro): 52.856%\n",
      "epoch 7: train_loss = 0.454205, dev_loss = 0.884390, dev_f1 = 0.5286\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:40:37.900612: step 10800/45030 (epoch 8/30), loss = 0.472444 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:40:54.310742: step 11200/45030 (epoch 8/30), loss = 0.625007 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:41:10.723879: step 11600/45030 (epoch 8/30), loss = 0.497733 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:41:27.204195: step 12000/45030 (epoch 8/30), loss = 0.267810 (0.035 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 42.621%\n",
      "   Recall (micro): 74.764%\n",
      "       F1 (micro): 54.292%\n",
      "epoch 8: train_loss = 0.439247, dev_loss = 0.878599, dev_f1 = 0.5429\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:41:52.117932: step 12400/45030 (epoch 9/30), loss = 0.400543 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:42:08.554131: step 12800/45030 (epoch 9/30), loss = 0.314625 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:42:24.938692: step 13200/45030 (epoch 9/30), loss = 0.446769 (0.043 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 38.749%\n",
      "   Recall (micro): 77.102%\n",
      "       F1 (micro): 51.577%\n",
      "epoch 9: train_loss = 0.423494, dev_loss = 1.017099, dev_f1 = 0.5158\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_9.pt\n",
      "\n",
      "2018-02-25 16:42:49.225766: step 13600/45030 (epoch 10/30), loss = 0.504270 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:43:05.684524: step 14000/45030 (epoch 10/30), loss = 0.477244 (0.045 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:43:22.132754: step 14400/45030 (epoch 10/30), loss = 0.274512 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:43:38.579483: step 14800/45030 (epoch 10/30), loss = 0.585952 (0.032 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 40.667%\n",
      "   Recall (micro): 76.297%\n",
      "       F1 (micro): 53.055%\n",
      "epoch 10: train_loss = 0.413817, dev_loss = 0.963454, dev_f1 = 0.5305\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_10.pt\n",
      "\n",
      "2018-02-25 16:44:03.624570: step 15200/45030 (epoch 11/30), loss = 0.612022 (0.030 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:44:20.067787: step 15600/45030 (epoch 11/30), loss = 0.603237 (0.045 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:44:36.542090: step 16000/45030 (epoch 11/30), loss = 0.463729 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:44:53.017392: step 16400/45030 (epoch 11/30), loss = 0.258495 (0.043 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 39.439%\n",
      "   Recall (micro): 79.207%\n",
      "       F1 (micro): 52.658%\n",
      "epoch 11: train_loss = 0.405225, dev_loss = 1.089612, dev_f1 = 0.5266\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_11.pt\n",
      "\n",
      "2018-02-25 16:45:17.719068: step 16800/45030 (epoch 12/30), loss = 0.618383 (0.039 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:45:34.124185: step 17200/45030 (epoch 12/30), loss = 0.268980 (0.035 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:45:50.564895: step 17600/45030 (epoch 12/30), loss = 0.435171 (0.037 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:46:07.059753: step 18000/45030 (epoch 12/30), loss = 0.368349 (0.042 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 40.034%\n",
      "   Recall (micro): 76.962%\n",
      "       F1 (micro): 52.670%\n",
      "epoch 12: train_loss = 0.388034, dev_loss = 1.027977, dev_f1 = 0.5267\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_12.pt\n",
      "\n",
      "2018-02-25 16:46:31.261597: step 18400/45030 (epoch 13/30), loss = 0.415380 (0.045 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:46:47.692282: step 18800/45030 (epoch 13/30), loss = 0.196580 (0.036 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:47:04.068320: step 19200/45030 (epoch 13/30), loss = 0.203333 (0.041 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 41.367%\n",
      "   Recall (micro): 75.043%\n",
      "       F1 (micro): 53.334%\n",
      "epoch 13: train_loss = 0.377041, dev_loss = 0.998437, dev_f1 = 0.5333\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_13.pt\n",
      "\n",
      "2018-02-25 16:47:28.173408: step 19600/45030 (epoch 14/30), loss = 0.264378 (0.031 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:47:44.645702: step 20000/45030 (epoch 14/30), loss = 0.326157 (0.045 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:48:01.097944: step 20400/45030 (epoch 14/30), loss = 0.350900 (0.045 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:48:17.490526: step 20800/45030 (epoch 14/30), loss = 0.245337 (0.037 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 42.057%\n",
      "   Recall (micro): 76.235%\n",
      "       F1 (micro): 54.208%\n",
      "epoch 14: train_loss = 0.363821, dev_loss = 0.963884, dev_f1 = 0.5421\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_14.pt\n",
      "\n",
      "2018-02-25 16:48:41.683347: step 21200/45030 (epoch 15/30), loss = 0.535442 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:48:58.069412: step 21600/45030 (epoch 15/30), loss = 0.304125 (0.035 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:49:14.494581: step 22000/45030 (epoch 15/30), loss = 0.274576 (0.031 sec/batch), lr: 1.000000\n",
      "2018-02-25 16:49:30.972895: step 22400/45030 (epoch 15/30), loss = 0.499092 (0.043 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 41.835%\n",
      "   Recall (micro): 74.532%\n",
      "       F1 (micro): 53.590%\n",
      "epoch 15: train_loss = 0.351887, dev_loss = 0.983479, dev_f1 = 0.5359\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_15.pt\n",
      "\n",
      "2018-02-25 16:49:55.603884: step 22800/45030 (epoch 16/30), loss = 0.324446 (0.037 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:50:12.083703: step 23200/45030 (epoch 16/30), loss = 0.543954 (0.041 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:50:28.514887: step 23600/45030 (epoch 16/30), loss = 0.307879 (0.034 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:50:45.011246: step 24000/45030 (epoch 16/30), loss = 0.359053 (0.043 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 42.967%\n",
      "   Recall (micro): 74.485%\n",
      "       F1 (micro): 54.497%\n",
      "epoch 16: train_loss = 0.332679, dev_loss = 0.917167, dev_f1 = 0.5450\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:51:09.578562: step 24400/45030 (epoch 17/30), loss = 0.194801 (0.040 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:51:26.027801: step 24800/45030 (epoch 17/30), loss = 0.356273 (0.043 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:51:42.400832: step 25200/45030 (epoch 17/30), loss = 0.243535 (0.035 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 44.361%\n",
      "   Recall (micro): 72.473%\n",
      "       F1 (micro): 55.035%\n",
      "epoch 17: train_loss = 0.320468, dev_loss = 0.895552, dev_f1 = 0.5503\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:52:06.935562: step 25600/45030 (epoch 18/30), loss = 0.197000 (0.044 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:52:23.385798: step 26000/45030 (epoch 18/30), loss = 0.354813 (0.038 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:52:39.860098: step 26400/45030 (epoch 18/30), loss = 0.298221 (0.040 sec/batch), lr: 0.900000\n",
      "2018-02-25 16:52:56.238141: step 26800/45030 (epoch 18/30), loss = 0.347947 (0.036 sec/batch), lr: 0.900000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 43.197%\n",
      "   Recall (micro): 72.751%\n",
      "       F1 (micro): 54.208%\n",
      "epoch 18: train_loss = 0.307930, dev_loss = 0.983168, dev_f1 = 0.5421\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_18.pt\n",
      "\n",
      "2018-02-25 16:53:20.436982: step 27200/45030 (epoch 19/30), loss = 0.371056 (0.035 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:53:36.827558: step 27600/45030 (epoch 19/30), loss = 0.152478 (0.031 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:53:53.259245: step 28000/45030 (epoch 19/30), loss = 0.143389 (0.038 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:54:09.705974: step 28400/45030 (epoch 19/30), loss = 0.429075 (0.037 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 45.655%\n",
      "   Recall (micro): 70.522%\n",
      "       F1 (micro): 55.427%\n",
      "epoch 19: train_loss = 0.290554, dev_loss = 0.873464, dev_f1 = 0.5543\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_19.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:54:34.327437: step 28800/45030 (epoch 20/30), loss = 0.283148 (0.032 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:54:50.748599: step 29200/45030 (epoch 20/30), loss = 0.254050 (0.042 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:55:07.204352: step 29600/45030 (epoch 20/30), loss = 0.255443 (0.031 sec/batch), lr: 0.810000\n",
      "2018-02-25 16:55:23.696700: step 30000/45030 (epoch 20/30), loss = 0.163826 (0.040 sec/batch), lr: 0.810000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 47.187%\n",
      "   Recall (micro): 66.357%\n",
      "       F1 (micro): 55.154%\n",
      "epoch 20: train_loss = 0.278986, dev_loss = 0.867549, dev_f1 = 0.5515\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_20.pt\n",
      "\n",
      "2018-02-25 16:55:47.723580: step 30400/45030 (epoch 21/30), loss = 0.119619 (0.043 sec/batch), lr: 0.729000\n",
      "2018-02-25 16:56:04.715757: step 30800/45030 (epoch 21/30), loss = 0.225067 (0.045 sec/batch), lr: 0.729000\n",
      "2018-02-25 16:56:21.550514: step 31200/45030 (epoch 21/30), loss = 0.181827 (0.042 sec/batch), lr: 0.729000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 44.958%\n",
      "   Recall (micro): 69.717%\n",
      "       F1 (micro): 54.665%\n",
      "epoch 21: train_loss = 0.260200, dev_loss = 0.939359, dev_f1 = 0.5466\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_21.pt\n",
      "\n",
      "2018-02-25 16:56:46.065692: step 31600/45030 (epoch 22/30), loss = 0.236744 (0.035 sec/batch), lr: 0.656100\n",
      "2018-02-25 16:57:02.878392: step 32000/45030 (epoch 22/30), loss = 0.217341 (0.043 sec/batch), lr: 0.656100\n",
      "2018-02-25 16:57:19.737214: step 32400/45030 (epoch 22/30), loss = 0.139892 (0.039 sec/batch), lr: 0.656100\n",
      "2018-02-25 16:57:36.372946: step 32800/45030 (epoch 22/30), loss = 0.173724 (0.037 sec/batch), lr: 0.656100\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 45.293%\n",
      "   Recall (micro): 67.781%\n",
      "       F1 (micro): 54.301%\n",
      "epoch 22: train_loss = 0.248368, dev_loss = 0.952725, dev_f1 = 0.5430\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_22.pt\n",
      "\n",
      "2018-02-25 16:58:00.869574: step 33200/45030 (epoch 23/30), loss = 0.210057 (0.039 sec/batch), lr: 0.590490\n",
      "2018-02-25 16:58:17.308279: step 33600/45030 (epoch 23/30), loss = 0.323288 (0.044 sec/batch), lr: 0.590490\n",
      "2018-02-25 16:58:33.788096: step 34000/45030 (epoch 23/30), loss = 0.248437 (0.042 sec/batch), lr: 0.590490\n",
      "2018-02-25 16:58:50.240337: step 34400/45030 (epoch 23/30), loss = 0.094789 (0.043 sec/batch), lr: 0.590490\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 47.918%\n",
      "   Recall (micro): 66.821%\n",
      "       F1 (micro): 55.813%\n",
      "epoch 23: train_loss = 0.234294, dev_loss = 0.891714, dev_f1 = 0.5581\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_23.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-25 16:59:14.908923: step 34800/45030 (epoch 24/30), loss = 0.448057 (0.041 sec/batch), lr: 0.590490\n",
      "2018-02-25 16:59:31.278444: step 35200/45030 (epoch 24/30), loss = 0.251810 (0.042 sec/batch), lr: 0.590490\n",
      "2018-02-25 16:59:47.744221: step 35600/45030 (epoch 24/30), loss = 0.178355 (0.039 sec/batch), lr: 0.590490\n",
      "2018-02-25 17:00:04.336334: step 36000/45030 (epoch 24/30), loss = 0.353787 (0.039 sec/batch), lr: 0.590490\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 48.161%\n",
      "   Recall (micro): 65.691%\n",
      "       F1 (micro): 55.577%\n",
      "epoch 24: train_loss = 0.224208, dev_loss = 0.877258, dev_f1 = 0.5558\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_24.pt\n",
      "\n",
      "2018-02-25 17:00:28.310574: step 36400/45030 (epoch 25/30), loss = 0.275001 (0.041 sec/batch), lr: 0.531441\n",
      "2018-02-25 17:00:44.744767: step 36800/45030 (epoch 25/30), loss = 0.278009 (0.044 sec/batch), lr: 0.531441\n",
      "2018-02-25 17:01:01.132841: step 37200/45030 (epoch 25/30), loss = 0.104161 (0.044 sec/batch), lr: 0.531441\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 48.575%\n",
      "   Recall (micro): 64.128%\n",
      "       F1 (micro): 55.278%\n",
      "epoch 25: train_loss = 0.210554, dev_loss = 0.922086, dev_f1 = 0.5528\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_25.pt\n",
      "\n",
      "2018-02-25 17:01:25.582343: step 37600/45030 (epoch 26/30), loss = 0.152275 (0.037 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:01:42.044110: step 38000/45030 (epoch 26/30), loss = 0.081090 (0.034 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:01:58.481311: step 38400/45030 (epoch 26/30), loss = 0.190244 (0.040 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:02:15.070918: step 38800/45030 (epoch 26/30), loss = 0.160720 (0.046 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 50.765%\n",
      "   Recall (micro): 61.109%\n",
      "       F1 (micro): 55.459%\n",
      "epoch 26: train_loss = 0.199353, dev_loss = 0.899322, dev_f1 = 0.5546\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_26.pt\n",
      "\n",
      "2018-02-25 17:02:39.520423: step 39200/45030 (epoch 27/30), loss = 0.105645 (0.041 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:02:56.088472: step 39600/45030 (epoch 27/30), loss = 0.332327 (0.040 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:03:12.703145: step 40000/45030 (epoch 27/30), loss = 0.272852 (0.035 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:03:29.184464: step 40400/45030 (epoch 27/30), loss = 0.243845 (0.043 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 48.550%\n",
      "   Recall (micro): 64.778%\n",
      "       F1 (micro): 55.502%\n",
      "epoch 27: train_loss = 0.192144, dev_loss = 0.969005, dev_f1 = 0.5550\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_27.pt\n",
      "\n",
      "2018-02-25 17:03:53.448474: step 40800/45030 (epoch 28/30), loss = 0.166113 (0.036 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:04:09.805963: step 41200/45030 (epoch 28/30), loss = 0.081020 (0.041 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:04:26.250684: step 41600/45030 (epoch 28/30), loss = 0.064074 (0.041 sec/batch), lr: 0.478297\n",
      "2018-02-25 17:04:42.745037: step 42000/45030 (epoch 28/30), loss = 0.288686 (0.043 sec/batch), lr: 0.478297\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 50.552%\n",
      "   Recall (micro): 58.833%\n",
      "       F1 (micro): 54.379%\n",
      "epoch 28: train_loss = 0.186531, dev_loss = 0.948453, dev_f1 = 0.5438\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_28.pt\n",
      "\n",
      "2018-02-25 17:05:07.460753: step 42400/45030 (epoch 29/30), loss = 0.168002 (0.034 sec/batch), lr: 0.430467\n",
      "2018-02-25 17:05:23.896454: step 42800/45030 (epoch 29/30), loss = 0.266682 (0.041 sec/batch), lr: 0.430467\n",
      "2018-02-25 17:05:40.293548: step 43200/45030 (epoch 29/30), loss = 0.070400 (0.044 sec/batch), lr: 0.430467\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 44.949%\n",
      "   Recall (micro): 64.824%\n",
      "       F1 (micro): 53.087%\n",
      "epoch 29: train_loss = 0.177361, dev_loss = 1.087330, dev_f1 = 0.5309\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_29.pt\n",
      "\n",
      "2018-02-25 17:06:05.057391: step 43600/45030 (epoch 30/30), loss = 0.033743 (0.035 sec/batch), lr: 0.387420\n",
      "2018-02-25 17:06:21.505119: step 44000/45030 (epoch 30/30), loss = 0.134941 (0.034 sec/batch), lr: 0.387420\n",
      "2018-02-25 17:06:37.942822: step 44400/45030 (epoch 30/30), loss = 0.190655 (0.037 sec/batch), lr: 0.387420\n",
      "2018-02-25 17:06:54.419127: step 44800/45030 (epoch 30/30), loss = 0.134712 (0.042 sec/batch), lr: 0.387420\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 48.334%\n",
      "   Recall (micro): 63.322%\n",
      "       F1 (micro): 54.822%\n",
      "epoch 30: train_loss = 0.168071, dev_loss = 1.033283, dev_f1 = 0.5482\n",
      "model saved to ./saved_models/02_self_attention_q/checkpoint_epoch_30.pt\n",
      "\n",
      "Training ended with 30 epochs.\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % 400 == 0: # if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(format_str.format(datetime.now(), global_step, max_steps, epoch,\\\n",
    "                    opt['num_epoch'], loss, duration, current_lr))\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1))\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and \\\n",
    "            opt['optim'] in ['sgd', 'adagrad']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
