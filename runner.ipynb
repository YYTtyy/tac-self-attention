{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of self-attention layers.')\n",
    "# dropout\n",
    "parser.add_argument('--dropout', type=float, default=0.3, help='Input and RNN dropout rate.')        # 0.5 original\n",
    "\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.15, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='31_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/31_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/31_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.3\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.15\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 31_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/31_self_attention_dropout\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n",
      "using weights [3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-12 11:39:15.129604: step 400/300200 (epoch 1/200), loss = 0.311481 (0.131 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:40:35.141704: step 800/300200 (epoch 1/200), loss = 0.347143 (0.207 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:41:54.574645: step 1200/300200 (epoch 1/200), loss = 0.331229 (0.172 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2555.0 / guessed_by_relation= 5256.0\n",
      "Calculating Recall: correct_by_relation= 2555.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 48.611%\n",
      "   Recall (micro): 39.557%\n",
      "       F1 (micro): 43.619%\n",
      "epoch 1: train_loss = 0.407067, dev_loss = 0.493326, dev_f1 = 0.4362\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:43:35.390833: step 1600/300200 (epoch 2/200), loss = 0.304707 (0.192 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:44:44.213087: step 2000/300200 (epoch 2/200), loss = 0.413279 (0.169 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:45:50.350710: step 2400/300200 (epoch 2/200), loss = 0.323961 (0.165 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:46:56.132702: step 2800/300200 (epoch 2/200), loss = 0.402885 (0.167 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3285.0 / guessed_by_relation= 6349.0\n",
      "Calculating Recall: correct_by_relation= 3285.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.740%\n",
      "   Recall (micro): 50.859%\n",
      "       F1 (micro): 51.296%\n",
      "epoch 2: train_loss = 0.305323, dev_loss = 0.458048, dev_f1 = 0.5130\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:48:21.723498: step 3200/300200 (epoch 3/200), loss = 0.242966 (0.137 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:49:27.614780: step 3600/300200 (epoch 3/200), loss = 0.295236 (0.208 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:50:33.900231: step 4000/300200 (epoch 3/200), loss = 0.189029 (0.170 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:51:39.786142: step 4400/300200 (epoch 3/200), loss = 0.241741 (0.172 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3641.0 / guessed_by_relation= 7323.0\n",
      "Calculating Recall: correct_by_relation= 3641.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.720%\n",
      "   Recall (micro): 56.371%\n",
      "       F1 (micro): 52.837%\n",
      "epoch 3: train_loss = 0.285314, dev_loss = 0.480054, dev_f1 = 0.5284\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 11:53:05.858939: step 4800/300200 (epoch 4/200), loss = 0.242565 (0.152 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:54:11.754232: step 5200/300200 (epoch 4/200), loss = 0.376034 (0.147 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:55:17.962357: step 5600/300200 (epoch 4/200), loss = 0.464213 (0.177 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:56:24.058183: step 6000/300200 (epoch 4/200), loss = 0.215812 (0.168 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2816.0 / guessed_by_relation= 4566.0\n",
      "Calculating Recall: correct_by_relation= 2816.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.673%\n",
      "   Recall (micro): 43.598%\n",
      "       F1 (micro): 51.084%\n",
      "epoch 4: train_loss = 0.271320, dev_loss = 0.362271, dev_f1 = 0.5108\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "\n",
      "2018-03-12 11:57:52.812585: step 6400/300200 (epoch 5/200), loss = 0.228125 (0.273 sec/batch), lr: 0.150000\n",
      "2018-03-12 11:59:03.198923: step 6800/300200 (epoch 5/200), loss = 0.187712 (0.183 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:00:10.912122: step 7200/300200 (epoch 5/200), loss = 0.266189 (0.155 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3297.0 / guessed_by_relation= 5743.0\n",
      "Calculating Recall: correct_by_relation= 3297.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.409%\n",
      "   Recall (micro): 51.045%\n",
      "       F1 (micro): 54.040%\n",
      "epoch 5: train_loss = 0.263409, dev_loss = 0.391912, dev_f1 = 0.5404\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:01:39.717064: step 7600/300200 (epoch 6/200), loss = 0.254049 (0.159 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:02:45.411823: step 8000/300200 (epoch 6/200), loss = 0.187148 (0.171 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:03:52.973549: step 8400/300200 (epoch 6/200), loss = 0.237204 (0.171 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:04:59.825387: step 8800/300200 (epoch 6/200), loss = 0.245183 (0.152 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2442.0 / guessed_by_relation= 3644.0\n",
      "Calculating Recall: correct_by_relation= 2442.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 67.014%\n",
      "   Recall (micro): 37.808%\n",
      "       F1 (micro): 48.342%\n",
      "epoch 6: train_loss = 0.254948, dev_loss = 0.314908, dev_f1 = 0.4834\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "\n",
      "2018-03-12 12:06:27.843634: step 9200/300200 (epoch 7/200), loss = 0.378831 (0.152 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:07:32.492612: step 9600/300200 (epoch 7/200), loss = 0.151022 (0.171 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:08:40.019245: step 10000/300200 (epoch 7/200), loss = 0.185678 (0.153 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:09:47.304105: step 10400/300200 (epoch 7/200), loss = 0.118902 (0.176 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3228.0 / guessed_by_relation= 5246.0\n",
      "Calculating Recall: correct_by_relation= 3228.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.533%\n",
      "   Recall (micro): 49.977%\n",
      "       F1 (micro): 55.156%\n",
      "epoch 7: train_loss = 0.248309, dev_loss = 0.356286, dev_f1 = 0.5516\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:11:17.764813: step 10800/300200 (epoch 8/200), loss = 0.199105 (0.153 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:12:23.896637: step 11200/300200 (epoch 8/200), loss = 0.409911 (0.165 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:13:32.149201: step 11600/300200 (epoch 8/200), loss = 0.258687 (0.175 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:14:40.870010: step 12000/300200 (epoch 8/200), loss = 0.123811 (0.138 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3496.0 / guessed_by_relation= 5675.0\n",
      "Calculating Recall: correct_by_relation= 3496.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.604%\n",
      "   Recall (micro): 54.126%\n",
      "       F1 (micro): 57.623%\n",
      "epoch 8: train_loss = 0.243227, dev_loss = 0.355115, dev_f1 = 0.5762\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:16:09.842215: step 12400/300200 (epoch 9/200), loss = 0.262626 (0.145 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:17:16.738146: step 12800/300200 (epoch 9/200), loss = 0.185109 (0.165 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:18:24.802670: step 13200/300200 (epoch 9/200), loss = 0.277579 (0.178 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3395.0 / guessed_by_relation= 5436.0\n",
      "Calculating Recall: correct_by_relation= 3395.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.454%\n",
      "   Recall (micro): 52.562%\n",
      "       F1 (micro): 57.083%\n",
      "epoch 9: train_loss = 0.239522, dev_loss = 0.344246, dev_f1 = 0.5708\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "\n",
      "2018-03-12 12:19:55.169116: step 13600/300200 (epoch 10/200), loss = 0.293008 (0.171 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:21:00.818756: step 14000/300200 (epoch 10/200), loss = 0.218038 (0.192 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:22:08.059176: step 14400/300200 (epoch 10/200), loss = 0.196739 (0.167 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:23:16.085136: step 14800/300200 (epoch 10/200), loss = 0.266394 (0.145 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3568.0 / guessed_by_relation= 5650.0\n",
      "Calculating Recall: correct_by_relation= 3568.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.150%\n",
      "   Recall (micro): 55.241%\n",
      "       F1 (micro): 58.931%\n",
      "epoch 10: train_loss = 0.234153, dev_loss = 0.343790, dev_f1 = 0.5893\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:24:47.439176: step 15200/300200 (epoch 11/200), loss = 0.388862 (0.130 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:25:52.397653: step 15600/300200 (epoch 11/200), loss = 0.259358 (0.218 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:27:03.824657: step 16000/300200 (epoch 11/200), loss = 0.205049 (0.167 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:28:12.738409: step 16400/300200 (epoch 11/200), loss = 0.161991 (0.180 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3550.0 / guessed_by_relation= 5635.0\n",
      "Calculating Recall: correct_by_relation= 3550.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.999%\n",
      "   Recall (micro): 54.962%\n",
      "       F1 (micro): 58.707%\n",
      "epoch 11: train_loss = 0.230789, dev_loss = 0.344392, dev_f1 = 0.5871\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "2018-03-12 12:29:41.640292: step 16800/300200 (epoch 12/200), loss = 0.336820 (0.164 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:30:45.287587: step 17200/300200 (epoch 12/200), loss = 0.222862 (0.130 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:31:52.801185: step 17600/300200 (epoch 12/200), loss = 0.188214 (0.153 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:33:00.274676: step 18000/300200 (epoch 12/200), loss = 0.155921 (0.171 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3834.0 / guessed_by_relation= 6386.0\n",
      "Calculating Recall: correct_by_relation= 3834.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.038%\n",
      "   Recall (micro): 59.359%\n",
      "       F1 (micro): 59.696%\n",
      "epoch 12: train_loss = 0.227355, dev_loss = 0.368998, dev_f1 = 0.5970\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:34:30.303372: step 18400/300200 (epoch 13/200), loss = 0.312672 (0.206 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:35:34.164237: step 18800/300200 (epoch 13/200), loss = 0.117236 (0.171 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:36:50.952185: step 19200/300200 (epoch 13/200), loss = 0.158101 (0.161 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3843.0 / guessed_by_relation= 6234.0\n",
      "Calculating Recall: correct_by_relation= 3843.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.646%\n",
      "   Recall (micro): 59.498%\n",
      "       F1 (micro): 60.553%\n",
      "epoch 13: train_loss = 0.224627, dev_loss = 0.362096, dev_f1 = 0.6055\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:38:29.398962: step 19600/300200 (epoch 14/200), loss = 0.178080 (0.186 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:39:48.018716: step 20000/300200 (epoch 14/200), loss = 0.202422 (0.214 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:41:03.607784: step 20400/300200 (epoch 14/200), loss = 0.192450 (0.213 sec/batch), lr: 0.150000\n",
      "2018-03-12 12:42:22.470926: step 20800/300200 (epoch 14/200), loss = 0.169165 (0.190 sec/batch), lr: 0.150000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3043.0 / guessed_by_relation= 4427.0\n",
      "Calculating Recall: correct_by_relation= 3043.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 68.737%\n",
      "   Recall (micro): 47.113%\n",
      "       F1 (micro): 55.907%\n",
      "epoch 14: train_loss = 0.222143, dev_loss = 0.305274, dev_f1 = 0.5591\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "2018-03-12 12:44:01.643743: step 21200/300200 (epoch 15/200), loss = 0.278551 (0.187 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:45:18.616428: step 21600/300200 (epoch 15/200), loss = 0.192444 (0.161 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:46:34.132313: step 22000/300200 (epoch 15/200), loss = 0.204955 (0.168 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:47:51.397994: step 22400/300200 (epoch 15/200), loss = 0.261855 (0.192 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3699.0 / guessed_by_relation= 5717.0\n",
      "Calculating Recall: correct_by_relation= 3699.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 64.702%\n",
      "   Recall (micro): 57.269%\n",
      "       F1 (micro): 60.759%\n",
      "epoch 15: train_loss = 0.218935, dev_loss = 0.340161, dev_f1 = 0.6076\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:49:31.033850: step 22800/300200 (epoch 16/200), loss = 0.163830 (0.198 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:50:47.970515: step 23200/300200 (epoch 16/200), loss = 0.217694 (0.174 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:52:03.516275: step 23600/300200 (epoch 16/200), loss = 0.199290 (0.179 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:53:19.872534: step 24000/300200 (epoch 16/200), loss = 0.245119 (0.193 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3913.0 / guessed_by_relation= 6421.0\n",
      "Calculating Recall: correct_by_relation= 3913.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.941%\n",
      "   Recall (micro): 60.582%\n",
      "       F1 (micro): 60.761%\n",
      "epoch 16: train_loss = 0.216458, dev_loss = 0.364662, dev_f1 = 0.6076\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 12:54:59.545682: step 24400/300200 (epoch 17/200), loss = 0.179207 (0.185 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:56:15.871699: step 24800/300200 (epoch 17/200), loss = 0.200798 (0.196 sec/batch), lr: 0.142500\n",
      "2018-03-12 12:57:31.915991: step 25200/300200 (epoch 17/200), loss = 0.234741 (0.154 sec/batch), lr: 0.142500\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3814.0 / guessed_by_relation= 6156.0\n",
      "Calculating Recall: correct_by_relation= 3814.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.956%\n",
      "   Recall (micro): 59.049%\n",
      "       F1 (micro): 60.468%\n",
      "epoch 17: train_loss = 0.214318, dev_loss = 0.356355, dev_f1 = 0.6047\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "\n",
      "2018-03-12 12:59:11.175037: step 25600/300200 (epoch 18/200), loss = 0.144282 (0.200 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:00:27.649472: step 26000/300200 (epoch 18/200), loss = 0.153313 (0.182 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:01:44.608337: step 26400/300200 (epoch 18/200), loss = 0.177900 (0.185 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:03:24.151273: step 26800/300200 (epoch 18/200), loss = 0.284123 (0.224 sec/batch), lr: 0.135375\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4159.0 / guessed_by_relation= 7019.0\n",
      "Calculating Recall: correct_by_relation= 4159.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.253%\n",
      "   Recall (micro): 64.391%\n",
      "       F1 (micro): 61.715%\n",
      "epoch 18: train_loss = 0.211588, dev_loss = 0.393248, dev_f1 = 0.6172\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:05:13.221824: step 27200/300200 (epoch 19/200), loss = 0.276071 (0.153 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:06:32.604008: step 27600/300200 (epoch 19/200), loss = 0.099603 (0.152 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:07:51.809505: step 28000/300200 (epoch 19/200), loss = 0.101345 (0.152 sec/batch), lr: 0.135375\n",
      "2018-03-12 13:09:04.946751: step 28400/300200 (epoch 19/200), loss = 0.229633 (0.152 sec/batch), lr: 0.135375\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3822.0 / guessed_by_relation= 6062.0\n",
      "Calculating Recall: correct_by_relation= 3822.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.048%\n",
      "   Recall (micro): 59.173%\n",
      "       F1 (micro): 61.049%\n",
      "epoch 19: train_loss = 0.207936, dev_loss = 0.347716, dev_f1 = 0.6105\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "\n",
      "2018-03-12 13:10:47.735139: step 28800/300200 (epoch 20/200), loss = 0.296816 (0.189 sec/batch), lr: 0.128606\n",
      "2018-03-12 13:12:06.612958: step 29200/300200 (epoch 20/200), loss = 0.141621 (0.243 sec/batch), lr: 0.128606\n",
      "2018-03-12 13:13:24.712747: step 29600/300200 (epoch 20/200), loss = 0.142210 (0.135 sec/batch), lr: 0.128606\n",
      "2018-03-12 13:14:41.576217: step 30000/300200 (epoch 20/200), loss = 0.171235 (0.192 sec/batch), lr: 0.128606\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4145.0 / guessed_by_relation= 7157.0\n",
      "Calculating Recall: correct_by_relation= 4145.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.915%\n",
      "   Recall (micro): 64.174%\n",
      "       F1 (micro): 60.884%\n",
      "epoch 20: train_loss = 0.205246, dev_loss = 0.401388, dev_f1 = 0.6088\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "2018-03-12 13:16:27.662425: step 30400/300200 (epoch 21/200), loss = 0.121480 (0.193 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:17:38.409848: step 30800/300200 (epoch 21/200), loss = 0.186633 (0.189 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:18:55.725813: step 31200/300200 (epoch 21/200), loss = 0.141147 (0.176 sec/batch), lr: 0.122176\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4185.0 / guessed_by_relation= 7115.0\n",
      "Calculating Recall: correct_by_relation= 4185.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.819%\n",
      "   Recall (micro): 64.793%\n",
      "       F1 (micro): 61.662%\n",
      "epoch 21: train_loss = 0.204496, dev_loss = 0.396542, dev_f1 = 0.6166\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "2018-03-12 13:20:40.884061: step 31600/300200 (epoch 22/200), loss = 0.266560 (0.163 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:21:59.778935: step 32000/300200 (epoch 22/200), loss = 0.096707 (0.219 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:23:18.954556: step 32400/300200 (epoch 22/200), loss = 0.189050 (0.192 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:24:32.165180: step 32800/300200 (epoch 22/200), loss = 0.135531 (0.155 sec/batch), lr: 0.122176\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4222.0 / guessed_by_relation= 7219.0\n",
      "Calculating Recall: correct_by_relation= 4222.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.485%\n",
      "   Recall (micro): 65.366%\n",
      "       F1 (micro): 61.734%\n",
      "epoch 22: train_loss = 0.201856, dev_loss = 0.402127, dev_f1 = 0.6173\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:26:13.689251: step 33200/300200 (epoch 23/200), loss = 0.131758 (0.197 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:27:32.611197: step 33600/300200 (epoch 23/200), loss = 0.142850 (0.218 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:28:51.180204: step 34000/300200 (epoch 23/200), loss = 0.295381 (0.235 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:30:07.665669: step 34400/300200 (epoch 23/200), loss = 0.117141 (0.209 sec/batch), lr: 0.122176\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4160.0 / guessed_by_relation= 6968.0\n",
      "Calculating Recall: correct_by_relation= 4160.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.701%\n",
      "   Recall (micro): 64.406%\n",
      "       F1 (micro): 61.965%\n",
      "epoch 23: train_loss = 0.199483, dev_loss = 0.380272, dev_f1 = 0.6196\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:31:53.886234: step 34800/300200 (epoch 24/200), loss = 0.571456 (0.186 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:33:04.175215: step 35200/300200 (epoch 24/200), loss = 0.163781 (0.158 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:34:22.702110: step 35600/300200 (epoch 24/200), loss = 0.148214 (0.219 sec/batch), lr: 0.122176\n",
      "2018-03-12 13:35:39.424205: step 36000/300200 (epoch 24/200), loss = 0.257902 (0.195 sec/batch), lr: 0.122176\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4066.0 / guessed_by_relation= 6745.0\n",
      "Calculating Recall: correct_by_relation= 4066.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.282%\n",
      "   Recall (micro): 62.951%\n",
      "       F1 (micro): 61.587%\n",
      "epoch 24: train_loss = 0.198789, dev_loss = 0.380205, dev_f1 = 0.6159\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "\n",
      "2018-03-12 13:37:24.828695: step 36400/300200 (epoch 25/200), loss = 0.153378 (0.197 sec/batch), lr: 0.116067\n",
      "2018-03-12 13:38:43.530054: step 36800/300200 (epoch 25/200), loss = 0.232897 (0.205 sec/batch), lr: 0.116067\n",
      "2018-03-12 13:39:57.910993: step 37200/300200 (epoch 25/200), loss = 0.153559 (0.149 sec/batch), lr: 0.116067\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4214.0 / guessed_by_relation= 7248.0\n",
      "Calculating Recall: correct_by_relation= 4214.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.140%\n",
      "   Recall (micro): 65.242%\n",
      "       F1 (micro): 61.487%\n",
      "epoch 25: train_loss = 0.196405, dev_loss = 0.412838, dev_f1 = 0.6149\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "2018-03-12 13:41:39.658606: step 37600/300200 (epoch 26/200), loss = 0.201781 (0.189 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:42:58.452211: step 38000/300200 (epoch 26/200), loss = 0.151744 (0.136 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:44:17.226765: step 38400/300200 (epoch 26/200), loss = 0.137502 (0.190 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:45:33.436366: step 38800/300200 (epoch 26/200), loss = 0.204292 (0.224 sec/batch), lr: 0.110264\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4036.0 / guessed_by_relation= 6471.0\n",
      "Calculating Recall: correct_by_relation= 4036.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.371%\n",
      "   Recall (micro): 62.486%\n",
      "       F1 (micro): 62.428%\n",
      "epoch 26: train_loss = 0.193478, dev_loss = 0.365097, dev_f1 = 0.6243\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 13:47:19.733134: step 39200/300200 (epoch 27/200), loss = 0.071213 (0.200 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:48:31.312548: step 39600/300200 (epoch 27/200), loss = 0.413432 (0.209 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:49:50.868179: step 40000/300200 (epoch 27/200), loss = 0.243118 (0.159 sec/batch), lr: 0.110264\n",
      "2018-03-12 13:51:07.010732: step 40400/300200 (epoch 27/200), loss = 0.249477 (0.206 sec/batch), lr: 0.110264\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4140.0 / guessed_by_relation= 6889.0\n",
      "Calculating Recall: correct_by_relation= 4140.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.096%\n",
      "   Recall (micro): 64.097%\n",
      "       F1 (micro): 62.032%\n",
      "epoch 27: train_loss = 0.192709, dev_loss = 0.393204, dev_f1 = 0.6203\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "2018-03-12 13:52:52.616662: step 40800/300200 (epoch 28/200), loss = 0.192498 (0.172 sec/batch), lr: 0.104751\n",
      "2018-03-12 13:54:11.242822: step 41200/300200 (epoch 28/200), loss = 0.219403 (0.201 sec/batch), lr: 0.104751\n",
      "2018-03-12 13:55:25.946546: step 41600/300200 (epoch 28/200), loss = 0.091205 (0.152 sec/batch), lr: 0.104751\n",
      "2018-03-12 13:56:38.705097: step 42000/300200 (epoch 28/200), loss = 0.253483 (0.211 sec/batch), lr: 0.104751\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3997.0 / guessed_by_relation= 6585.0\n",
      "Calculating Recall: correct_by_relation= 3997.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.699%\n",
      "   Recall (micro): 61.883%\n",
      "       F1 (micro): 61.285%\n",
      "epoch 28: train_loss = 0.189681, dev_loss = 0.376139, dev_f1 = 0.6128\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "\n",
      "2018-03-12 13:58:25.347785: step 42400/300200 (epoch 29/200), loss = 0.206609 (0.191 sec/batch), lr: 0.099513\n",
      "2018-03-12 13:59:43.692195: step 42800/300200 (epoch 29/200), loss = 0.261957 (0.230 sec/batch), lr: 0.099513\n",
      "2018-03-12 14:01:00.712081: step 43200/300200 (epoch 29/200), loss = 0.073874 (0.189 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4013.0 / guessed_by_relation= 6466.0\n",
      "Calculating Recall: correct_by_relation= 4013.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.063%\n",
      "   Recall (micro): 62.130%\n",
      "       F1 (micro): 62.097%\n",
      "epoch 29: train_loss = 0.188337, dev_loss = 0.369352, dev_f1 = 0.6210\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "\n",
      "2018-03-12 14:02:46.016208: step 43600/300200 (epoch 30/200), loss = 0.140170 (0.175 sec/batch), lr: 0.099513\n",
      "2018-03-12 14:03:56.155791: step 44000/300200 (epoch 30/200), loss = 0.214191 (0.162 sec/batch), lr: 0.099513\n",
      "2018-03-12 14:05:14.999530: step 44400/300200 (epoch 30/200), loss = 0.169787 (0.209 sec/batch), lr: 0.099513\n",
      "2018-03-12 14:06:31.233325: step 44800/300200 (epoch 30/200), loss = 0.225167 (0.201 sec/batch), lr: 0.099513\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3949.0 / guessed_by_relation= 6398.0\n",
      "Calculating Recall: correct_by_relation= 3949.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.722%\n",
      "   Recall (micro): 61.139%\n",
      "       F1 (micro): 61.430%\n",
      "epoch 30: train_loss = 0.187108, dev_loss = 0.367897, dev_f1 = 0.6143\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "2018-03-12 14:08:17.826883: step 45200/300200 (epoch 31/200), loss = 0.202916 (0.199 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:09:36.463068: step 45600/300200 (epoch 31/200), loss = 0.248416 (0.226 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:10:50.884041: step 46000/300200 (epoch 31/200), loss = 0.193981 (0.165 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:12:02.891593: step 46400/300200 (epoch 31/200), loss = 0.067730 (0.170 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4052.0 / guessed_by_relation= 6510.0\n",
      "Calculating Recall: correct_by_relation= 4052.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.243%\n",
      "   Recall (micro): 62.734%\n",
      "       F1 (micro): 62.487%\n",
      "epoch 31: train_loss = 0.183278, dev_loss = 0.371365, dev_f1 = 0.6249\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 14:13:49.245513: step 46800/300200 (epoch 32/200), loss = 0.147682 (0.278 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:15:07.323213: step 47200/300200 (epoch 32/200), loss = 0.339981 (0.215 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:16:25.717757: step 47600/300200 (epoch 32/200), loss = 0.126951 (0.191 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:17:42.346603: step 48000/300200 (epoch 32/200), loss = 0.137878 (0.231 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4134.0 / guessed_by_relation= 6772.0\n",
      "Calculating Recall: correct_by_relation= 4134.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.045%\n",
      "   Recall (micro): 64.004%\n",
      "       F1 (micro): 62.490%\n",
      "epoch 32: train_loss = 0.182791, dev_loss = 0.387459, dev_f1 = 0.6249\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 14:19:21.257723: step 48400/300200 (epoch 33/200), loss = 0.228458 (0.155 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:20:39.800661: step 48800/300200 (epoch 33/200), loss = 0.329192 (0.222 sec/batch), lr: 0.094537\n",
      "2018-03-12 14:21:56.668142: step 49200/300200 (epoch 33/200), loss = 0.099807 (0.209 sec/batch), lr: 0.094537\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4116.0 / guessed_by_relation= 6733.0\n",
      "Calculating Recall: correct_by_relation= 4116.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.132%\n",
      "   Recall (micro): 63.725%\n",
      "       F1 (micro): 62.401%\n",
      "epoch 33: train_loss = 0.182078, dev_loss = 0.384399, dev_f1 = 0.6240\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "\n",
      "2018-03-12 14:23:42.665112: step 49600/300200 (epoch 34/200), loss = 0.286024 (0.151 sec/batch), lr: 0.089811\n",
      "2018-03-12 14:25:01.154909: step 50000/300200 (epoch 34/200), loss = 0.151789 (0.205 sec/batch), lr: 0.089811\n",
      "2018-03-12 14:26:15.141727: step 50400/300200 (epoch 34/200), loss = 0.141641 (0.137 sec/batch), lr: 0.089811\n",
      "2018-03-12 14:27:26.543669: step 50800/300200 (epoch 34/200), loss = 0.169012 (0.182 sec/batch), lr: 0.089811\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4164.0 / guessed_by_relation= 6916.0\n",
      "Calculating Recall: correct_by_relation= 4164.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.208%\n",
      "   Recall (micro): 64.468%\n",
      "       F1 (micro): 62.265%\n",
      "epoch 34: train_loss = 0.179818, dev_loss = 0.401851, dev_f1 = 0.6227\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "2018-03-12 14:29:13.000863: step 51200/300200 (epoch 35/200), loss = 0.166502 (0.207 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:30:31.578894: step 51600/300200 (epoch 35/200), loss = 0.085142 (0.184 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:31:50.348435: step 52000/300200 (epoch 35/200), loss = 0.203946 (0.161 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:33:06.328555: step 52400/300200 (epoch 35/200), loss = 0.117679 (0.242 sec/batch), lr: 0.085320\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4125.0 / guessed_by_relation= 6779.0\n",
      "Calculating Recall: correct_by_relation= 4125.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.850%\n",
      "   Recall (micro): 63.864%\n",
      "       F1 (micro): 62.321%\n",
      "epoch 35: train_loss = 0.177407, dev_loss = 0.391970, dev_f1 = 0.6232\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "\n",
      "2018-03-12 14:34:44.948903: step 52800/300200 (epoch 36/200), loss = 0.179185 (0.171 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:36:03.603136: step 53200/300200 (epoch 36/200), loss = 0.130662 (0.197 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:37:21.767066: step 53600/300200 (epoch 36/200), loss = 0.218406 (0.187 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:38:38.204403: step 54000/300200 (epoch 36/200), loss = 0.167671 (0.150 sec/batch), lr: 0.085320\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3999.0 / guessed_by_relation= 6370.0\n",
      "Calculating Recall: correct_by_relation= 3999.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.779%\n",
      "   Recall (micro): 61.914%\n",
      "       F1 (micro): 62.343%\n",
      "epoch 36: train_loss = 0.177122, dev_loss = 0.370098, dev_f1 = 0.6234\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "2018-03-12 14:40:24.381853: step 54400/300200 (epoch 37/200), loss = 0.143672 (0.192 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:41:38.742666: step 54800/300200 (epoch 37/200), loss = 0.127739 (0.169 sec/batch), lr: 0.085320\n",
      "2018-03-12 14:42:51.429024: step 55200/300200 (epoch 37/200), loss = 0.239100 (0.175 sec/batch), lr: 0.085320\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4151.0 / guessed_by_relation= 6934.0\n",
      "Calculating Recall: correct_by_relation= 4151.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.864%\n",
      "   Recall (micro): 64.267%\n",
      "       F1 (micro): 61.988%\n",
      "epoch 37: train_loss = 0.173294, dev_loss = 0.403715, dev_f1 = 0.6199\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "2018-03-12 14:44:37.347787: step 55600/300200 (epoch 38/200), loss = 0.099410 (0.175 sec/batch), lr: 0.081054\n",
      "2018-03-12 14:45:55.760378: step 56000/300200 (epoch 38/200), loss = 0.117950 (0.194 sec/batch), lr: 0.081054\n",
      "2018-03-12 14:47:14.557993: step 56400/300200 (epoch 38/200), loss = 0.286255 (0.186 sec/batch), lr: 0.081054\n",
      "2018-03-12 14:48:30.478956: step 56800/300200 (epoch 38/200), loss = 0.087566 (0.200 sec/batch), lr: 0.081054\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4097.0 / guessed_by_relation= 6810.0\n",
      "Calculating Recall: correct_by_relation= 4097.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.162%\n",
      "   Recall (micro): 63.431%\n",
      "       F1 (micro): 61.753%\n",
      "epoch 38: train_loss = 0.173145, dev_loss = 0.403941, dev_f1 = 0.6175\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "2018-03-12 14:50:08.864679: step 57200/300200 (epoch 39/200), loss = 0.151242 (0.169 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:51:27.159958: step 57600/300200 (epoch 39/200), loss = 0.284330 (0.186 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:52:45.527429: step 58000/300200 (epoch 39/200), loss = 0.316000 (0.205 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:54:01.672990: step 58400/300200 (epoch 39/200), loss = 0.134503 (0.193 sec/batch), lr: 0.077001\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4132.0 / guessed_by_relation= 6819.0\n",
      "Calculating Recall: correct_by_relation= 4132.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.595%\n",
      "   Recall (micro): 63.973%\n",
      "       F1 (micro): 62.238%\n",
      "epoch 39: train_loss = 0.171410, dev_loss = 0.401217, dev_f1 = 0.6224\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "2018-03-12 14:55:48.100104: step 58800/300200 (epoch 40/200), loss = 0.203930 (0.177 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:57:02.253365: step 59200/300200 (epoch 40/200), loss = 0.154562 (0.149 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:58:16.761570: step 59600/300200 (epoch 40/200), loss = 0.185507 (0.166 sec/batch), lr: 0.077001\n",
      "2018-03-12 14:59:33.542822: step 60000/300200 (epoch 40/200), loss = 0.244415 (0.186 sec/batch), lr: 0.077001\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4059.0 / guessed_by_relation= 6700.0\n",
      "Calculating Recall: correct_by_relation= 4059.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.582%\n",
      "   Recall (micro): 62.843%\n",
      "       F1 (micro): 61.692%\n",
      "epoch 40: train_loss = 0.169839, dev_loss = 0.398974, dev_f1 = 0.6169\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "2018-03-12 15:01:18.966266: step 60400/300200 (epoch 41/200), loss = 0.178577 (0.193 sec/batch), lr: 0.073151\n",
      "2018-03-12 15:02:37.481129: step 60800/300200 (epoch 41/200), loss = 0.151361 (0.204 sec/batch), lr: 0.073151\n",
      "2018-03-12 15:03:54.552151: step 61200/300200 (epoch 41/200), loss = 0.177812 (0.209 sec/batch), lr: 0.073151\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4172.0 / guessed_by_relation= 6857.0\n",
      "Calculating Recall: correct_by_relation= 4172.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.843%\n",
      "   Recall (micro): 64.592%\n",
      "       F1 (micro): 62.661%\n",
      "epoch 41: train_loss = 0.167439, dev_loss = 0.411194, dev_f1 = 0.6266\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_41.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-12 15:05:31.623378: step 61600/300200 (epoch 42/200), loss = 0.154646 (0.216 sec/batch), lr: 0.073151\n",
      "2018-03-12 15:06:50.390913: step 62000/300200 (epoch 42/200), loss = 0.319995 (0.190 sec/batch), lr: 0.073151\n",
      "2018-03-12 15:08:08.952901: step 62400/300200 (epoch 42/200), loss = 0.164885 (0.170 sec/batch), lr: 0.073151\n",
      "2018-03-12 15:09:24.968115: step 62800/300200 (epoch 42/200), loss = 0.159917 (0.186 sec/batch), lr: 0.073151\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4041.0 / guessed_by_relation= 6484.0\n",
      "Calculating Recall: correct_by_relation= 4041.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.323%\n",
      "   Recall (micro): 62.564%\n",
      "       F1 (micro): 62.443%\n",
      "epoch 42: train_loss = 0.167274, dev_loss = 0.389577, dev_f1 = 0.6244\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_42.pt\n",
      "\n",
      "2018-03-12 15:11:11.103454: step 63200/300200 (epoch 43/200), loss = 0.160189 (0.187 sec/batch), lr: 0.069494\n",
      "2018-03-12 15:12:25.628704: step 63600/300200 (epoch 43/200), loss = 0.107749 (0.156 sec/batch), lr: 0.069494\n",
      "2018-03-12 15:13:40.055693: step 64000/300200 (epoch 43/200), loss = 0.293382 (0.235 sec/batch), lr: 0.069494\n",
      "2018-03-12 15:14:55.989691: step 64400/300200 (epoch 43/200), loss = 0.126913 (0.182 sec/batch), lr: 0.069494\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4181.0 / guessed_by_relation= 6982.0\n",
      "Calculating Recall: correct_by_relation= 4181.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.883%\n",
      "   Recall (micro): 64.731%\n",
      "       F1 (micro): 62.213%\n",
      "epoch 43: train_loss = 0.165515, dev_loss = 0.412909, dev_f1 = 0.6221\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_43.pt\n",
      "\n",
      "2018-03-12 15:16:42.062863: step 64800/300200 (epoch 44/200), loss = 0.121677 (0.190 sec/batch), lr: 0.066019\n",
      "2018-03-12 15:18:00.832404: step 65200/300200 (epoch 44/200), loss = 0.120428 (0.191 sec/batch), lr: 0.066019\n",
      "2018-03-12 15:19:18.978286: step 65600/300200 (epoch 44/200), loss = 0.291910 (0.203 sec/batch), lr: 0.066019\n",
      "2018-03-12 15:20:27.725164: step 66000/300200 (epoch 44/200), loss = 0.260291 (0.175 sec/batch), lr: 0.066019\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4211.0 / guessed_by_relation= 6986.0\n",
      "Calculating Recall: correct_by_relation= 4211.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.278%\n",
      "   Recall (micro): 65.196%\n",
      "       F1 (micro): 62.640%\n",
      "epoch 44: train_loss = 0.163277, dev_loss = 0.421682, dev_f1 = 0.6264\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_44.pt\n",
      "\n",
      "2018-03-12 15:22:13.653953: step 66400/300200 (epoch 45/200), loss = 0.118053 (0.179 sec/batch), lr: 0.066019\n",
      "2018-03-12 15:23:32.319216: step 66800/300200 (epoch 45/200), loss = 0.155767 (0.211 sec/batch), lr: 0.066019\n",
      "2018-03-12 15:24:49.132553: step 67200/300200 (epoch 45/200), loss = 0.295700 (0.173 sec/batch), lr: 0.066019\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3943.0 / guessed_by_relation= 6211.0\n",
      "Calculating Recall: correct_by_relation= 3943.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.484%\n",
      "   Recall (micro): 61.047%\n",
      "       F1 (micro): 62.242%\n",
      "epoch 45: train_loss = 0.162350, dev_loss = 0.377240, dev_f1 = 0.6224\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_45.pt\n",
      "\n",
      "2018-03-12 15:26:34.473779: step 67600/300200 (epoch 46/200), loss = 0.197932 (0.206 sec/batch), lr: 0.062718\n",
      "2018-03-12 15:27:49.065205: step 68000/300200 (epoch 46/200), loss = 0.229547 (0.136 sec/batch), lr: 0.062718\n",
      "2018-03-12 15:29:03.850146: step 68400/300200 (epoch 46/200), loss = 0.164046 (0.183 sec/batch), lr: 0.062718\n",
      "2018-03-12 15:30:19.457275: step 68800/300200 (epoch 46/200), loss = 0.161777 (0.175 sec/batch), lr: 0.062718\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4023.0 / guessed_by_relation= 6628.0\n",
      "Calculating Recall: correct_by_relation= 4023.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.697%\n",
      "   Recall (micro): 62.285%\n",
      "       F1 (micro): 61.481%\n",
      "epoch 46: train_loss = 0.159616, dev_loss = 0.409203, dev_f1 = 0.6148\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_46.pt\n",
      "\n",
      "2018-03-12 15:32:05.317882: step 69200/300200 (epoch 47/200), loss = 0.194809 (0.171 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:33:24.230804: step 69600/300200 (epoch 47/200), loss = 0.138160 (0.191 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:34:42.929156: step 70000/300200 (epoch 47/200), loss = 0.276024 (0.148 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:35:51.192748: step 70400/300200 (epoch 47/200), loss = 0.128532 (0.222 sec/batch), lr: 0.059582\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4146.0 / guessed_by_relation= 6844.0\n",
      "Calculating Recall: correct_by_relation= 4146.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.579%\n",
      "   Recall (micro): 64.190%\n",
      "       F1 (micro): 62.332%\n",
      "epoch 47: train_loss = 0.158727, dev_loss = 0.422941, dev_f1 = 0.6233\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_47.pt\n",
      "\n",
      "2018-03-12 15:37:36.654294: step 70800/300200 (epoch 48/200), loss = 0.157406 (0.219 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:38:54.944560: step 71200/300200 (epoch 48/200), loss = 0.255744 (0.221 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:40:13.723125: step 71600/300200 (epoch 48/200), loss = 0.038289 (0.187 sec/batch), lr: 0.059582\n",
      "2018-03-12 15:41:30.027107: step 72000/300200 (epoch 48/200), loss = 0.128424 (0.192 sec/batch), lr: 0.059582\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3956.0 / guessed_by_relation= 6347.0\n",
      "Calculating Recall: correct_by_relation= 3956.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.329%\n",
      "   Recall (micro): 61.248%\n",
      "       F1 (micro): 61.784%\n",
      "epoch 48: train_loss = 0.158160, dev_loss = 0.390097, dev_f1 = 0.6178\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_48.pt\n",
      "\n",
      "2018-03-12 15:43:12.186869: step 72400/300200 (epoch 49/200), loss = 0.179660 (0.139 sec/batch), lr: 0.056603\n",
      "2018-03-12 15:44:25.696418: step 72800/300200 (epoch 49/200), loss = 0.263496 (0.203 sec/batch), lr: 0.056603\n",
      "2018-03-12 15:45:42.906811: step 73200/300200 (epoch 49/200), loss = 0.147027 (0.166 sec/batch), lr: 0.056603\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4151.0 / guessed_by_relation= 7150.0\n",
      "Calculating Recall: correct_by_relation= 4151.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.056%\n",
      "   Recall (micro): 64.267%\n",
      "       F1 (micro): 61.004%\n",
      "epoch 49: train_loss = 0.154949, dev_loss = 0.445966, dev_f1 = 0.6100\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_49.pt\n",
      "\n",
      "2018-03-12 15:47:28.001381: step 73600/300200 (epoch 50/200), loss = 0.244359 (0.198 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:48:46.815038: step 74000/300200 (epoch 50/200), loss = 0.099833 (0.192 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:50:05.327896: step 74400/300200 (epoch 50/200), loss = 0.217595 (0.216 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:51:14.126914: step 74800/300200 (epoch 50/200), loss = 0.120044 (0.231 sec/batch), lr: 0.053773\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4209.0 / guessed_by_relation= 7259.0\n",
      "Calculating Recall: correct_by_relation= 4209.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.983%\n",
      "   Recall (micro): 65.165%\n",
      "       F1 (micro): 61.365%\n",
      "epoch 50: train_loss = 0.154231, dev_loss = 0.457110, dev_f1 = 0.6136\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_50.pt\n",
      "\n",
      "2018-03-12 15:53:00.428695: step 75200/300200 (epoch 51/200), loss = 0.081845 (0.203 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:54:19.152113: step 75600/300200 (epoch 51/200), loss = 0.241419 (0.186 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:55:37.500534: step 76000/300200 (epoch 51/200), loss = 0.164886 (0.198 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:56:53.708259: step 76400/300200 (epoch 51/200), loss = 0.146923 (0.212 sec/batch), lr: 0.053773\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4114.0 / guessed_by_relation= 6774.0\n",
      "Calculating Recall: correct_by_relation= 4114.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.732%\n",
      "   Recall (micro): 63.694%\n",
      "       F1 (micro): 62.178%\n",
      "epoch 51: train_loss = 0.153886, dev_loss = 0.420004, dev_f1 = 0.6218\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_51.pt\n",
      "\n",
      "2018-03-12 15:58:36.221964: step 76800/300200 (epoch 52/200), loss = 0.217743 (0.157 sec/batch), lr: 0.053773\n",
      "2018-03-12 15:59:50.041336: step 77200/300200 (epoch 52/200), loss = 0.132741 (0.187 sec/batch), lr: 0.053773\n",
      "2018-03-12 16:01:08.681532: step 77600/300200 (epoch 52/200), loss = 0.173188 (0.175 sec/batch), lr: 0.053773\n",
      "2018-03-12 16:02:24.601493: step 78000/300200 (epoch 52/200), loss = 0.138018 (0.235 sec/batch), lr: 0.053773\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3888.0 / guessed_by_relation= 6214.0\n",
      "Calculating Recall: correct_by_relation= 3888.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.568%\n",
      "   Recall (micro): 60.195%\n",
      "       F1 (micro): 61.359%\n",
      "epoch 52: train_loss = 0.153024, dev_loss = 0.394200, dev_f1 = 0.6136\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_52.pt\n",
      "\n",
      "2018-03-12 16:04:11.235157: step 78400/300200 (epoch 53/200), loss = 0.190236 (0.206 sec/batch), lr: 0.051084\n",
      "2018-03-12 16:05:29.859311: step 78800/300200 (epoch 53/200), loss = 0.207088 (0.216 sec/batch), lr: 0.051084\n",
      "2018-03-12 16:06:39.513656: step 79200/300200 (epoch 53/200), loss = 0.091283 (0.206 sec/batch), lr: 0.051084\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4192.0 / guessed_by_relation= 7032.0\n",
      "Calculating Recall: correct_by_relation= 4192.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.613%\n",
      "   Recall (micro): 64.902%\n",
      "       F1 (micro): 62.145%\n",
      "epoch 53: train_loss = 0.150698, dev_loss = 0.447396, dev_f1 = 0.6215\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_53.pt\n",
      "\n",
      "2018-03-12 16:08:25.219499: step 79600/300200 (epoch 54/200), loss = 0.076167 (0.195 sec/batch), lr: 0.051084\n",
      "2018-03-12 16:09:44.068635: step 80000/300200 (epoch 54/200), loss = 0.145844 (0.149 sec/batch), lr: 0.051084\n",
      "2018-03-12 16:11:02.628616: step 80400/300200 (epoch 54/200), loss = 0.180677 (0.193 sec/batch), lr: 0.051084\n",
      "2018-03-12 16:12:19.489493: step 80800/300200 (epoch 54/200), loss = 0.076330 (0.217 sec/batch), lr: 0.051084\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4126.0 / guessed_by_relation= 6868.0\n",
      "Calculating Recall: correct_by_relation= 4126.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.076%\n",
      "   Recall (micro): 63.880%\n",
      "       F1 (micro): 61.919%\n",
      "epoch 54: train_loss = 0.149064, dev_loss = 0.434286, dev_f1 = 0.6192\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_54.pt\n",
      "\n",
      "2018-03-12 16:14:06.751713: step 81200/300200 (epoch 55/200), loss = 0.104322 (0.139 sec/batch), lr: 0.048530\n",
      "2018-03-12 16:15:22.260587: step 81600/300200 (epoch 55/200), loss = 0.185722 (0.195 sec/batch), lr: 0.048530\n",
      "2018-03-12 16:16:40.896773: step 82000/300200 (epoch 55/200), loss = 0.164638 (0.190 sec/batch), lr: 0.048530\n",
      "2018-03-12 16:17:57.002626: step 82400/300200 (epoch 55/200), loss = 0.137498 (0.196 sec/batch), lr: 0.048530\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4092.0 / guessed_by_relation= 6897.0\n",
      "Calculating Recall: correct_by_relation= 4092.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.330%\n",
      "   Recall (micro): 63.353%\n",
      "       F1 (micro): 61.276%\n",
      "epoch 55: train_loss = 0.147918, dev_loss = 0.441567, dev_f1 = 0.6128\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_55.pt\n",
      "\n",
      "2018-03-12 16:19:43.503061: step 82800/300200 (epoch 56/200), loss = 0.184109 (0.194 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:21:02.712884: step 83200/300200 (epoch 56/200), loss = 0.178707 (0.199 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:22:13.401246: step 83600/300200 (epoch 56/200), loss = 0.193194 (0.159 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:23:30.639713: step 84000/300200 (epoch 56/200), loss = 0.147514 (0.185 sec/batch), lr: 0.046104\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4044.0 / guessed_by_relation= 6682.0\n",
      "Calculating Recall: correct_by_relation= 4044.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.521%\n",
      "   Recall (micro): 62.610%\n",
      "       F1 (micro): 61.548%\n",
      "epoch 56: train_loss = 0.146273, dev_loss = 0.432865, dev_f1 = 0.6155\n",
      "model saved to ./saved_models/31_self_attention_dropout/checkpoint_epoch_56.pt\n",
      "\n",
      "2018-03-12 16:25:54.117895: step 84400/300200 (epoch 57/200), loss = 0.167823 (0.330 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:27:29.948353: step 84800/300200 (epoch 57/200), loss = 0.196562 (0.189 sec/batch), lr: 0.046104\n",
      "2018-03-12 16:28:48.222098: step 85200/300200 (epoch 57/200), loss = 0.169120 (0.185 sec/batch), lr: 0.046104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6bc31db1d5bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;31m# print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    384\u001b[0m             outputs, enc_slf_attn = self.self_attention_encoder(\n\u001b[0;32m    385\u001b[0m                 \u001b[0menc_non_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[0msrc_seq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_self\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minst_position\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             )\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\Models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, enc_non_embedded, src_seq, src_pos)\u001b[0m\n\u001b[0;32m    111\u001b[0m             enc_output, enc_slf_attn = enc_layer(\n\u001b[0;32m    112\u001b[0m                 \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                 \u001b[0mslf_attn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_slf_attn_mask\u001b[0m  \u001b[1;31m# enc_slf_attn_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             )\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\Layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, enc_input, slf_attn_mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# here q, k, w are all the same at input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         enc_output, enc_slf_attn = self.slf_attn(\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0menc_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslf_attn_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         )\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\transformer\\SubLayers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, q, k, v, attn_mask)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mq_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# n_head x (mb_size*len_q) x d_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mk_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# n_head x (mb_size*len_k) x d_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mv_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# n_head x (mb_size*len_v) x d_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# treat the result as a (n_head * mb_size) size batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, *repeats)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mrepeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mRepeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\_functions\\tensor.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, input, repeats)\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, *sizes)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0murtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0murtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! without any dropout precision is quite high, also 60% fscore after 28 epochs!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using class weights, drouput 0.2\n",
    "\"\"\"\n",
    "weights = [\n",
    "    1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
    "    0.8, 0.8\n",
    "]\n",
    "Final Score:\n",
    "Calculating Precision: correct_by_relation= 2762.0 / guessed_by_relation= 4970.0\n",
    "Calculating Recall: correct_by_relation= 2762.0 / gold_by_relation= 4143.0\n",
    "only_non_relation: 4143.0\n",
    "Precision (micro): 55.573%\n",
    "   Recall (micro): 66.667%\n",
    "       F1 (micro): 60.617%\n",
    "Prediction scores saved to saved_models/out/test_4.pkl.\n",
    "Evaluation ended.\n",
    "\n",
    "\n",
    "        weights = [\n",
    "            3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,\n",
    "            1.0, 1.0\n",
    "        ]\n",
    "        dropout=0.3\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
