{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0b0+591e73e\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')               # 200\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of RNN layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='Input and RNN dropout rate.')           # 0.5\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=360, help='Attention size.')                      # 200\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1.0, help='Applies to SGD and Adagrad.')\n",
    "parser.add_argument('--lr_decay', type=float, default=0.8)\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')  # sgd\n",
    "parser.add_argument('--num_epoch', type=int, default=100)                                       # 30\n",
    "parser.add_argument('--batch_size', type=int, default=50)\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=20, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='07_self_attention',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/06_self_attention/config.json\n",
      "Overwriting old vocab file at ./saved_models/06_self_attention/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 360\n",
      "\tpe_dim : 30\n",
      "\tlr : 1.0\n",
      "\tlr_decay : 0.8\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 100\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 20\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 06_self_attention\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/06_self_attention\n",
      "\n",
      "\n",
      "using self-attention\n",
      "Finetune all embeddings.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:325: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-26 19:59:41.979854: step 400/150100 (epoch 1/100), loss = 0.817120 (0.036 sec/batch), lr: 1.000000\n",
      "2018-02-26 19:59:58.152350: step 800/150100 (epoch 1/100), loss = 0.949206 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:00:14.204526: step 1200/150100 (epoch 1/100), loss = 0.701192 (0.031 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 33.595%\n",
      "   Recall (micro): 41.694%\n",
      "       F1 (micro): 37.209%\n",
      "epoch 1: train_loss = 1.261704, dev_loss = 0.945303, dev_f1 = 0.3721\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:00:38.429926: step 1600/150100 (epoch 2/100), loss = 0.618000 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:00:54.816989: step 2000/150100 (epoch 2/100), loss = 0.763590 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:01:11.038110: step 2400/150100 (epoch 2/100), loss = 0.638795 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:01:27.217622: step 2800/150100 (epoch 2/100), loss = 0.652719 (0.045 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 40.322%\n",
      "   Recall (micro): 58.244%\n",
      "       F1 (micro): 47.653%\n",
      "epoch 2: train_loss = 0.585311, dev_loss = 0.774195, dev_f1 = 0.4765\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:01:52.081720: step 3200/150100 (epoch 3/100), loss = 0.474039 (0.033 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:02:08.000037: step 3600/150100 (epoch 3/100), loss = 0.571178 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:02:23.956956: step 4000/150100 (epoch 3/100), loss = 0.322871 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:02:39.924908: step 4400/150100 (epoch 3/100), loss = 0.511353 (0.039 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 46.099%\n",
      "   Recall (micro): 60.102%\n",
      "       F1 (micro): 52.177%\n",
      "epoch 3: train_loss = 0.506669, dev_loss = 0.662492, dev_f1 = 0.5218\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:03:04.077619: step 4800/150100 (epoch 4/100), loss = 0.352077 (0.034 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:03:20.062613: step 5200/150100 (epoch 4/100), loss = 0.606593 (0.036 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:03:35.983440: step 5600/150100 (epoch 4/100), loss = 0.820318 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:03:51.951890: step 6000/150100 (epoch 4/100), loss = 0.340362 (0.043 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 45.550%\n",
      "   Recall (micro): 64.902%\n",
      "       F1 (micro): 53.531%\n",
      "epoch 4: train_loss = 0.465759, dev_loss = 0.643699, dev_f1 = 0.5353\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:04:16.684137: step 6400/150100 (epoch 5/100), loss = 0.498098 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:04:32.706234: step 6800/150100 (epoch 5/100), loss = 0.310691 (0.039 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:04:48.589959: step 7200/150100 (epoch 5/100), loss = 0.412200 (0.037 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 49.432%\n",
      "   Recall (micro): 66.667%\n",
      "       F1 (micro): 56.770%\n",
      "epoch 5: train_loss = 0.441908, dev_loss = 0.615676, dev_f1 = 0.5677\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:05:12.394741: step 7600/150100 (epoch 6/100), loss = 0.423553 (0.044 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:05:28.537665: step 8000/150100 (epoch 6/100), loss = 0.293401 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:05:44.693614: step 8400/150100 (epoch 6/100), loss = 0.461718 (0.038 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:06:01.129808: step 8800/150100 (epoch 6/100), loss = 0.432426 (0.034 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 50.520%\n",
      "   Recall (micro): 64.716%\n",
      "       F1 (micro): 56.743%\n",
      "epoch 6: train_loss = 0.419117, dev_loss = 0.595967, dev_f1 = 0.5674\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_6.pt\n",
      "\n",
      "2018-02-26 20:06:25.138632: step 9200/150100 (epoch 7/100), loss = 0.564385 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:06:41.108085: step 9600/150100 (epoch 7/100), loss = 0.198461 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:06:57.399895: step 10000/150100 (epoch 7/100), loss = 0.222997 (0.037 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:07:13.503705: step 10400/150100 (epoch 7/100), loss = 0.175546 (0.038 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 51.952%\n",
      "   Recall (micro): 67.363%\n",
      "       F1 (micro): 58.663%\n",
      "epoch 7: train_loss = 0.398915, dev_loss = 0.589435, dev_f1 = 0.5866\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:07:37.353615: step 10800/150100 (epoch 8/100), loss = 0.341658 (0.040 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:07:53.659462: step 11200/150100 (epoch 8/100), loss = 0.504306 (0.043 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:08:09.787335: step 11600/150100 (epoch 8/100), loss = 0.455931 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:08:26.083657: step 12000/150100 (epoch 8/100), loss = 0.268382 (0.036 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 52.250%\n",
      "   Recall (micro): 67.596%\n",
      "       F1 (micro): 58.940%\n",
      "epoch 8: train_loss = 0.387729, dev_loss = 0.595363, dev_f1 = 0.5894\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:08:50.044855: step 12400/150100 (epoch 9/100), loss = 0.346479 (0.042 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:09:06.384793: step 12800/150100 (epoch 9/100), loss = 0.367156 (0.041 sec/batch), lr: 1.000000\n",
      "2018-02-26 20:09:22.378310: step 13200/150100 (epoch 9/100), loss = 0.455125 (0.042 sec/batch), lr: 1.000000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 48.247%\n",
      "   Recall (micro): 67.983%\n",
      "       F1 (micro): 56.440%\n",
      "epoch 9: train_loss = 0.377067, dev_loss = 0.628374, dev_f1 = 0.5644\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_9.pt\n",
      "\n",
      "2018-02-26 20:09:46.199134: step 13600/150100 (epoch 10/100), loss = 0.393659 (0.039 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:10:02.318486: step 14000/150100 (epoch 10/100), loss = 0.404870 (0.045 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:10:18.525069: step 14400/150100 (epoch 10/100), loss = 0.226663 (0.042 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:10:35.124196: step 14800/150100 (epoch 10/100), loss = 0.430763 (0.031 sec/batch), lr: 0.800000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 57.953%\n",
      "   Recall (micro): 65.939%\n",
      "       F1 (micro): 61.689%\n",
      "epoch 10: train_loss = 0.347640, dev_loss = 0.514074, dev_f1 = 0.6169\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:10:58.966076: step 15200/150100 (epoch 11/100), loss = 0.381651 (0.031 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:11:15.153107: step 15600/150100 (epoch 11/100), loss = 0.306983 (0.044 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:11:31.505077: step 16000/150100 (epoch 11/100), loss = 0.337231 (0.040 sec/batch), lr: 0.800000\n",
      "2018-02-26 20:11:47.720183: step 16400/150100 (epoch 11/100), loss = 0.317640 (0.042 sec/batch), lr: 0.800000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 55.143%\n",
      "   Recall (micro): 65.985%\n",
      "       F1 (micro): 60.079%\n",
      "epoch 11: train_loss = 0.340690, dev_loss = 0.542252, dev_f1 = 0.6008\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_11.pt\n",
      "\n",
      "2018-02-26 20:12:11.984687: step 16800/150100 (epoch 12/100), loss = 0.442044 (0.039 sec/batch), lr: 0.640000\n",
      "2018-02-26 20:12:28.186758: step 17200/150100 (epoch 12/100), loss = 0.285006 (0.036 sec/batch), lr: 0.640000\n",
      "2018-02-26 20:12:44.545245: step 17600/150100 (epoch 12/100), loss = 0.270117 (0.039 sec/batch), lr: 0.640000\n",
      "2018-02-26 20:13:01.113791: step 18000/150100 (epoch 12/100), loss = 0.365966 (0.042 sec/batch), lr: 0.640000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 59.125%\n",
      "   Recall (micro): 66.512%\n",
      "       F1 (micro): 62.601%\n",
      "epoch 12: train_loss = 0.318274, dev_loss = 0.514391, dev_f1 = 0.6260\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-02-26 20:13:25.447491: step 18400/150100 (epoch 13/100), loss = 0.474619 (0.045 sec/batch), lr: 0.640000\n",
      "2018-02-26 20:13:41.596928: step 18800/150100 (epoch 13/100), loss = 0.240627 (0.036 sec/batch), lr: 0.640000\n",
      "2018-02-26 20:13:57.936368: step 19200/150100 (epoch 13/100), loss = 0.210111 (0.041 sec/batch), lr: 0.640000\n",
      "Evaluating on dev set...\n",
      "Precision (micro): 55.745%\n",
      "   Recall (micro): 66.775%\n",
      "       F1 (micro): 60.764%\n",
      "epoch 13: train_loss = 0.307581, dev_loss = 0.544303, dev_f1 = 0.6076\n",
      "model saved to ./saved_models/06_self_attention/checkpoint_epoch_13.pt\n",
      "\n",
      "2018-02-26 20:14:25.964878: step 19600/150100 (epoch 14/100), loss = 0.171952 (0.030 sec/batch), lr: 0.512000\n",
      "2018-02-26 20:14:42.280751: step 20000/150100 (epoch 14/100), loss = 0.254720 (0.045 sec/batch), lr: 0.512000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a479e9ba23c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m400\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# if global_step % opt['log_step'] == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % 400 == 0:   # if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1))\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 5 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
