{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of self-attention layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0.2, help='Input and RNN dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.')\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.2, help='Applies to SGD and Adagrad.')            # lr\n",
    "parser.add_argument('--lr_decay', type=float, default=0.8)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=100)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=5, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='17_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/17_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/17_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tdropout : 0.2\n",
      "\tword_dropout : 0.04\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tself_att : True\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.2\n",
      "\tlr_decay : 0.8\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 100\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 5\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 17_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/17_self_attention_dropout\n",
      "\n",
      "\n",
      "Finetune all embeddings.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-07 16:44:43.463328: step 400/150100 (epoch 1/100), loss = 0.526289 (0.061 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:45:11.090822: step 800/150100 (epoch 1/100), loss = 0.501513 (0.071 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:45:38.354348: step 1200/150100 (epoch 1/100), loss = 0.615034 (0.053 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4384.0 / guessed_by_relation= 13449.0\n",
      "Calculating Recall: correct_by_relation= 4384.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 32.597%\n",
      "   Recall (micro): 67.874%\n",
      "       F1 (micro): 44.043%\n",
      "epoch 1: train_loss = 0.896569, dev_loss = 1.008153, dev_f1 = 0.4404\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 16:46:18.699166: step 1600/150100 (epoch 2/100), loss = 0.489273 (0.074 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:46:46.368772: step 2000/150100 (epoch 2/100), loss = 0.731279 (0.074 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:47:14.016593: step 2400/150100 (epoch 2/100), loss = 0.514073 (0.072 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:47:41.498755: step 2800/150100 (epoch 2/100), loss = 0.724395 (0.070 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4491.0 / guessed_by_relation= 12302.0\n",
      "Calculating Recall: correct_by_relation= 4491.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 36.506%\n",
      "   Recall (micro): 69.531%\n",
      "       F1 (micro): 47.876%\n",
      "epoch 2: train_loss = 0.533465, dev_loss = 0.875779, dev_f1 = 0.4788\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 16:48:22.096220: step 3200/150100 (epoch 3/100), loss = 0.454652 (0.057 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:48:49.580566: step 3600/150100 (epoch 3/100), loss = 0.370391 (0.067 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:49:17.125285: step 4000/150100 (epoch 3/100), loss = 0.376314 (0.074 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:49:44.645834: step 4400/150100 (epoch 3/100), loss = 0.483444 (0.068 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4092.0 / guessed_by_relation= 9040.0\n",
      "Calculating Recall: correct_by_relation= 4092.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 45.265%\n",
      "   Recall (micro): 63.353%\n",
      "       F1 (micro): 52.803%\n",
      "epoch 3: train_loss = 0.491901, dev_loss = 0.640639, dev_f1 = 0.5280\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 16:50:25.093515: step 4800/150100 (epoch 4/100), loss = 0.365046 (0.058 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:50:52.480874: step 5200/150100 (epoch 4/100), loss = 0.545839 (0.064 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:51:20.004631: step 5600/150100 (epoch 4/100), loss = 0.820056 (0.076 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:51:47.595343: step 6000/150100 (epoch 4/100), loss = 0.265253 (0.076 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4630.0 / guessed_by_relation= 10885.0\n",
      "Calculating Recall: correct_by_relation= 4630.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 42.536%\n",
      "   Recall (micro): 71.683%\n",
      "       F1 (micro): 53.390%\n",
      "epoch 4: train_loss = 0.459099, dev_loss = 0.692672, dev_f1 = 0.5339\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 16:52:28.049030: step 6400/150100 (epoch 5/100), loss = 0.424266 (0.075 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:52:55.549275: step 6800/150100 (epoch 5/100), loss = 0.299784 (0.068 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:53:22.911279: step 7200/150100 (epoch 5/100), loss = 0.478046 (0.063 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4757.0 / guessed_by_relation= 11504.0\n",
      "Calculating Recall: correct_by_relation= 4757.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 41.351%\n",
      "   Recall (micro): 73.649%\n",
      "       F1 (micro): 52.964%\n",
      "epoch 5: train_loss = 0.437359, dev_loss = 0.717401, dev_f1 = 0.5296\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "\n",
      "2018-03-07 16:54:03.638985: step 7600/150100 (epoch 6/100), loss = 0.455936 (0.073 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:54:31.104047: step 8000/150100 (epoch 6/100), loss = 0.323861 (0.070 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:54:58.526997: step 8400/150100 (epoch 6/100), loss = 0.427300 (0.066 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:55:25.817782: step 8800/150100 (epoch 6/100), loss = 0.477191 (0.059 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4488.0 / guessed_by_relation= 9352.0\n",
      "Calculating Recall: correct_by_relation= 4488.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.990%\n",
      "   Recall (micro): 69.484%\n",
      "       F1 (micro): 56.771%\n",
      "epoch 6: train_loss = 0.421843, dev_loss = 0.602816, dev_f1 = 0.5677\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 16:56:06.737303: step 9200/150100 (epoch 7/100), loss = 0.499178 (0.075 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:56:34.092071: step 9600/150100 (epoch 7/100), loss = 0.253635 (0.070 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:57:01.523875: step 10000/150100 (epoch 7/100), loss = 0.279875 (0.064 sec/batch), lr: 0.200000\n",
      "2018-03-07 16:57:28.972895: step 10400/150100 (epoch 7/100), loss = 0.175831 (0.066 sec/batch), lr: 0.200000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4815.0 / guessed_by_relation= 10878.0\n",
      "Calculating Recall: correct_by_relation= 4815.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 44.264%\n",
      "   Recall (micro): 74.547%\n",
      "       F1 (micro): 55.546%\n",
      "epoch 7: train_loss = 0.407053, dev_loss = 0.648942, dev_f1 = 0.5555\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "\n",
      "2018-03-07 16:58:09.662135: step 10800/150100 (epoch 8/100), loss = 0.352804 (0.070 sec/batch), lr: 0.160000\n",
      "2018-03-07 16:58:37.065032: step 11200/150100 (epoch 8/100), loss = 0.481207 (0.073 sec/batch), lr: 0.160000\n",
      "2018-03-07 16:59:04.430829: step 11600/150100 (epoch 8/100), loss = 0.403119 (0.071 sec/batch), lr: 0.160000\n",
      "2018-03-07 16:59:31.886933: step 12000/150100 (epoch 8/100), loss = 0.219073 (0.061 sec/batch), lr: 0.160000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4551.0 / guessed_by_relation= 8623.0\n",
      "Calculating Recall: correct_by_relation= 4551.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.777%\n",
      "   Recall (micro): 70.460%\n",
      "       F1 (micro): 60.350%\n",
      "epoch 8: train_loss = 0.383784, dev_loss = 0.544661, dev_f1 = 0.6035\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:00:12.752643: step 12400/150100 (epoch 9/100), loss = 0.369524 (0.070 sec/batch), lr: 0.160000\n",
      "2018-03-07 17:00:40.183614: step 12800/150100 (epoch 9/100), loss = 0.329670 (0.072 sec/batch), lr: 0.160000\n",
      "2018-03-07 17:01:07.521652: step 13200/150100 (epoch 9/100), loss = 0.392400 (0.072 sec/batch), lr: 0.160000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4587.0 / guessed_by_relation= 8850.0\n",
      "Calculating Recall: correct_by_relation= 4587.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.831%\n",
      "   Recall (micro): 71.017%\n",
      "       F1 (micro): 59.926%\n",
      "epoch 9: train_loss = 0.375565, dev_loss = 0.558305, dev_f1 = 0.5993\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "\n",
      "2018-03-07 17:01:48.270050: step 13600/150100 (epoch 10/100), loss = 0.474129 (0.068 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:02:16.061288: step 14000/150100 (epoch 10/100), loss = 0.452522 (0.075 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:02:43.566370: step 14400/150100 (epoch 10/100), loss = 0.253070 (0.071 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:03:10.942195: step 14800/150100 (epoch 10/100), loss = 0.457747 (0.052 sec/batch), lr: 0.128000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4515.0 / guessed_by_relation= 8247.0\n",
      "Calculating Recall: correct_by_relation= 4515.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.747%\n",
      "   Recall (micro): 69.902%\n",
      "       F1 (micro): 61.404%\n",
      "epoch 10: train_loss = 0.357376, dev_loss = 0.527668, dev_f1 = 0.6140\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:03:51.893227: step 15200/150100 (epoch 11/100), loss = 0.469885 (0.049 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:04:19.261704: step 15600/150100 (epoch 11/100), loss = 0.380157 (0.075 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:04:46.850094: step 16000/150100 (epoch 11/100), loss = 0.411206 (0.073 sec/batch), lr: 0.128000\n",
      "2018-03-07 17:05:14.982078: step 16400/150100 (epoch 11/100), loss = 0.169861 (0.071 sec/batch), lr: 0.128000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4594.0 / guessed_by_relation= 8532.0\n",
      "Calculating Recall: correct_by_relation= 4594.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.844%\n",
      "   Recall (micro): 71.126%\n",
      "       F1 (micro): 61.290%\n",
      "epoch 11: train_loss = 0.351742, dev_loss = 0.538565, dev_f1 = 0.6129\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "2018-03-07 17:05:55.973122: step 16800/150100 (epoch 12/100), loss = 0.526955 (0.064 sec/batch), lr: 0.102400\n",
      "2018-03-07 17:06:23.438184: step 17200/150100 (epoch 12/100), loss = 0.262650 (0.060 sec/batch), lr: 0.102400\n",
      "2018-03-07 17:06:51.582052: step 17600/150100 (epoch 12/100), loss = 0.299014 (0.063 sec/batch), lr: 0.102400\n",
      "2018-03-07 17:07:20.131813: step 18000/150100 (epoch 12/100), loss = 0.290894 (0.074 sec/batch), lr: 0.102400\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4510.0 / guessed_by_relation= 8081.0\n",
      "Calculating Recall: correct_by_relation= 4510.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.810%\n",
      "   Recall (micro): 69.825%\n",
      "       F1 (micro): 62.036%\n",
      "epoch 12: train_loss = 0.337755, dev_loss = 0.520748, dev_f1 = 0.6204\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:08:02.038603: step 18400/150100 (epoch 13/100), loss = 0.462950 (0.074 sec/batch), lr: 0.102400\n",
      "2018-03-07 17:08:29.779399: step 18800/150100 (epoch 13/100), loss = 0.166966 (0.060 sec/batch), lr: 0.102400\n",
      "2018-03-07 17:08:57.643522: step 19200/150100 (epoch 13/100), loss = 0.181061 (0.071 sec/batch), lr: 0.102400\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4535.0 / guessed_by_relation= 8196.0\n",
      "Calculating Recall: correct_by_relation= 4535.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.332%\n",
      "   Recall (micro): 70.212%\n",
      "       F1 (micro): 61.890%\n",
      "epoch 13: train_loss = 0.331983, dev_loss = 0.527087, dev_f1 = 0.6189\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "\n",
      "2018-03-07 17:09:39.228377: step 19600/150100 (epoch 14/100), loss = 0.237481 (0.054 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:10:07.674036: step 20000/150100 (epoch 14/100), loss = 0.332566 (0.076 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:10:35.108015: step 20400/150100 (epoch 14/100), loss = 0.324798 (0.074 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:11:02.721513: step 20800/150100 (epoch 14/100), loss = 0.226898 (0.065 sec/batch), lr: 0.081920\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4297.0 / guessed_by_relation= 7123.0\n",
      "Calculating Recall: correct_by_relation= 4297.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.326%\n",
      "   Recall (micro): 66.527%\n",
      "       F1 (micro): 63.275%\n",
      "epoch 14: train_loss = 0.317168, dev_loss = 0.501879, dev_f1 = 0.6327\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:11:44.301840: step 21200/150100 (epoch 15/100), loss = 0.460325 (0.067 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:12:12.006539: step 21600/150100 (epoch 15/100), loss = 0.218804 (0.060 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:12:39.395798: step 22000/150100 (epoch 15/100), loss = 0.335401 (0.053 sec/batch), lr: 0.081920\n",
      "2018-03-07 17:13:07.156662: step 22400/150100 (epoch 15/100), loss = 0.393156 (0.073 sec/batch), lr: 0.081920\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4422.0 / guessed_by_relation= 7674.0\n",
      "Calculating Recall: correct_by_relation= 4422.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.623%\n",
      "   Recall (micro): 68.463%\n",
      "       F1 (micro): 62.577%\n",
      "epoch 15: train_loss = 0.314545, dev_loss = 0.512909, dev_f1 = 0.6258\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "2018-03-07 17:13:48.139874: step 22800/150100 (epoch 16/100), loss = 0.292898 (0.063 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:14:15.879288: step 23200/150100 (epoch 16/100), loss = 0.460255 (0.068 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:14:43.257117: step 23600/150100 (epoch 16/100), loss = 0.191866 (0.056 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:15:10.798382: step 24000/150100 (epoch 16/100), loss = 0.167336 (0.075 sec/batch), lr: 0.065536\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4306.0 / guessed_by_relation= 7195.0\n",
      "Calculating Recall: correct_by_relation= 4306.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.847%\n",
      "   Recall (micro): 66.667%\n",
      "       F1 (micro): 63.073%\n",
      "epoch 16: train_loss = 0.303290, dev_loss = 0.501362, dev_f1 = 0.6307\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "\n",
      "2018-03-07 17:15:51.835548: step 24400/150100 (epoch 17/100), loss = 0.210780 (0.067 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:16:19.822978: step 24800/150100 (epoch 17/100), loss = 0.350794 (0.076 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:16:48.663471: step 25200/150100 (epoch 17/100), loss = 0.261989 (0.061 sec/batch), lr: 0.065536\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4321.0 / guessed_by_relation= 7178.0\n",
      "Calculating Recall: correct_by_relation= 4321.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 60.198%\n",
      "   Recall (micro): 66.899%\n",
      "       F1 (micro): 63.372%\n",
      "epoch 17: train_loss = 0.298642, dev_loss = 0.503629, dev_f1 = 0.6337\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:17:30.707291: step 25600/150100 (epoch 18/100), loss = 0.223442 (0.074 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:17:58.795653: step 26000/150100 (epoch 18/100), loss = 0.230711 (0.062 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:18:26.582571: step 26400/150100 (epoch 18/100), loss = 0.291630 (0.063 sec/batch), lr: 0.065536\n",
      "2018-03-07 17:18:54.779934: step 26800/150100 (epoch 18/100), loss = 0.413033 (0.057 sec/batch), lr: 0.065536\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4208.0 / guessed_by_relation= 6869.0\n",
      "Calculating Recall: correct_by_relation= 4208.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.261%\n",
      "   Recall (micro): 65.149%\n",
      "       F1 (micro): 63.145%\n",
      "epoch 18: train_loss = 0.292554, dev_loss = 0.506171, dev_f1 = 0.6315\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "\n",
      "2018-03-07 17:19:35.776979: step 27200/150100 (epoch 19/100), loss = 0.393678 (0.058 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:20:03.233676: step 27600/150100 (epoch 19/100), loss = 0.145252 (0.054 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:20:31.028210: step 28000/150100 (epoch 19/100), loss = 0.093572 (0.064 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:20:58.858536: step 28400/150100 (epoch 19/100), loss = 0.385575 (0.062 sec/batch), lr: 0.052429\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4161.0 / guessed_by_relation= 6662.0\n",
      "Calculating Recall: correct_by_relation= 4161.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.459%\n",
      "   Recall (micro): 64.422%\n",
      "       F1 (micro): 63.425%\n",
      "epoch 19: train_loss = 0.282660, dev_loss = 0.503862, dev_f1 = 0.6343\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:21:40.204984: step 28800/150100 (epoch 20/100), loss = 0.365456 (0.054 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:22:20.578017: step 29200/150100 (epoch 20/100), loss = 0.214725 (0.144 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:23:31.600223: step 29600/150100 (epoch 20/100), loss = 0.188159 (0.100 sec/batch), lr: 0.052429\n",
      "2018-03-07 17:24:25.388301: step 30000/150100 (epoch 20/100), loss = 0.185258 (0.129 sec/batch), lr: 0.052429\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4207.0 / guessed_by_relation= 6893.0\n",
      "Calculating Recall: correct_by_relation= 4207.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.033%\n",
      "   Recall (micro): 65.134%\n",
      "       F1 (micro): 63.017%\n",
      "epoch 20: train_loss = 0.277618, dev_loss = 0.516013, dev_f1 = 0.6302\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "2018-03-07 17:25:42.966930: step 30400/150100 (epoch 21/100), loss = 0.151653 (0.138 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:26:36.906482: step 30800/150100 (epoch 21/100), loss = 0.215202 (0.139 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:27:30.846817: step 31200/150100 (epoch 21/100), loss = 0.189206 (0.142 sec/batch), lr: 0.041943\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4132.0 / guessed_by_relation= 6626.0\n",
      "Calculating Recall: correct_by_relation= 4132.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.360%\n",
      "   Recall (micro): 63.973%\n",
      "       F1 (micro): 63.156%\n",
      "epoch 21: train_loss = 0.268409, dev_loss = 0.517223, dev_f1 = 0.6316\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "2018-03-07 17:28:48.630941: step 31600/150100 (epoch 22/100), loss = 0.315136 (0.117 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:29:41.384014: step 32000/150100 (epoch 22/100), loss = 0.200856 (0.140 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:30:35.520228: step 32400/150100 (epoch 22/100), loss = 0.315326 (0.133 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:31:29.398982: step 32800/150100 (epoch 22/100), loss = 0.150323 (0.136 sec/batch), lr: 0.041943\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4240.0 / guessed_by_relation= 6892.0\n",
      "Calculating Recall: correct_by_relation= 4240.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.521%\n",
      "   Recall (micro): 65.645%\n",
      "       F1 (micro): 63.516%\n",
      "epoch 22: train_loss = 0.263594, dev_loss = 0.525106, dev_f1 = 0.6352\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "new best model saved.\n",
      "\n",
      "2018-03-07 17:32:47.132367: step 33200/150100 (epoch 23/100), loss = 0.254936 (0.135 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:33:39.441228: step 33600/150100 (epoch 23/100), loss = 0.214412 (0.139 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:34:33.314692: step 34000/150100 (epoch 23/100), loss = 0.349631 (0.136 sec/batch), lr: 0.041943\n",
      "2018-03-07 17:35:27.305462: step 34400/150100 (epoch 23/100), loss = 0.122871 (0.140 sec/batch), lr: 0.041943\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4084.0 / guessed_by_relation= 6521.0\n",
      "Calculating Recall: correct_by_relation= 4084.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.628%\n",
      "   Recall (micro): 63.230%\n",
      "       F1 (micro): 62.928%\n",
      "epoch 23: train_loss = 0.262413, dev_loss = 0.522875, dev_f1 = 0.6293\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "\n",
      "2018-03-07 17:37:05.720707: step 34800/150100 (epoch 24/100), loss = 0.566647 (0.136 sec/batch), lr: 0.033554\n",
      "2018-03-07 17:38:29.615235: step 35200/150100 (epoch 24/100), loss = 0.282850 (0.138 sec/batch), lr: 0.033554\n",
      "2018-03-07 17:39:23.204792: step 35600/150100 (epoch 24/100), loss = 0.143205 (0.133 sec/batch), lr: 0.033554\n",
      "2018-03-07 17:40:16.948354: step 36000/150100 (epoch 24/100), loss = 0.321209 (0.136 sec/batch), lr: 0.033554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4178.0 / guessed_by_relation= 6810.0\n",
      "Calculating Recall: correct_by_relation= 4178.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.351%\n",
      "   Recall (micro): 64.685%\n",
      "       F1 (micro): 62.974%\n",
      "epoch 24: train_loss = 0.254027, dev_loss = 0.529757, dev_f1 = 0.6297\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "\n",
      "2018-03-07 17:41:32.684998: step 36400/150100 (epoch 25/100), loss = 0.232910 (0.134 sec/batch), lr: 0.033554\n",
      "2018-03-07 17:42:26.514193: step 36800/150100 (epoch 25/100), loss = 0.186245 (0.144 sec/batch), lr: 0.033554\n",
      "2018-03-07 17:43:20.185069: step 37200/150100 (epoch 25/100), loss = 0.161346 (0.143 sec/batch), lr: 0.033554\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4001.0 / guessed_by_relation= 6396.0\n",
      "Calculating Recall: correct_by_relation= 4001.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 62.555%\n",
      "   Recall (micro): 61.945%\n",
      "       F1 (micro): 62.248%\n",
      "epoch 25: train_loss = 0.248537, dev_loss = 0.529217, dev_f1 = 0.6225\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "2018-03-07 17:44:37.552880: step 37600/150100 (epoch 26/100), loss = 0.186904 (0.123 sec/batch), lr: 0.026844\n",
      "2018-03-07 17:45:30.344471: step 38000/150100 (epoch 26/100), loss = 0.161854 (0.116 sec/batch), lr: 0.026844\n",
      "2018-03-07 17:46:25.181582: step 38400/150100 (epoch 26/100), loss = 0.174685 (0.132 sec/batch), lr: 0.026844\n",
      "2018-03-07 17:47:20.016910: step 38800/150100 (epoch 26/100), loss = 0.282945 (0.137 sec/batch), lr: 0.026844\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3990.0 / guessed_by_relation= 6236.0\n",
      "Calculating Recall: correct_by_relation= 3990.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.983%\n",
      "   Recall (micro): 61.774%\n",
      "       F1 (micro): 62.859%\n",
      "epoch 26: train_loss = 0.240938, dev_loss = 0.535899, dev_f1 = 0.6286\n",
      "model saved to ./saved_models/17_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ff75530c699e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\PyCharmProjects\\tac-self-attention\\model\\rnn.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "    \n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 5 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        current_lr *= opt['lr_decay']\n",
    "        model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is 2 layer encoder + 0.2 lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
