{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([[[  0.,   1.,   2.],\n",
      "         [  3.,   4.,   5.],\n",
      "         [  6.,   7.,   8.],\n",
      "         [  9.,  10.,  11.]],\n",
      "\n",
      "        [[ 12.,  13.,  14.],\n",
      "         [ 15.,  16.,  17.],\n",
      "         [ 18.,  19.,  20.],\n",
      "         [ 21.,  22.,  23.]]])\n",
      "inverse stripe:\n",
      "\n",
      "tensor([[[  2.,   4.,   6.],\n",
      "         [  5.,   7.,   9.]],\n",
      "\n",
      "        [[ 14.,  16.,  18.],\n",
      "         [ 17.,  19.,  21.]]])\n",
      "\n",
      "tensor([[[  6.,   4.,   2.],\n",
      "         [  9.,   7.,   5.]],\n",
      "\n",
      "        [[ 18.,  16.,  14.],\n",
      "         [ 21.,  19.,  17.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def flip(x, dim):\n",
    "    indices = [slice(None)] * x.dim()\n",
    "    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\n",
    "                                dtype=torch.long, device=x.device)\n",
    "    return x[tuple(indices)]\n",
    "\n",
    "\n",
    "def batch_stripe(a):\n",
    "    b, i, j = a.size()\n",
    "    assert i >= j\n",
    "    b_s, k, l = a.stride()\n",
    "    return torch.as_strided(a, (b, i-j+1, j), (b_s, k, k+l))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = torch.arange(24).view(2,4,3)\n",
    "    print('original tensor:')\n",
    "    print(a)\n",
    "    print('inverse stripe:')\n",
    "    # do we flip twice?\n",
    "    print()\n",
    "    print(batch_stripe(flip(a, -1)))\n",
    "    print()\n",
    "    print(flip(batch_stripe(flip(a, -1)), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab\n",
    "\n",
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "9.1\n",
      "7005\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([[[  0.,   1.,   2.],\n",
      "         [  3.,   4.,   5.],\n",
      "         [  6.,   7.,   8.],\n",
      "         [  9.,  10.,  11.]],\n",
      "\n",
      "        [[ 12.,  13.,  14.],\n",
      "         [ 15.,  16.,  17.],\n",
      "         [ 18.,  19.,  20.],\n",
      "         [ 21.,  22.,  23.]]])\n",
      "inverse stripe:\n",
      "tensor([[[  6.,   4.,   2.],\n",
      "         [  9.,   7.,   5.]],\n",
      "\n",
      "        [[ 18.,  16.,  14.],\n",
      "         [ 21.,  19.,  17.]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')              # 200 original\n",
    "parser.add_argument('--hidden_self', type=int, default=150,\n",
    "                    help='Hidden size for self-attention.')  # n_model*2 in the paper  # used to be 720\n",
    "parser.add_argument('--query_size_attn', type=int, default=360,\n",
    "                    help='Embedding for query size in the positional attention.')  # n_model*2 in the paper\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of lstm layers.')\n",
    "\n",
    "# encoder layers\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=1, help='Num of self-attention encoders.')  # 2\n",
    "parser.add_argument('--dropout', type=float, default=0.4, help='Input and attn dropout rate.')            # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')   # 0.1 original\n",
    "parser.add_argument('--temper_value', type=float, default=0.5, help='Temper value for Scaled Attention.') # 0.5 original\n",
    "\n",
    "parser.add_argument('--word_dropout', type=float, default=0.06,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=1.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=1.0, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', help='Use self-attention layer instead of LSTM.', default=True)\n",
    "parser.add_argument('--no_self_att', dest='self_att', action='store_false',\n",
    "    help='Use self-attention layer instead of LSTM.')\n",
    "parser.set_defaults(self_att=True)\n",
    "\n",
    "\n",
    "# use self-attention + hidden LSTM layer\n",
    "parser.add_argument('--self_att_and_rnn', dest='self_att_and_rnn', action='store_true', default=\"true\",\n",
    "    help='Use self-attention for outputs and LSTM for the last hidden layer.')\n",
    "parser.add_argument('--no_self_att_and_rnn', dest='self_att_and_rnn', action='store_false',\n",
    "    help='Use self-attention for outputs and LSTM for the last hidden layer.')\n",
    "parser.set_defaults(self_att_and_rnn=False)\n",
    "\n",
    "\n",
    "# use lemmas instead of raw text\n",
    "parser.add_argument('--use_lemmas', dest='use_lemmas', action='store_true', default=\"true\",\n",
    "    help='Instead of raw text, use spacy lemmas.')\n",
    "parser.add_argument('--no_lemmas', dest='use_lemmas', action='store_false',\n",
    "    help='Instead of raw text, use spacy lemmas.')\n",
    "parser.set_defaults(use_lemmas=False)\n",
    "\n",
    "# preload lemmatized text pickles\n",
    "parser.add_argument('--preload_lemmas', dest='preload_lemmas', action='store_true', default=\"true\",\n",
    "    help='Instead of raw text, use spacy lemmas.')\n",
    "parser.add_argument('--no_preload_lemmas', dest='preload_lemmas', action='store_false', default=\"true\",\n",
    "    help='Instead of raw text, use spacy lemmas.')\n",
    "parser.set_defaults(preload_lemmas=False)\n",
    "\n",
    "# use object and subject positional encodings\n",
    "parser.add_argument('--obj_sub_pos', dest='obj_sub_pos', action='store_true',\n",
    "    help='In self-attention add obj/subg positional vectors.', default=True)\n",
    "\n",
    "# batch norm\n",
    "parser.add_argument('--use_batch_norm', dest='use_batch_norm', action='store_true', \n",
    "    help='BatchNorm if true, else LayerNorm in self-attention.', default=True)\n",
    "parser.add_argument('--use_layer_norm', dest='use_batch_norm', action='store_true',\n",
    "    help='BatchNorm if true, else LayerNorm in self-attention.', default=False)\n",
    "parser.set_defaults(use_batch_norm=True)\n",
    "\n",
    "# dpa\n",
    "parser.add_argument('--diagonal_positional_attention', dest='diagonal_positional_attention', action='store_true',\n",
    "    help='Use diagonal attention.', default=True)\n",
    "parser.add_argument('--no_diagonal_positional_attention', dest='diagonal_positional_attention', action='store_false')\n",
    "parser.set_defaults(diagonal_positional_attention=True)\n",
    "\n",
    "# relative positional vectors\n",
    "parser.add_argument('--relative_positions', dest='relative_positions', action='store_true', \n",
    "    help='Use relative positions for subj/obj positional vectors.', default=True)\n",
    "parser.add_argument('--no_relative_positions', dest='relative_positions', action='store_false')\n",
    "parser.set_defaults(relative_positions=True)\n",
    "\n",
    "# how to use residual connections\n",
    "parser.add_argument('--new_residual', dest='new_residual', action='store_true', \n",
    "    help='Use a different residual connection than in usual self-attention.', default=True)\n",
    "parser.add_argument('--old_residual', dest='new_residual', action='store_false')\n",
    "parser.set_defaults(new_residual=True)\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=3, help='Number of self-attention heads.')\n",
    "\n",
    "# use positional attention from stanford paper\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.1, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.9)                  # lr_decay 0.9 original\n",
    "parser.add_argument('--decay_epoch', type=int, default=15, help='Start LR decay from this epoch.')\n",
    "\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, asgd, adagrad, adam, nadam or adamax.')    # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=100)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=5.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=1, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='70_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am here\n",
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75050/75050 [00:08<00:00, 9366.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 batches created for dataset/tacred/train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25764/25764 [00:02<00:00, 9226.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/70_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/70_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\thidden_self : 150\n",
      "\tquery_size_attn : 360\n",
      "\tnum_layers : 2\n",
      "\tnum_layers_encoder : 1\n",
      "\tdropout : 0.4\n",
      "\tscaled_dropout : 0.1\n",
      "\ttemper_value : 0.5\n",
      "\tword_dropout : 0.06\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 1.0\n",
      "\tweight_rest : 1.0\n",
      "\tself_att : True\n",
      "\tself_att_and_rnn : False\n",
      "\tuse_lemmas : False\n",
      "\tpreload_lemmas : False\n",
      "\tobj_sub_pos : True\n",
      "\tuse_batch_norm : True\n",
      "\tdiagonal_positional_attention : True\n",
      "\trelative_positions : True\n",
      "\tnew_residual : True\n",
      "\tn_head : 3\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.1\n",
      "\tlr_decay : 0.9\n",
      "\tdecay_epoch : 15\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 100\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 5.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 1\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 70_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/70_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  3\n",
      "d_v and d_k:  120.0\n",
      "Fine-tune all embeddings.\n",
      "Using weights [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 08:12:43.731474: step 400/150100 (epoch 1/100), loss = 0.672178 (1.983 sec/batch), lr: 0.100000\n",
      "2018-05-09 08:27:42.352340: step 800/150100 (epoch 1/100), loss = 0.643259 (2.480 sec/batch), lr: 0.100000\n",
      "2018-05-09 08:42:34.086090: step 1200/150100 (epoch 1/100), loss = 0.568134 (1.775 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2572.0 / guessed_by_relation= 5549.0\n",
      "Calculating Recall: correct_by_relation= 2572.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 46.351%\n",
      "   Recall (micro): 39.820%\n",
      "       F1 (micro): 42.838%\n",
      "epoch 1: train_loss = 0.725241, dev_loss = 0.734849, dev_f1 = 0.4284\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 09:02:25.346406: step 1600/150100 (epoch 2/100), loss = 0.372514 (2.128 sec/batch), lr: 0.100000\n",
      "2018-05-09 09:17:21.813433: step 2000/150100 (epoch 2/100), loss = 0.798355 (2.416 sec/batch), lr: 0.100000\n",
      "2018-05-09 09:32:20.217613: step 2400/150100 (epoch 2/100), loss = 0.575154 (2.485 sec/batch), lr: 0.100000\n",
      "2018-05-09 09:47:13.637534: step 2800/150100 (epoch 2/100), loss = 0.727426 (2.468 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3540.0 / guessed_by_relation= 7801.0\n",
      "Calculating Recall: correct_by_relation= 3540.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 45.379%\n",
      "   Recall (micro): 54.807%\n",
      "       F1 (micro): 49.649%\n",
      "epoch 2: train_loss = 0.564449, dev_loss = 0.691617, dev_f1 = 0.4965\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 10:07:09.126099: step 3200/150100 (epoch 3/100), loss = 0.590020 (1.858 sec/batch), lr: 0.100000\n",
      "2018-05-09 10:22:08.985150: step 3600/150100 (epoch 3/100), loss = 0.498986 (2.403 sec/batch), lr: 0.100000\n",
      "2018-05-09 10:37:04.724240: step 4000/150100 (epoch 3/100), loss = 0.420339 (2.139 sec/batch), lr: 0.100000\n",
      "2018-05-09 10:52:01.306574: step 4400/150100 (epoch 3/100), loss = 0.552669 (2.325 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3639.0 / guessed_by_relation= 7651.0\n",
      "Calculating Recall: correct_by_relation= 3639.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.562%\n",
      "   Recall (micro): 56.340%\n",
      "       F1 (micro): 51.580%\n",
      "epoch 3: train_loss = 0.516387, dev_loss = 0.627284, dev_f1 = 0.5158\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 11:11:50.899454: step 4800/150100 (epoch 4/100), loss = 0.391856 (1.944 sec/batch), lr: 0.100000\n",
      "2018-05-09 11:26:50.376489: step 5200/150100 (epoch 4/100), loss = 0.703808 (2.086 sec/batch), lr: 0.100000\n",
      "2018-05-09 11:41:56.675674: step 5600/150100 (epoch 4/100), loss = 0.731819 (2.122 sec/batch), lr: 0.100000\n",
      "2018-05-09 11:57:09.237521: step 6000/150100 (epoch 4/100), loss = 0.294241 (2.233 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3846.0 / guessed_by_relation= 7982.0\n",
      "Calculating Recall: correct_by_relation= 3846.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 48.183%\n",
      "   Recall (micro): 59.545%\n",
      "       F1 (micro): 53.265%\n",
      "epoch 4: train_loss = 0.485088, dev_loss = 0.619034, dev_f1 = 0.5327\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 12:17:18.474664: step 6400/150100 (epoch 5/100), loss = 0.431132 (2.177 sec/batch), lr: 0.100000\n",
      "2018-05-09 12:32:37.375375: step 6800/150100 (epoch 5/100), loss = 0.374609 (2.457 sec/batch), lr: 0.100000\n",
      "2018-05-09 12:47:47.104686: step 7200/150100 (epoch 5/100), loss = 0.475376 (2.152 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3838.0 / guessed_by_relation= 7327.0\n",
      "Calculating Recall: correct_by_relation= 3838.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.382%\n",
      "   Recall (micro): 59.421%\n",
      "       F1 (micro): 55.680%\n",
      "epoch 5: train_loss = 0.468431, dev_loss = 0.559762, dev_f1 = 0.5568\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 13:07:55.125593: step 7600/150100 (epoch 6/100), loss = 0.511035 (2.132 sec/batch), lr: 0.100000\n",
      "2018-05-09 13:23:07.714512: step 8000/150100 (epoch 6/100), loss = 0.273401 (2.581 sec/batch), lr: 0.100000\n",
      "2018-05-09 13:38:21.070471: step 8400/150100 (epoch 6/100), loss = 0.449323 (2.368 sec/batch), lr: 0.100000\n",
      "2018-05-09 13:53:33.473896: step 8800/150100 (epoch 6/100), loss = 0.414616 (2.158 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4242.0 / guessed_by_relation= 8340.0\n",
      "Calculating Recall: correct_by_relation= 4242.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.863%\n",
      "   Recall (micro): 65.676%\n",
      "       F1 (micro): 57.328%\n",
      "epoch 6: train_loss = 0.451394, dev_loss = 0.575479, dev_f1 = 0.5733\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 14:13:44.323329: step 9200/150100 (epoch 7/100), loss = 0.537911 (2.276 sec/batch), lr: 0.100000\n",
      "2018-05-09 14:28:57.162915: step 9600/150100 (epoch 7/100), loss = 0.180731 (2.463 sec/batch), lr: 0.100000\n",
      "2018-05-09 14:44:11.851419: step 10000/150100 (epoch 7/100), loss = 0.380228 (2.264 sec/batch), lr: 0.100000\n",
      "2018-05-09 14:59:26.525886: step 10400/150100 (epoch 7/100), loss = 0.202390 (2.431 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4169.0 / guessed_by_relation= 7817.0\n",
      "Calculating Recall: correct_by_relation= 4169.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.332%\n",
      "   Recall (micro): 64.546%\n",
      "       F1 (micro): 58.406%\n",
      "epoch 7: train_loss = 0.440680, dev_loss = 0.547952, dev_f1 = 0.5841\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 15:19:34.542783: step 10800/150100 (epoch 8/100), loss = 0.333510 (2.551 sec/batch), lr: 0.100000\n",
      "2018-05-09 15:34:50.360291: step 11200/150100 (epoch 8/100), loss = 0.661242 (2.550 sec/batch), lr: 0.100000\n",
      "2018-05-09 15:50:04.974598: step 11600/150100 (epoch 8/100), loss = 0.446051 (2.514 sec/batch), lr: 0.100000\n",
      "2018-05-09 16:05:20.969579: step 12000/150100 (epoch 8/100), loss = 0.273066 (2.040 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4159.0 / guessed_by_relation= 7471.0\n",
      "Calculating Recall: correct_by_relation= 4159.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 55.669%\n",
      "   Recall (micro): 64.391%\n",
      "       F1 (micro): 59.713%\n",
      "epoch 8: train_loss = 0.434478, dev_loss = 0.527501, dev_f1 = 0.5971\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 16:25:32.234116: step 12400/150100 (epoch 9/100), loss = 0.381990 (2.507 sec/batch), lr: 0.100000\n",
      "2018-05-09 16:40:50.305621: step 12800/150100 (epoch 9/100), loss = 0.356744 (2.552 sec/batch), lr: 0.100000\n",
      "2018-05-09 16:56:01.367478: step 13200/150100 (epoch 9/100), loss = 0.501884 (2.680 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4281.0 / guessed_by_relation= 7634.0\n",
      "Calculating Recall: correct_by_relation= 4281.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.078%\n",
      "   Recall (micro): 66.280%\n",
      "       F1 (micro): 60.754%\n",
      "epoch 9: train_loss = 0.428965, dev_loss = 0.532143, dev_f1 = 0.6075\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 17:16:08.458911: step 13600/150100 (epoch 10/100), loss = 0.478090 (2.416 sec/batch), lr: 0.100000\n",
      "2018-05-09 17:31:24.560174: step 14000/150100 (epoch 10/100), loss = 0.482859 (2.188 sec/batch), lr: 0.100000\n",
      "2018-05-09 17:46:40.124008: step 14400/150100 (epoch 10/100), loss = 0.260602 (2.460 sec/batch), lr: 0.100000\n",
      "2018-05-09 18:01:51.890739: step 14800/150100 (epoch 10/100), loss = 0.557801 (1.826 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4219.0 / guessed_by_relation= 7393.0\n",
      "Calculating Recall: correct_by_relation= 4219.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 57.067%\n",
      "   Recall (micro): 65.320%\n",
      "       F1 (micro): 60.915%\n",
      "epoch 10: train_loss = 0.423308, dev_loss = 0.519485, dev_f1 = 0.6092\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 18:22:00.459103: step 15200/150100 (epoch 11/100), loss = 0.546064 (1.646 sec/batch), lr: 0.100000\n",
      "2018-05-09 18:36:56.384414: step 15600/150100 (epoch 11/100), loss = 0.481607 (2.287 sec/batch), lr: 0.100000\n",
      "2018-05-09 18:51:52.390042: step 16000/150100 (epoch 11/100), loss = 0.392474 (2.579 sec/batch), lr: 0.100000\n",
      "2018-05-09 19:06:44.207397: step 16400/150100 (epoch 11/100), loss = 0.281474 (2.374 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4434.0 / guessed_by_relation= 8534.0\n",
      "Calculating Recall: correct_by_relation= 4434.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.957%\n",
      "   Recall (micro): 68.648%\n",
      "       F1 (micro): 59.148%\n",
      "epoch 11: train_loss = 0.417789, dev_loss = 0.563710, dev_f1 = 0.5915\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 19:26:39.712416: step 16800/150100 (epoch 12/100), loss = 0.609022 (2.191 sec/batch), lr: 0.100000\n",
      "2018-05-09 19:41:31.619666: step 17200/150100 (epoch 12/100), loss = 0.319042 (1.940 sec/batch), lr: 0.100000\n",
      "2018-05-09 19:56:27.839345: step 17600/150100 (epoch 12/100), loss = 0.394123 (2.096 sec/batch), lr: 0.100000\n",
      "2018-05-09 20:11:16.233088: step 18000/150100 (epoch 12/100), loss = 0.381593 (2.287 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4468.0 / guessed_by_relation= 8471.0\n",
      "Calculating Recall: correct_by_relation= 4468.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.745%\n",
      "   Recall (micro): 69.175%\n",
      "       F1 (micro): 59.853%\n",
      "epoch 12: train_loss = 0.412748, dev_loss = 0.553524, dev_f1 = 0.5985\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 20:31:16.403541: step 18400/150100 (epoch 13/100), loss = 0.458054 (2.415 sec/batch), lr: 0.100000\n",
      "2018-05-09 20:46:15.778304: step 18800/150100 (epoch 13/100), loss = 0.190527 (1.986 sec/batch), lr: 0.100000\n",
      "2018-05-09 21:01:14.350932: step 19200/150100 (epoch 13/100), loss = 0.296041 (2.266 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4391.0 / guessed_by_relation= 8032.0\n",
      "Calculating Recall: correct_by_relation= 4391.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.669%\n",
      "   Recall (micro): 67.983%\n",
      "       F1 (micro): 60.603%\n",
      "epoch 13: train_loss = 0.412182, dev_loss = 0.534284, dev_f1 = 0.6060\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 21:21:37.026830: step 19600/150100 (epoch 14/100), loss = 0.235359 (1.772 sec/batch), lr: 0.100000\n",
      "2018-05-09 21:36:35.662879: step 20000/150100 (epoch 14/100), loss = 0.450734 (2.314 sec/batch), lr: 0.100000\n",
      "2018-05-09 21:51:26.242327: step 20400/150100 (epoch 14/100), loss = 0.427523 (2.262 sec/batch), lr: 0.100000\n",
      "2018-05-09 22:06:08.451441: step 20800/150100 (epoch 14/100), loss = 0.298579 (2.091 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4475.0 / guessed_by_relation= 8359.0\n",
      "Calculating Recall: correct_by_relation= 4475.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.535%\n",
      "   Recall (micro): 69.283%\n",
      "       F1 (micro): 60.400%\n",
      "epoch 14: train_loss = 0.404234, dev_loss = 0.546138, dev_f1 = 0.6040\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 22:26:05.945067: step 21200/150100 (epoch 15/100), loss = 0.494139 (2.418 sec/batch), lr: 0.100000\n",
      "2018-05-09 22:41:31.918595: step 21600/150100 (epoch 15/100), loss = 0.333924 (2.033 sec/batch), lr: 0.100000\n",
      "2018-05-09 22:56:57.998406: step 22000/150100 (epoch 15/100), loss = 0.352226 (1.843 sec/batch), lr: 0.100000\n",
      "2018-05-09 23:12:24.315849: step 22400/150100 (epoch 15/100), loss = 0.488601 (2.607 sec/batch), lr: 0.100000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4481.0 / guessed_by_relation= 8456.0\n",
      "Calculating Recall: correct_by_relation= 4481.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 52.992%\n",
      "   Recall (micro): 69.376%\n",
      "       F1 (micro): 60.087%\n",
      "epoch 15: train_loss = 0.403141, dev_loss = 0.557869, dev_f1 = 0.6009\n",
      "model saved to ./saved_models/70_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "Current params:  heads-3 enc_layers-1  drop-0.4 scaled_drop-0.1 lr-0.1  lr_decay-0.9 max_grad_norm-5.0\n",
      " weight_no_rel-1.0 weight_rest-1.0 attn-True attn_dim-200  obj_sub_pos-True new_residual-True\n",
      " use_batch_norm-True relative_positions-True  decay_epoch-15 use_lemmas-False  hidden_self-150\n",
      "2018-05-09 23:32:51.790513: step 22800/150100 (epoch 16/100), loss = 0.360429 (2.182 sec/batch), lr: 0.100000\n",
      "2018-05-09 23:48:18.201204: step 23200/150100 (epoch 16/100), loss = 0.401604 (2.437 sec/batch), lr: 0.100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # run training in main() so that the dataloader runs in a multiprocessing mode regardless of the OS used\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(1234)\n",
    "    if args.cpu:\n",
    "        args.cuda = False\n",
    "    elif args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    print(\"i am here\")\n",
    "    # make opt\n",
    "    opt = vars(args)\n",
    "    opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "    # load vocab\n",
    "    vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "    vocab = Vocab(vocab_file, load=True)\n",
    "\n",
    "    # new_vocab_size = 30000\n",
    "    opt['vocab_size'] = vocab.size\n",
    "    emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "    emb_matrix = np.load(emb_file)\n",
    "    assert emb_matrix.shape[0] == vocab.size\n",
    "    assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "    # load data\n",
    "    print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "    train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "    dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "    model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "    model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "    opt['model_save_dir'] = model_save_dir\n",
    "    helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "    # save config\n",
    "    helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "    vocab.save(model_save_dir + '/vocab.pkl')\n",
    "    file_logger = helper.FileLogger(\n",
    "        model_save_dir + '/' + opt['log'],\n",
    "        header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_p\\tdev_r\\tdev_f1\"\n",
    "    )\n",
    "\n",
    "    # print model info\n",
    "    helper.print_config(opt)\n",
    "\n",
    "    # model\n",
    "    model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "    id2label = dict([(v, k) for k, v in constant.LABEL_TO_ID.items()])\n",
    "    dev_f1_history = []\n",
    "    current_lr = opt['lr']\n",
    "\n",
    "    global_step = 0\n",
    "    global_start_time = time.time()\n",
    "    format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "    max_steps = len(train_batch) * opt['num_epoch']\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(1, opt['num_epoch']+1):\n",
    "\n",
    "        print(\n",
    "            \"Current params: \" + \" heads-\" + str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "            \" drop-\" + str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\" + str(opt[\"lr\"]),\n",
    "            \" lr_decay-\" + str(opt[\"lr_decay\"]) + \" max_grad_norm-\" + str(opt[\"max_grad_norm\"])\n",
    "        )\n",
    "        print(\n",
    "            \" weight_no_rel-\" + str(opt[\"weight_no_rel\"]) +\n",
    "            \" weight_rest-\" + str(opt[\"weight_rest\"]) + \" attn-\" + str(opt[\"attn\"]) + \" attn_dim-\" + str(opt[\"attn_dim\"]),\n",
    "            \" obj_sub_pos-\" + str(opt[\"obj_sub_pos\"]) + \" new_residual-\" + str(opt[\"new_residual\"])\n",
    "        )\n",
    "        print(\n",
    "            \" use_batch_norm-\"+str(opt[\"use_batch_norm\"]) + \" relative_positions-\"+str(opt[\"relative_positions\"]),\n",
    "            \" decay_epoch-\"+str(opt[\"decay_epoch\"]) + \" use_lemmas-\"+str(opt[\"use_lemmas\"]),\n",
    "            \" hidden_self-\"+str(opt[\"hidden_self\"])\n",
    "        )\n",
    "\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_batch):\n",
    "            start_time = time.time()\n",
    "            global_step += 1\n",
    "\n",
    "            loss = model.update(batch)\n",
    "\n",
    "            train_loss += loss\n",
    "            if global_step % opt['log_step'] == 0:\n",
    "                duration = time.time() - start_time\n",
    "                print(\n",
    "                    format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                    opt['num_epoch'], loss, duration, current_lr)\n",
    "                )\n",
    "            # do garbage collection,\n",
    "            # as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "            del loss\n",
    "\n",
    "        # eval on dev\n",
    "        print(\"Evaluating on dev set...\")\n",
    "        predictions = []\n",
    "        dev_loss = 0\n",
    "        for i, batch in enumerate(dev_batch):\n",
    "            preds, _, loss = model.predict(batch)\n",
    "            predictions += preds\n",
    "            dev_loss += loss\n",
    "\n",
    "        predictions = [id2label[p] for p in predictions]\n",
    "        dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "\n",
    "        train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "        dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "        print(\n",
    "            \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "                train_loss, dev_loss, dev_f1)\n",
    "            )\n",
    "        file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(\n",
    "            epoch, train_loss, dev_loss, dev_p, dev_r, dev_f1)\n",
    "        )\n",
    "\n",
    "        # save\n",
    "        model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "        model.save(model_file, epoch)\n",
    "        if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "            copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "            print(\"new best model saved.\")\n",
    "        if epoch % opt['save_epoch'] != 0:\n",
    "            os.remove(model_file)\n",
    "\n",
    "        # decay schedule # 15 is best!\n",
    "        if len(dev_f1_history) > opt['decay_epoch'] and dev_f1 <= dev_f1_history[-1]:  # and opt['optim'] in ['sgd', 'asgd', 'adagrad', 'adam', 'nadam']\n",
    "            current_lr *= opt['lr_decay']\n",
    "            model.update_lr(current_lr)\n",
    "\n",
    "        dev_f1_history += [dev_f1]\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
