{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of lstm layers.')\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=3, help='Num of self-attention encoders.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Input and attn dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')        # 0.1 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=2.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=1.0, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=1, help='Number of self-attention heads.')\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=True)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.3, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=10, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='37_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/37_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/37_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tnum_layers_encoder : 3\n",
      "\tdropout : 0.6\n",
      "\tscaled_dropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 2.0\n",
      "\tweight_rest : 1.0\n",
      "\tself_att : True\n",
      "\tn_head : 1\n",
      "\tattn : True\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.3\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 1.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 10\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 37_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/37_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  1\n",
      "d_v and d_k:  360.0\n",
      "Finetune all embeddings.\n",
      "Using weights [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-21 15:12:36.847543: step 400/300200 (epoch 1/200), loss = 0.418173 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:13:03.972126: step 800/300200 (epoch 1/200), loss = 0.464162 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:13:30.459014: step 1200/300200 (epoch 1/200), loss = 0.476079 (0.057 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 2520.0 / guessed_by_relation= 5857.0\n",
      "Calculating Recall: correct_by_relation= 2520.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 43.025%\n",
      "   Recall (micro): 39.015%\n",
      "       F1 (micro): 40.922%\n",
      "epoch 1: train_loss = 0.506464, dev_loss = 0.623236, dev_f1 = 0.4092\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:14:08.029868: step 1600/300200 (epoch 2/200), loss = 0.323578 (0.069 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:14:34.773940: step 2000/300200 (epoch 2/200), loss = 0.606650 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:15:01.811293: step 2400/300200 (epoch 2/200), loss = 0.416640 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:15:28.438053: step 2800/300200 (epoch 2/200), loss = 0.532935 (0.066 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3312.0 / guessed_by_relation= 6989.0\n",
      "Calculating Recall: correct_by_relation= 3312.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 47.389%\n",
      "   Recall (micro): 51.277%\n",
      "       F1 (micro): 49.256%\n",
      "epoch 2: train_loss = 0.397227, dev_loss = 0.567044, dev_f1 = 0.4926\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:16:05.961271: step 3200/300200 (epoch 3/200), loss = 0.399616 (0.057 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:16:32.116281: step 3600/300200 (epoch 3/200), loss = 0.347402 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:16:58.213638: step 4000/300200 (epoch 3/200), loss = 0.287297 (0.071 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:17:25.804961: step 4400/300200 (epoch 3/200), loss = 0.362363 (0.067 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3171.0 / guessed_by_relation= 6142.0\n",
      "Calculating Recall: correct_by_relation= 3171.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.628%\n",
      "   Recall (micro): 49.094%\n",
      "       F1 (micro): 50.329%\n",
      "epoch 3: train_loss = 0.365361, dev_loss = 0.518727, dev_f1 = 0.5033\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:18:04.591536: step 4800/300200 (epoch 4/200), loss = 0.286578 (0.060 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:18:31.933698: step 5200/300200 (epoch 4/200), loss = 0.514012 (0.060 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:18:57.729749: step 5600/300200 (epoch 4/200), loss = 0.542993 (0.068 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:19:21.956134: step 6000/300200 (epoch 4/200), loss = 0.254378 (0.068 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3334.0 / guessed_by_relation= 6749.0\n",
      "Calculating Recall: correct_by_relation= 3334.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.400%\n",
      "   Recall (micro): 51.618%\n",
      "       F1 (micro): 50.485%\n",
      "epoch 4: train_loss = 0.348693, dev_loss = 0.513484, dev_f1 = 0.5048\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:19:56.358558: step 6400/300200 (epoch 5/200), loss = 0.366009 (0.066 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:20:21.255223: step 6800/300200 (epoch 5/200), loss = 0.292269 (0.063 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:20:46.076188: step 7200/300200 (epoch 5/200), loss = 0.386081 (0.059 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3347.0 / guessed_by_relation= 6196.0\n",
      "Calculating Recall: correct_by_relation= 3347.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.019%\n",
      "   Recall (micro): 51.819%\n",
      "       F1 (micro): 52.896%\n",
      "epoch 5: train_loss = 0.336672, dev_loss = 0.475047, dev_f1 = 0.5290\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:21:21.052135: step 7600/300200 (epoch 6/200), loss = 0.321410 (0.067 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:21:46.046057: step 8000/300200 (epoch 6/200), loss = 0.214230 (0.064 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:22:11.010398: step 8400/300200 (epoch 6/200), loss = 0.333333 (0.062 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:22:35.692491: step 8800/300200 (epoch 6/200), loss = 0.334780 (0.052 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3589.0 / guessed_by_relation= 7116.0\n",
      "Calculating Recall: correct_by_relation= 3589.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 50.436%\n",
      "   Recall (micro): 55.566%\n",
      "       F1 (micro): 52.877%\n",
      "epoch 6: train_loss = 0.326452, dev_loss = 0.533040, dev_f1 = 0.5288\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:23:10.868470: step 9200/300200 (epoch 7/200), loss = 0.384801 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:23:36.162191: step 9600/300200 (epoch 7/200), loss = 0.187464 (0.062 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:24:01.514063: step 10000/300200 (epoch 7/200), loss = 0.224112 (0.061 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:24:26.763663: step 10400/300200 (epoch 7/200), loss = 0.206587 (0.059 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3843.0 / guessed_by_relation= 7837.0\n",
      "Calculating Recall: correct_by_relation= 3843.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 49.037%\n",
      "   Recall (micro): 59.498%\n",
      "       F1 (micro): 53.763%\n",
      "epoch 7: train_loss = 0.320861, dev_loss = 0.532013, dev_f1 = 0.5376\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:25:02.558286: step 10800/300200 (epoch 8/200), loss = 0.299596 (0.066 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:25:27.778312: step 11200/300200 (epoch 8/200), loss = 0.585584 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:25:52.474945: step 11600/300200 (epoch 8/200), loss = 0.337982 (0.064 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:26:17.753623: step 12000/300200 (epoch 8/200), loss = 0.162795 (0.054 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3836.0 / guessed_by_relation= 7114.0\n",
      "Calculating Recall: correct_by_relation= 3836.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 53.922%\n",
      "   Recall (micro): 59.390%\n",
      "       F1 (micro): 56.524%\n",
      "epoch 8: train_loss = 0.317293, dev_loss = 0.497519, dev_f1 = 0.5652\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:26:52.647355: step 12400/300200 (epoch 9/200), loss = 0.414723 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:27:18.208784: step 12800/300200 (epoch 9/200), loss = 0.257012 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:27:43.292444: step 13200/300200 (epoch 9/200), loss = 0.371746 (0.066 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3898.0 / guessed_by_relation= 7575.0\n",
      "Calculating Recall: correct_by_relation= 3898.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 51.459%\n",
      "   Recall (micro): 60.350%\n",
      "       F1 (micro): 55.551%\n",
      "epoch 9: train_loss = 0.312862, dev_loss = 0.528097, dev_f1 = 0.5555\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:28:19.039440: step 13600/300200 (epoch 10/200), loss = 0.362714 (0.066 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:28:44.556752: step 14000/300200 (epoch 10/200), loss = 0.322302 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:29:09.975803: step 14400/300200 (epoch 10/200), loss = 0.247492 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:29:35.115611: step 14800/300200 (epoch 10/200), loss = 0.306992 (0.051 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3704.0 / guessed_by_relation= 6346.0\n",
      "Calculating Recall: correct_by_relation= 3704.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.367%\n",
      "   Recall (micro): 57.346%\n",
      "       F1 (micro): 57.852%\n",
      "epoch 10: train_loss = 0.307373, dev_loss = 0.454362, dev_f1 = 0.5785\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:30:09.779730: step 15200/300200 (epoch 11/200), loss = 0.471386 (0.046 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:30:34.525996: step 15600/300200 (epoch 11/200), loss = 0.372465 (0.068 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:30:59.083762: step 16000/300200 (epoch 11/200), loss = 0.321505 (0.065 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:31:24.505821: step 16400/300200 (epoch 11/200), loss = 0.288129 (0.065 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3944.0 / guessed_by_relation= 7199.0\n",
      "Calculating Recall: correct_by_relation= 3944.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 54.785%\n",
      "   Recall (micro): 61.062%\n",
      "       F1 (micro): 57.754%\n",
      "epoch 11: train_loss = 0.305261, dev_loss = 0.478633, dev_f1 = 0.5775\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:32:00.524038: step 16800/300200 (epoch 12/200), loss = 0.509814 (0.062 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:32:25.342994: step 17200/300200 (epoch 12/200), loss = 0.276205 (0.052 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:32:49.748852: step 17600/300200 (epoch 12/200), loss = 0.253624 (0.056 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:33:14.299595: step 18000/300200 (epoch 12/200), loss = 0.254414 (0.065 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3964.0 / guessed_by_relation= 7019.0\n",
      "Calculating Recall: correct_by_relation= 3964.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.475%\n",
      "   Recall (micro): 61.372%\n",
      "       F1 (micro): 58.822%\n",
      "epoch 12: train_loss = 0.302113, dev_loss = 0.469172, dev_f1 = 0.5882\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:33:48.408740: step 18400/300200 (epoch 13/200), loss = 0.393286 (0.069 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:34:13.619236: step 18800/300200 (epoch 13/200), loss = 0.158765 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:34:38.854298: step 19200/300200 (epoch 13/200), loss = 0.230903 (0.060 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3881.0 / guessed_by_relation= 6534.0\n",
      "Calculating Recall: correct_by_relation= 3881.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 59.397%\n",
      "   Recall (micro): 60.087%\n",
      "       F1 (micro): 59.740%\n",
      "epoch 13: train_loss = 0.298804, dev_loss = 0.449686, dev_f1 = 0.5974\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:35:14.117508: step 19600/300200 (epoch 14/200), loss = 0.237653 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:35:39.764168: step 20000/300200 (epoch 14/200), loss = 0.309235 (0.070 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:36:05.276967: step 20400/300200 (epoch 14/200), loss = 0.311467 (0.067 sec/batch), lr: 0.300000\n",
      "2018-03-21 15:36:30.700028: step 20800/300200 (epoch 14/200), loss = 0.215276 (0.064 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3852.0 / guessed_by_relation= 6814.0\n",
      "Calculating Recall: correct_by_relation= 3852.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.531%\n",
      "   Recall (micro): 59.638%\n",
      "       F1 (micro): 58.043%\n",
      "epoch 14: train_loss = 0.296625, dev_loss = 0.460938, dev_f1 = 0.5804\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:37:06.234466: step 21200/300200 (epoch 15/200), loss = 0.344642 (0.065 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:37:31.486071: step 21600/300200 (epoch 15/200), loss = 0.249414 (0.054 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:37:56.722136: step 22000/300200 (epoch 15/200), loss = 0.271910 (0.050 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:38:22.406896: step 22400/300200 (epoch 15/200), loss = 0.304169 (0.066 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3883.0 / guessed_by_relation= 6680.0\n",
      "Calculating Recall: correct_by_relation= 3883.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.129%\n",
      "   Recall (micro): 60.118%\n",
      "       F1 (micro): 59.106%\n",
      "epoch 15: train_loss = 0.292440, dev_loss = 0.451308, dev_f1 = 0.5911\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:38:57.419444: step 22800/300200 (epoch 16/200), loss = 0.156099 (0.057 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:39:22.367743: step 23200/300200 (epoch 16/200), loss = 0.297024 (0.063 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:39:47.507555: step 23600/300200 (epoch 16/200), loss = 0.231833 (0.056 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:40:12.662410: step 24000/300200 (epoch 16/200), loss = 0.261610 (0.066 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3612.0 / guessed_by_relation= 5703.0\n",
      "Calculating Recall: correct_by_relation= 3612.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 63.335%\n",
      "   Recall (micro): 55.922%\n",
      "       F1 (micro): 59.398%\n",
      "epoch 16: train_loss = 0.290287, dev_loss = 0.411937, dev_f1 = 0.5940\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:40:47.870474: step 24400/300200 (epoch 17/200), loss = 0.206464 (0.064 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:41:13.075459: step 24800/300200 (epoch 17/200), loss = 0.293876 (0.066 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:41:37.960590: step 25200/300200 (epoch 17/200), loss = 0.310207 (0.054 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 3816.0 / guessed_by_relation= 6198.0\n",
      "Calculating Recall: correct_by_relation= 3816.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 61.568%\n",
      "   Recall (micro): 59.080%\n",
      "       F1 (micro): 60.299%\n",
      "epoch 17: train_loss = 0.289414, dev_loss = 0.425678, dev_f1 = 0.6030\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:42:12.086282: step 25600/300200 (epoch 18/200), loss = 0.190057 (0.065 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:42:36.283585: step 26000/300200 (epoch 18/200), loss = 0.207003 (0.055 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:43:01.364237: step 26400/300200 (epoch 18/200), loss = 0.241256 (0.058 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:43:26.225806: step 26800/300200 (epoch 18/200), loss = 0.399364 (0.052 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4184.0 / guessed_by_relation= 7202.0\n",
      "Calculating Recall: correct_by_relation= 4184.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 58.095%\n",
      "   Recall (micro): 64.778%\n",
      "       F1 (micro): 61.255%\n",
      "epoch 18: train_loss = 0.286525, dev_loss = 0.465585, dev_f1 = 0.6125\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-21 15:44:01.491529: step 27200/300200 (epoch 19/200), loss = 0.367241 (0.053 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:44:26.394709: step 27600/300200 (epoch 19/200), loss = 0.144855 (0.050 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:44:51.420214: step 28000/300200 (epoch 19/200), loss = 0.174681 (0.056 sec/batch), lr: 0.285000\n",
      "2018-03-21 15:45:16.442209: step 28400/300200 (epoch 19/200), loss = 0.301265 (0.059 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4208.0 / guessed_by_relation= 7476.0\n",
      "Calculating Recall: correct_by_relation= 4208.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 56.287%\n",
      "   Recall (micro): 65.149%\n",
      "       F1 (micro): 60.395%\n",
      "epoch 19: train_loss = 0.284610, dev_loss = 0.464476, dev_f1 = 0.6039\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.6 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    \n",
    "    print(\n",
    "        \"Current params: \"+ \" heads-\"+ str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "        \" drop-\"+ str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\"+ str(opt[\"lr\"]),\n",
    "        \" lr_decay-\"+ str(opt[\"lr_decay\"]) + \" grad_norm-\"+ str(opt[\"max_grad_norm\"])\n",
    "    )\n",
    "    print(\n",
    "        \" weight_no_rel-\"+ str(opt[\"weight_no_rel\"]) +\n",
    "        \" weight_rest-\"+ str(opt[\"weight_rest\"]) + \" attn_dim-\"+ str(opt[\"attn_dim\"])\n",
    "    )\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    \n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save the model during every epoch, if fscore is the best, move it to best_model.pkl\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    # delete single checkpoints based on the save_epoch int\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "\n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        # don't go lower than 0.01 lr\n",
    "        if current_lr >= 0.01:\n",
    "            current_lr *= opt['lr_decay']\n",
    "            model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout\n",
    "# try with lower maxnorm\n",
    "# try adam and nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without residual and with ulinear\n",
    "# other weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
