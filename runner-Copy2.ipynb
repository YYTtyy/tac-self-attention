{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model on TACRED.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data.loader import DataLoader\n",
    "from model.rnn import RelationModel\n",
    "from utils import scorer, constant, helper\n",
    "from utils.vocab import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.argv=['']; del sys  # this has to be done if argparse is used in the notebook\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default='dataset/tacred')\n",
    "parser.add_argument('--vocab_dir', type=str, default='dataset/vocab')\n",
    "parser.add_argument('--emb_dim', type=int, default=300, help='Word embedding dimension.')\n",
    "parser.add_argument('--ner_dim', type=int, default=30, help='NER embedding dimension.')\n",
    "parser.add_argument('--pos_dim', type=int, default=30, help='POS embedding dimension.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=360, help='RNN hidden state size.')            # 200 original\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='Num of lstm layers.')\n",
    "parser.add_argument('--num_layers_encoder', type=int, default=3, help='Num of self-attention encoders.')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='Input and attn dropout rate.')        # 0.5 original\n",
    "parser.add_argument('--scaled_dropout', type=float, default=0.1, help='Input and scaled dropout rate.')        # 0.1 original\n",
    "parser.add_argument('--word_dropout', type=float, default=0.04,                                      # 0.04\n",
    "                    help='The rate at which randomly set a word to UNK.'\n",
    "                   )\n",
    "parser.add_argument('--lstm_dropout', type=float, default=0.5, help='Input and RNN dropout rate.')\n",
    "parser.add_argument('--topn', type=int, default=1e10, help='Only finetune top N embeddings.')\n",
    "parser.add_argument('--lower', dest='lower', action='store_true', help='Lowercase all words.',\n",
    "                   default=True)\n",
    "\n",
    "parser.add_argument('--no-lower', dest='lower', action='store_false')\n",
    "parser.set_defaults(lower=False)\n",
    "\n",
    "parser.add_argument('--weight_no_rel', type=float, default=2.0, help='Weight for no_relation class.')\n",
    "parser.add_argument('--weight_rest', type=float, default=1.0, help='Weight for other classes.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--self-attn', dest='self_att', action='store_true', \n",
    "    help='Use self-attention layer instead of LSTM.', default=True\n",
    ")\n",
    "parser.add_argument('--obj_sub_pos', dest='obj_sub_pos', action='store_true', \n",
    "    help='In self-attention add obj/subg positional vectors.', default=True)\n",
    "parser.add_argument('--use_batch_norm', dest='use_batch_norm', action='store_true', \n",
    "    help='BatchNorm if True, else LayerNorm in self-attention.', default=True)\n",
    "\n",
    "\n",
    "parser.add_argument('--n_head', type=int, default=1, help='Number of self-attention heads.')\n",
    "parser.add_argument('--attn', dest='attn', action='store_true', help='Use attention layer.', default=\"true\")\n",
    "parser.add_argument('--no-attn', dest='attn', action='store_false')\n",
    "parser.set_defaults(attn=False)\n",
    "\n",
    "parser.add_argument('--attn_dim', type=int, default=200, help='Attention size.')                    # 200 original\n",
    "parser.add_argument('--pe_dim', type=int, default=30, help='Position encoding dimension.')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.3, help='Applies to SGD and Adagrad.')            # lr 1.0 orig\n",
    "parser.add_argument('--lr_decay', type=float, default=0.95)                                          # lr_decay 0.9 original\n",
    "parser.add_argument('--optim', type=str, default='sgd', help='sgd, adagrad, adam or adamax.')       # sgd original\n",
    "parser.add_argument('--num_epoch', type=int, default=200)                                           # epochs 30 original\n",
    "parser.add_argument('--batch_size', type=int, default=50)                                           # batch size 50 original\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.0, help='Gradient clipping.')\n",
    "\n",
    "# info for model saving\n",
    "parser.add_argument('--log_step', type=int, default=400, help='Print log every k steps.')\n",
    "parser.add_argument('--log', type=str, default='logs.txt', help='Write training log to file.')\n",
    "parser.add_argument('--save_epoch', type=int, default=10, help='Save model checkpoints every k epochs.')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models', help='Root dir for saving models.')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--id', type=str, \n",
    "    default='37_self_attention_dropout',                                 # change model folder output before running\n",
    "    help='Model ID under which to save models.'\n",
    "   )\n",
    "\n",
    "parser.add_argument('--info', type=str, default='', help='Optional info for the experiment.')\n",
    "parser.add_argument('--seed', type=int, default=1234)\n",
    "parser.add_argument('--cuda', type=bool, default=torch.cuda.is_available())\n",
    "parser.add_argument('--cpu', action='store_true', help='Ignore CUDA.')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 55950 loaded from file\n",
      "Loading data from dataset/tacred with batch size 50...\n",
      "1501 batches created for dataset/tacred/train.json\n",
      "516 batches created for dataset/tacred/dev.json\n",
      "Config saved to file ./saved_models/37_self_attention_dropout/config.json\n",
      "Overwriting old vocab file at ./saved_models/37_self_attention_dropout/vocab.pkl\n",
      "\n",
      "Running with the following configs:\n",
      "\tdata_dir : dataset/tacred\n",
      "\tvocab_dir : dataset/vocab\n",
      "\temb_dim : 300\n",
      "\tner_dim : 30\n",
      "\tpos_dim : 30\n",
      "\thidden_dim : 360\n",
      "\tnum_layers : 2\n",
      "\tnum_layers_encoder : 3\n",
      "\tdropout : 0.1\n",
      "\tscaled_dropout : 0.1\n",
      "\tword_dropout : 0.04\n",
      "\tlstm_dropout : 0.5\n",
      "\ttopn : 10000000000.0\n",
      "\tlower : False\n",
      "\tweight_no_rel : 2.0\n",
      "\tweight_rest : 1.0\n",
      "\tself_att : True\n",
      "\tn_head : 1\n",
      "\tattn : False\n",
      "\tattn_dim : 200\n",
      "\tpe_dim : 30\n",
      "\tlr : 0.3\n",
      "\tlr_decay : 0.95\n",
      "\toptim : sgd\n",
      "\tnum_epoch : 200\n",
      "\tbatch_size : 50\n",
      "\tmax_grad_norm : 1.0\n",
      "\tlog_step : 400\n",
      "\tlog : logs.txt\n",
      "\tsave_epoch : 10\n",
      "\tsave_dir : ./saved_models\n",
      "\tid : 37_self_attention_dropout\n",
      "\tinfo : \n",
      "\tseed : 1234\n",
      "\tcuda : True\n",
      "\tcpu : False\n",
      "\tnum_class : 42\n",
      "\tvocab_size : 55950\n",
      "\tmodel_save_dir : ./saved_models/37_self_attention_dropout\n",
      "\n",
      "\n",
      "Number of heads:  1\n",
      "d_v and d_k:  360.0\n",
      "Finetune all embeddings.\n",
      "Using weights [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(1234)\n",
    "if args.cpu:\n",
    "    args.cuda = False\n",
    "elif args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# make opt\n",
    "opt = vars(args)\n",
    "opt['num_class'] = len(constant.LABEL_TO_ID)\n",
    "\n",
    "# load vocab\n",
    "vocab_file = opt['vocab_dir'] + '/vocab.pkl'\n",
    "vocab = Vocab(vocab_file, load=True)\n",
    "opt['vocab_size'] = vocab.size\n",
    "emb_file = opt['vocab_dir'] + '/embedding.npy'\n",
    "emb_matrix = np.load(emb_file)\n",
    "assert emb_matrix.shape[0] == vocab.size\n",
    "assert emb_matrix.shape[1] == opt['emb_dim']\n",
    "\n",
    "# load data\n",
    "print(\"Loading data from {} with batch size {}...\".format(opt['data_dir'], opt['batch_size']))\n",
    "train_batch = DataLoader(opt['data_dir'] + '/train.json', opt['batch_size'], opt, vocab, evaluation=False)\n",
    "dev_batch = DataLoader(opt['data_dir'] + '/dev.json', opt['batch_size'], opt, vocab, evaluation=True)\n",
    "\n",
    "model_id = opt['id'] if len(opt['id']) > 1 else '0' + opt['id']\n",
    "model_save_dir = opt['save_dir'] + '/' + model_id\n",
    "opt['model_save_dir'] = model_save_dir\n",
    "helper.ensure_dir(model_save_dir, verbose=True)\n",
    "\n",
    "# save config\n",
    "helper.save_config(opt, model_save_dir + '/config.json', verbose=True)\n",
    "vocab.save(model_save_dir + '/vocab.pkl')\n",
    "file_logger = helper.FileLogger(model_save_dir + '/' + opt['log'], header=\"# epoch\\ttrain_loss\\tdev_loss\\tdev_f1\")\n",
    "\n",
    "# print model info\n",
    "helper.print_config(opt)\n",
    "\n",
    "# model\n",
    "model = RelationModel(opt, emb_matrix=emb_matrix)\n",
    "\n",
    "id2label = dict([(v,k) for k,v in constant.LABEL_TO_ID.items()])\n",
    "dev_f1_history = []\n",
    "current_lr = opt['lr']\n",
    "\n",
    "global_step = 0\n",
    "global_start_time = time.time()\n",
    "format_str = '{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}'\n",
    "max_steps = len(train_batch) * opt['num_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improves speed of cuda somehow, set to False by default due to memory usage\n",
    "torch.backends.cudnn.fastest=True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "J:\\Anaconda_Python3_6\\envs\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:357: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-22 19:04:47.390166: step 400/300200 (epoch 1/200), loss = 0.417585 (0.080 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:05:22.142786: step 800/300200 (epoch 1/200), loss = 0.473307 (0.090 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:05:55.620347: step 1200/300200 (epoch 1/200), loss = 0.384577 (0.068 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4285.0 / guessed_by_relation= 16544.0\n",
      "Calculating Recall: correct_by_relation= 4285.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 25.901%\n",
      "   Recall (micro): 66.342%\n",
      "       F1 (micro): 37.256%\n",
      "epoch 1: train_loss = 0.496414, dev_loss = 2.633952, dev_f1 = 0.3726\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_1.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:06:39.660993: step 1600/300200 (epoch 2/200), loss = 0.298455 (0.081 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:07:13.116992: step 2000/300200 (epoch 2/200), loss = 0.393380 (0.079 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:07:46.394966: step 2400/300200 (epoch 2/200), loss = 0.416046 (0.079 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:08:20.060106: step 2800/300200 (epoch 2/200), loss = 0.427434 (0.082 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4572.0 / guessed_by_relation= 18218.0\n",
      "Calculating Recall: correct_by_relation= 4572.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 25.096%\n",
      "   Recall (micro): 70.785%\n",
      "       F1 (micro): 37.055%\n",
      "epoch 2: train_loss = 0.348271, dev_loss = 2.812752, dev_f1 = 0.3705\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_2.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:09:04.330029: step 3200/300200 (epoch 3/200), loss = 0.301030 (0.064 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:09:31.722282: step 3600/300200 (epoch 3/200), loss = 0.268615 (0.049 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:09:51.959718: step 4000/300200 (epoch 3/200), loss = 0.242840 (0.053 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:10:12.688464: step 4400/300200 (epoch 3/200), loss = 0.263841 (0.048 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4696.0 / guessed_by_relation= 16724.0\n",
      "Calculating Recall: correct_by_relation= 4696.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 28.079%\n",
      "   Recall (micro): 72.705%\n",
      "       F1 (micro): 40.512%\n",
      "epoch 3: train_loss = 0.321008, dev_loss = 2.227030, dev_f1 = 0.4051\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_3.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:10:40.969697: step 4800/300200 (epoch 4/200), loss = 0.202229 (0.042 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:11:01.112280: step 5200/300200 (epoch 4/200), loss = 0.471439 (0.045 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:11:22.009748: step 5600/300200 (epoch 4/200), loss = 0.563826 (0.054 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:11:42.332810: step 6000/300200 (epoch 4/200), loss = 0.178201 (0.054 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4824.0 / guessed_by_relation= 16191.0\n",
      "Calculating Recall: correct_by_relation= 4824.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 29.794%\n",
      "   Recall (micro): 74.686%\n",
      "       F1 (micro): 42.596%\n",
      "epoch 4: train_loss = 0.304673, dev_loss = 2.021056, dev_f1 = 0.4260\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_4.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:12:10.869122: step 6400/300200 (epoch 5/200), loss = 0.311479 (0.058 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:12:31.050809: step 6800/300200 (epoch 5/200), loss = 0.205031 (0.049 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:12:51.120932: step 7200/300200 (epoch 5/200), loss = 0.315296 (0.047 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4727.0 / guessed_by_relation= 15189.0\n",
      "Calculating Recall: correct_by_relation= 4727.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 31.121%\n",
      "   Recall (micro): 73.185%\n",
      "       F1 (micro): 43.671%\n",
      "epoch 5: train_loss = 0.293665, dev_loss = 1.744419, dev_f1 = 0.4367\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_5.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:13:19.613355: step 7600/300200 (epoch 6/200), loss = 0.255491 (0.053 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:13:39.837154: step 8000/300200 (epoch 6/200), loss = 0.170858 (0.050 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:14:00.074319: step 8400/300200 (epoch 6/200), loss = 0.272676 (0.050 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:14:20.189830: step 8800/300200 (epoch 6/200), loss = 0.314967 (0.043 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4675.0 / guessed_by_relation= 15460.0\n",
      "Calculating Recall: correct_by_relation= 4675.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 30.239%\n",
      "   Recall (micro): 72.380%\n",
      "       F1 (micro): 42.657%\n",
      "epoch 6: train_loss = 0.283799, dev_loss = 1.828811, dev_f1 = 0.4266\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_6.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:14:48.706689: step 9200/300200 (epoch 7/200), loss = 0.361234 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:15:08.811171: step 9600/300200 (epoch 7/200), loss = 0.151116 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:15:28.949743: step 10000/300200 (epoch 7/200), loss = 0.161833 (0.048 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:15:49.099345: step 10400/300200 (epoch 7/200), loss = 0.126749 (0.048 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4576.0 / guessed_by_relation= 14771.0\n",
      "Calculating Recall: correct_by_relation= 4576.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 30.980%\n",
      "   Recall (micro): 70.847%\n",
      "       F1 (micro): 43.109%\n",
      "epoch 7: train_loss = 0.275273, dev_loss = 1.619816, dev_f1 = 0.4311\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_7.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:16:18.022284: step 10800/300200 (epoch 8/200), loss = 0.214774 (0.052 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:16:38.348355: step 11200/300200 (epoch 8/200), loss = 0.361900 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:16:58.697487: step 11600/300200 (epoch 8/200), loss = 0.221666 (0.053 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:17:19.099761: step 12000/300200 (epoch 8/200), loss = 0.182385 (0.044 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4660.0 / guessed_by_relation= 14755.0\n",
      "Calculating Recall: correct_by_relation= 4660.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 31.583%\n",
      "   Recall (micro): 72.147%\n",
      "       F1 (micro): 43.933%\n",
      "epoch 8: train_loss = 0.269867, dev_loss = 1.607498, dev_f1 = 0.4393\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_8.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:17:48.441817: step 12400/300200 (epoch 9/200), loss = 0.247798 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:18:08.841082: step 12800/300200 (epoch 9/200), loss = 0.224259 (0.052 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:18:29.152113: step 13200/300200 (epoch 9/200), loss = 0.285331 (0.052 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4655.0 / guessed_by_relation= 14665.0\n",
      "Calculating Recall: correct_by_relation= 4655.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 31.742%\n",
      "   Recall (micro): 72.070%\n",
      "       F1 (micro): 44.073%\n",
      "epoch 9: train_loss = 0.264870, dev_loss = 1.606354, dev_f1 = 0.4407\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_9.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:18:58.039959: step 13600/300200 (epoch 10/200), loss = 0.336722 (0.050 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:19:18.475954: step 14000/300200 (epoch 10/200), loss = 0.325472 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:19:38.816062: step 14400/300200 (epoch 10/200), loss = 0.189938 (0.051 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:19:59.114059: step 14800/300200 (epoch 10/200), loss = 0.316291 (0.040 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4680.0 / guessed_by_relation= 14683.0\n",
      "Calculating Recall: correct_by_relation= 4680.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 31.874%\n",
      "   Recall (micro): 72.457%\n",
      "       F1 (micro): 44.272%\n",
      "epoch 10: train_loss = 0.259522, dev_loss = 1.654807, dev_f1 = 0.4427\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_10.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:20:28.226502: step 15200/300200 (epoch 11/200), loss = 0.423548 (0.039 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:20:48.530514: step 15600/300200 (epoch 11/200), loss = 0.279762 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:21:08.948831: step 16000/300200 (epoch 11/200), loss = 0.252182 (0.055 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:21:29.875844: step 16400/300200 (epoch 11/200), loss = 0.196652 (0.052 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4485.0 / guessed_by_relation= 14642.0\n",
      "Calculating Recall: correct_by_relation= 4485.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 30.631%\n",
      "   Recall (micro): 69.438%\n",
      "       F1 (micro): 42.510%\n",
      "epoch 11: train_loss = 0.256282, dev_loss = 1.665280, dev_f1 = 0.4251\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_11.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:21:59.199128: step 16800/300200 (epoch 12/200), loss = 0.463022 (0.049 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:22:19.882104: step 17200/300200 (epoch 12/200), loss = 0.246552 (0.043 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:22:40.344297: step 17600/300200 (epoch 12/200), loss = 0.185436 (0.047 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:23:00.699971: step 18000/300200 (epoch 12/200), loss = 0.210729 (0.053 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4696.0 / guessed_by_relation= 14031.0\n",
      "Calculating Recall: correct_by_relation= 4696.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 33.469%\n",
      "   Recall (micro): 72.705%\n",
      "       F1 (micro): 45.837%\n",
      "epoch 12: train_loss = 0.251166, dev_loss = 1.486453, dev_f1 = 0.4584\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_12.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:23:30.059125: step 18400/300200 (epoch 13/200), loss = 0.334117 (0.056 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:23:50.552641: step 18800/300200 (epoch 13/200), loss = 0.183600 (0.043 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:24:10.748294: step 19200/300200 (epoch 13/200), loss = 0.220266 (0.048 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4646.0 / guessed_by_relation= 13237.0\n",
      "Calculating Recall: correct_by_relation= 4646.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 35.099%\n",
      "   Recall (micro): 71.931%\n",
      "       F1 (micro): 47.177%\n",
      "epoch 13: train_loss = 0.246833, dev_loss = 1.356548, dev_f1 = 0.4718\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_13.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:24:39.534265: step 19600/300200 (epoch 14/200), loss = 0.169761 (0.039 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:24:59.791149: step 20000/300200 (epoch 14/200), loss = 0.295363 (0.056 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:25:20.275641: step 20400/300200 (epoch 14/200), loss = 0.275603 (0.053 sec/batch), lr: 0.300000\n",
      "2018-03-22 19:25:40.655856: step 20800/300200 (epoch 14/200), loss = 0.161334 (0.049 sec/batch), lr: 0.300000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4658.0 / guessed_by_relation= 14404.0\n",
      "Calculating Recall: correct_by_relation= 4658.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 32.338%\n",
      "   Recall (micro): 72.116%\n",
      "       F1 (micro): 44.653%\n",
      "epoch 14: train_loss = 0.243654, dev_loss = 1.566766, dev_f1 = 0.4465\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_14.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:26:09.336688: step 21200/300200 (epoch 15/200), loss = 0.350242 (0.049 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:26:29.663021: step 21600/300200 (epoch 15/200), loss = 0.207916 (0.043 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:26:50.168569: step 22000/300200 (epoch 15/200), loss = 0.172897 (0.041 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:27:11.058139: step 22400/300200 (epoch 15/200), loss = 0.291980 (0.053 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4569.0 / guessed_by_relation= 13173.0\n",
      "Calculating Recall: correct_by_relation= 4569.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 34.685%\n",
      "   Recall (micro): 70.739%\n",
      "       F1 (micro): 46.546%\n",
      "epoch 15: train_loss = 0.238378, dev_loss = 1.376515, dev_f1 = 0.4655\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_15.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:27:40.393176: step 22800/300200 (epoch 16/200), loss = 0.236395 (0.044 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:28:00.696403: step 23200/300200 (epoch 16/200), loss = 0.243458 (0.049 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:28:21.233034: step 23600/300200 (epoch 16/200), loss = 0.167625 (0.045 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:28:41.711511: step 24000/300200 (epoch 16/200), loss = 0.261787 (0.053 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4742.0 / guessed_by_relation= 13748.0\n",
      "Calculating Recall: correct_by_relation= 4742.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 34.492%\n",
      "   Recall (micro): 73.417%\n",
      "       F1 (micro): 46.934%\n",
      "epoch 16: train_loss = 0.235810, dev_loss = 1.431727, dev_f1 = 0.4693\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_16.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:29:10.520933: step 24400/300200 (epoch 17/200), loss = 0.147555 (0.050 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:29:30.985371: step 24800/300200 (epoch 17/200), loss = 0.307497 (0.056 sec/batch), lr: 0.285000\n",
      "2018-03-22 19:29:51.407708: step 25200/300200 (epoch 17/200), loss = 0.210459 (0.041 sec/batch), lr: 0.285000\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4673.0 / guessed_by_relation= 13480.0\n",
      "Calculating Recall: correct_by_relation= 4673.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 34.666%\n",
      "   Recall (micro): 72.349%\n",
      "       F1 (micro): 46.873%\n",
      "epoch 17: train_loss = 0.232943, dev_loss = 1.351407, dev_f1 = 0.4687\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_17.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:30:20.380864: step 25600/300200 (epoch 18/200), loss = 0.132579 (0.056 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:30:41.042269: step 26000/300200 (epoch 18/200), loss = 0.168304 (0.044 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:31:01.368687: step 26400/300200 (epoch 18/200), loss = 0.195778 (0.047 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:31:21.891446: step 26800/300200 (epoch 18/200), loss = 0.312815 (0.042 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4725.0 / guessed_by_relation= 12450.0\n",
      "Calculating Recall: correct_by_relation= 4725.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.952%\n",
      "   Recall (micro): 73.154%\n",
      "       F1 (micro): 49.976%\n",
      "epoch 18: train_loss = 0.227902, dev_loss = 1.205630, dev_f1 = 0.4998\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_18.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:31:50.832956: step 27200/300200 (epoch 19/200), loss = 0.300440 (0.044 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:32:11.394958: step 27600/300200 (epoch 19/200), loss = 0.082493 (0.041 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:32:32.127649: step 28000/300200 (epoch 19/200), loss = 0.106800 (0.045 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:32:52.638671: step 28400/300200 (epoch 19/200), loss = 0.244660 (0.047 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4773.0 / guessed_by_relation= 12478.0\n",
      "Calculating Recall: correct_by_relation= 4773.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.251%\n",
      "   Recall (micro): 73.897%\n",
      "       F1 (micro): 50.409%\n",
      "epoch 19: train_loss = 0.225157, dev_loss = 1.176539, dev_f1 = 0.5041\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_19.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:33:21.491081: step 28800/300200 (epoch 20/200), loss = 0.281220 (0.041 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:33:41.785067: step 29200/300200 (epoch 20/200), loss = 0.226745 (0.053 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:34:02.213271: step 29600/300200 (epoch 20/200), loss = 0.150952 (0.041 sec/batch), lr: 0.270750\n",
      "2018-03-22 19:34:22.673653: step 30000/300200 (epoch 20/200), loss = 0.193458 (0.047 sec/batch), lr: 0.270750\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4833.0 / guessed_by_relation= 12981.0\n",
      "Calculating Recall: correct_by_relation= 4833.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.231%\n",
      "   Recall (micro): 74.826%\n",
      "       F1 (micro): 49.722%\n",
      "epoch 20: train_loss = 0.221795, dev_loss = 1.262927, dev_f1 = 0.4972\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_20.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:34:51.575907: step 30400/300200 (epoch 21/200), loss = 0.136310 (0.053 sec/batch), lr: 0.257212\n",
      "2018-03-22 19:35:12.116901: step 30800/300200 (epoch 21/200), loss = 0.250574 (0.057 sec/batch), lr: 0.257212\n",
      "2018-03-22 19:35:32.495825: step 31200/300200 (epoch 21/200), loss = 0.131099 (0.052 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4825.0 / guessed_by_relation= 12862.0\n",
      "Calculating Recall: correct_by_relation= 4825.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.514%\n",
      "   Recall (micro): 74.702%\n",
      "       F1 (micro): 49.946%\n",
      "epoch 21: train_loss = 0.216482, dev_loss = 1.236053, dev_f1 = 0.4995\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_21.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:36:01.381229: step 31600/300200 (epoch 22/200), loss = 0.212893 (0.043 sec/batch), lr: 0.257212\n",
      "2018-03-22 19:36:21.952954: step 32000/300200 (epoch 22/200), loss = 0.120599 (0.050 sec/batch), lr: 0.257212\n",
      "2018-03-22 19:36:42.476412: step 32400/300200 (epoch 22/200), loss = 0.151244 (0.044 sec/batch), lr: 0.257212\n",
      "2018-03-22 19:37:03.332197: step 32800/300200 (epoch 22/200), loss = 0.150774 (0.044 sec/batch), lr: 0.257212\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4868.0 / guessed_by_relation= 13443.0\n",
      "Calculating Recall: correct_by_relation= 4868.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 36.212%\n",
      "   Recall (micro): 75.368%\n",
      "       F1 (micro): 48.920%\n",
      "epoch 22: train_loss = 0.215461, dev_loss = 1.302747, dev_f1 = 0.4892\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_22.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:37:32.219030: step 33200/300200 (epoch 23/200), loss = 0.165040 (0.047 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:37:52.727587: step 33600/300200 (epoch 23/200), loss = 0.113078 (0.056 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:38:13.773423: step 34000/300200 (epoch 23/200), loss = 0.332725 (0.053 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:38:36.648931: step 34400/300200 (epoch 23/200), loss = 0.091557 (0.055 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4830.0 / guessed_by_relation= 12721.0\n",
      "Calculating Recall: correct_by_relation= 4830.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.969%\n",
      "   Recall (micro): 74.779%\n",
      "       F1 (micro): 50.365%\n",
      "epoch 23: train_loss = 0.210161, dev_loss = 1.227456, dev_f1 = 0.5036\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_23.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:39:06.215761: step 34800/300200 (epoch 24/200), loss = 0.476725 (0.054 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:39:26.750386: step 35200/300200 (epoch 24/200), loss = 0.175372 (0.051 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:39:47.494363: step 35600/300200 (epoch 24/200), loss = 0.179831 (0.050 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:40:08.561928: step 36000/300200 (epoch 24/200), loss = 0.305997 (0.051 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4856.0 / guessed_by_relation= 12747.0\n",
      "Calculating Recall: correct_by_relation= 4856.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.095%\n",
      "   Recall (micro): 75.182%\n",
      "       F1 (micro): 50.568%\n",
      "epoch 24: train_loss = 0.207969, dev_loss = 1.272715, dev_f1 = 0.5057\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_24.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:40:37.375577: step 36400/300200 (epoch 25/200), loss = 0.161885 (0.050 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:40:57.772019: step 36800/300200 (epoch 25/200), loss = 0.257962 (0.056 sec/batch), lr: 0.244352\n",
      "2018-03-22 19:41:18.206378: step 37200/300200 (epoch 25/200), loss = 0.132229 (0.053 sec/batch), lr: 0.244352\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4776.0 / guessed_by_relation= 12600.0\n",
      "Calculating Recall: correct_by_relation= 4776.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.905%\n",
      "   Recall (micro): 73.943%\n",
      "       F1 (micro): 50.118%\n",
      "epoch 25: train_loss = 0.203765, dev_loss = 1.189830, dev_f1 = 0.5012\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_25.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:41:46.856543: step 37600/300200 (epoch 26/200), loss = 0.173775 (0.045 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:42:07.470695: step 38000/300200 (epoch 26/200), loss = 0.161686 (0.042 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:42:27.926553: step 38400/300200 (epoch 26/200), loss = 0.137596 (0.051 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:42:48.435626: step 38800/300200 (epoch 26/200), loss = 0.213109 (0.058 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4879.0 / guessed_by_relation= 12618.0\n",
      "Calculating Recall: correct_by_relation= 4879.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.667%\n",
      "   Recall (micro): 75.538%\n",
      "       F1 (micro): 51.151%\n",
      "epoch 26: train_loss = 0.198955, dev_loss = 1.254710, dev_f1 = 0.5115\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_26.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:43:17.448278: step 39200/300200 (epoch 27/200), loss = 0.087094 (0.049 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:43:38.103382: step 39600/300200 (epoch 27/200), loss = 0.342649 (0.052 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:43:58.766845: step 40000/300200 (epoch 27/200), loss = 0.216309 (0.042 sec/batch), lr: 0.232134\n",
      "2018-03-22 19:44:19.338569: step 40400/300200 (epoch 27/200), loss = 0.224942 (0.055 sec/batch), lr: 0.232134\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4816.0 / guessed_by_relation= 12806.0\n",
      "Calculating Recall: correct_by_relation= 4816.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.607%\n",
      "   Recall (micro): 74.563%\n",
      "       F1 (micro): 49.997%\n",
      "epoch 27: train_loss = 0.196743, dev_loss = 1.258119, dev_f1 = 0.5000\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_27.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:44:48.317752: step 40800/300200 (epoch 28/200), loss = 0.256673 (0.043 sec/batch), lr: 0.220528\n",
      "2018-03-22 19:45:08.555633: step 41200/300200 (epoch 28/200), loss = 0.275709 (0.052 sec/batch), lr: 0.220528\n",
      "2018-03-22 19:45:29.172477: step 41600/300200 (epoch 28/200), loss = 0.101802 (0.050 sec/batch), lr: 0.220528\n",
      "2018-03-22 19:45:49.666585: step 42000/300200 (epoch 28/200), loss = 0.233530 (0.055 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4814.0 / guessed_by_relation= 12185.0\n",
      "Calculating Recall: correct_by_relation= 4814.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 39.508%\n",
      "   Recall (micro): 74.532%\n",
      "       F1 (micro): 51.641%\n",
      "epoch 28: train_loss = 0.191055, dev_loss = 1.182968, dev_f1 = 0.5164\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_28.pt\n",
      "new best model saved.\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:46:18.960992: step 42400/300200 (epoch 29/200), loss = 0.257468 (0.042 sec/batch), lr: 0.220528\n",
      "2018-03-22 19:46:39.336362: step 42800/300200 (epoch 29/200), loss = 0.269290 (0.051 sec/batch), lr: 0.220528\n",
      "2018-03-22 19:46:59.951364: step 43200/300200 (epoch 29/200), loss = 0.108386 (0.053 sec/batch), lr: 0.220528\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4827.0 / guessed_by_relation= 12451.0\n",
      "Calculating Recall: correct_by_relation= 4827.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.768%\n",
      "   Recall (micro): 74.733%\n",
      "       F1 (micro): 51.052%\n",
      "epoch 29: train_loss = 0.189452, dev_loss = 1.239921, dev_f1 = 0.5105\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_29.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:47:29.551105: step 43600/300200 (epoch 30/200), loss = 0.089025 (0.040 sec/batch), lr: 0.209501\n",
      "2018-03-22 19:47:51.109454: step 44000/300200 (epoch 30/200), loss = 0.289714 (0.044 sec/batch), lr: 0.209501\n",
      "2018-03-22 19:48:12.314299: step 44400/300200 (epoch 30/200), loss = 0.195579 (0.046 sec/batch), lr: 0.209501\n",
      "2018-03-22 19:48:34.074743: step 44800/300200 (epoch 30/200), loss = 0.270438 (0.058 sec/batch), lr: 0.209501\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4880.0 / guessed_by_relation= 12764.0\n",
      "Calculating Recall: correct_by_relation= 4880.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.233%\n",
      "   Recall (micro): 75.553%\n",
      "       F1 (micro): 50.773%\n",
      "epoch 30: train_loss = 0.183440, dev_loss = 1.298231, dev_f1 = 0.5077\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_30.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:49:04.946852: step 45200/300200 (epoch 31/200), loss = 0.137987 (0.059 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:49:25.778267: step 45600/300200 (epoch 31/200), loss = 0.296828 (0.053 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:49:46.801485: step 46000/300200 (epoch 31/200), loss = 0.192304 (0.052 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:50:07.828420: step 46400/300200 (epoch 31/200), loss = 0.076192 (0.051 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4857.0 / guessed_by_relation= 12510.0\n",
      "Calculating Recall: correct_by_relation= 4857.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.825%\n",
      "   Recall (micro): 75.197%\n",
      "       F1 (micro): 51.210%\n",
      "epoch 31: train_loss = 0.179363, dev_loss = 1.275577, dev_f1 = 0.5121\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_31.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:50:37.368001: step 46800/300200 (epoch 32/200), loss = 0.195149 (0.062 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:50:58.471139: step 47200/300200 (epoch 32/200), loss = 0.230153 (0.057 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:51:19.651492: step 47600/300200 (epoch 32/200), loss = 0.156410 (0.056 sec/batch), lr: 0.199026\n",
      "2018-03-22 19:51:40.732097: step 48000/300200 (epoch 32/200), loss = 0.095245 (0.054 sec/batch), lr: 0.199026\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4918.0 / guessed_by_relation= 13010.0\n",
      "Calculating Recall: correct_by_relation= 4918.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.802%\n",
      "   Recall (micro): 76.142%\n",
      "       F1 (micro): 50.521%\n",
      "epoch 32: train_loss = 0.175148, dev_loss = 1.370931, dev_f1 = 0.5052\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_32.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:52:10.075155: step 48400/300200 (epoch 33/200), loss = 0.196849 (0.039 sec/batch), lr: 0.189075\n",
      "2018-03-22 19:52:30.727093: step 48800/300200 (epoch 33/200), loss = 0.347424 (0.055 sec/batch), lr: 0.189075\n",
      "2018-03-22 19:52:51.568535: step 49200/300200 (epoch 33/200), loss = 0.102753 (0.058 sec/batch), lr: 0.189075\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4947.0 / guessed_by_relation= 13127.0\n",
      "Calculating Recall: correct_by_relation= 4947.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.686%\n",
      "   Recall (micro): 76.591%\n",
      "       F1 (micro): 50.516%\n",
      "epoch 33: train_loss = 0.170632, dev_loss = 1.403259, dev_f1 = 0.5052\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_33.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:53:20.548027: step 49600/300200 (epoch 34/200), loss = 0.257616 (0.044 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:53:41.259173: step 50000/300200 (epoch 34/200), loss = 0.179537 (0.050 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:54:02.448540: step 50400/300200 (epoch 34/200), loss = 0.151927 (0.041 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:54:23.144380: step 50800/300200 (epoch 34/200), loss = 0.113670 (0.054 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4884.0 / guessed_by_relation= 12516.0\n",
      "Calculating Recall: correct_by_relation= 4884.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 39.022%\n",
      "   Recall (micro): 75.615%\n",
      "       F1 (micro): 51.478%\n",
      "epoch 34: train_loss = 0.165781, dev_loss = 1.289392, dev_f1 = 0.5148\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_34.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:54:52.055476: step 51200/300200 (epoch 35/200), loss = 0.167875 (0.056 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:55:12.555241: step 51600/300200 (epoch 35/200), loss = 0.058017 (0.055 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:55:33.464864: step 52000/300200 (epoch 35/200), loss = 0.160585 (0.045 sec/batch), lr: 0.179621\n",
      "2018-03-22 19:55:54.220871: step 52400/300200 (epoch 35/200), loss = 0.126835 (0.051 sec/batch), lr: 0.179621\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4889.0 / guessed_by_relation= 13073.0\n",
      "Calculating Recall: correct_by_relation= 4889.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.398%\n",
      "   Recall (micro): 75.693%\n",
      "       F1 (micro): 50.061%\n",
      "epoch 35: train_loss = 0.162789, dev_loss = 1.410691, dev_f1 = 0.5006\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_35.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:56:23.111372: step 52800/300200 (epoch 36/200), loss = 0.107878 (0.040 sec/batch), lr: 0.170640\n",
      "2018-03-22 19:56:43.584867: step 53200/300200 (epoch 36/200), loss = 0.092844 (0.052 sec/batch), lr: 0.170640\n",
      "2018-03-22 19:57:04.581402: step 53600/300200 (epoch 36/200), loss = 0.204924 (0.040 sec/batch), lr: 0.170640\n",
      "2018-03-22 19:57:25.445572: step 54000/300200 (epoch 36/200), loss = 0.217713 (0.050 sec/batch), lr: 0.170640\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4818.0 / guessed_by_relation= 12288.0\n",
      "Calculating Recall: correct_by_relation= 4818.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 39.209%\n",
      "   Recall (micro): 74.594%\n",
      "       F1 (micro): 51.400%\n",
      "epoch 36: train_loss = 0.157060, dev_loss = 1.321094, dev_f1 = 0.5140\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_36.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:57:54.454394: step 54400/300200 (epoch 37/200), loss = 0.132892 (0.052 sec/batch), lr: 0.170640\n",
      "2018-03-22 19:58:14.817564: step 54800/300200 (epoch 37/200), loss = 0.076276 (0.045 sec/batch), lr: 0.170640\n",
      "2018-03-22 19:58:35.784648: step 55200/300200 (epoch 37/200), loss = 0.171496 (0.043 sec/batch), lr: 0.170640\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4910.0 / guessed_by_relation= 13237.0\n",
      "Calculating Recall: correct_by_relation= 4910.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 37.093%\n",
      "   Recall (micro): 76.018%\n",
      "       F1 (micro): 49.858%\n",
      "epoch 37: train_loss = 0.153407, dev_loss = 1.499607, dev_f1 = 0.4986\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_37.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 19:59:04.402648: step 55600/300200 (epoch 38/200), loss = 0.101313 (0.045 sec/batch), lr: 0.162108\n",
      "2018-03-22 19:59:24.848037: step 56000/300200 (epoch 38/200), loss = 0.115672 (0.048 sec/batch), lr: 0.162108\n",
      "2018-03-22 19:59:45.314108: step 56400/300200 (epoch 38/200), loss = 0.190833 (0.054 sec/batch), lr: 0.162108\n",
      "2018-03-22 20:00:05.952009: step 56800/300200 (epoch 38/200), loss = 0.050432 (0.056 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4901.0 / guessed_by_relation= 12891.0\n",
      "Calculating Recall: correct_by_relation= 4901.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.019%\n",
      "   Recall (micro): 75.879%\n",
      "       F1 (micro): 50.656%\n",
      "epoch 38: train_loss = 0.150544, dev_loss = 1.420351, dev_f1 = 0.5066\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_38.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:00:35.390935: step 57200/300200 (epoch 39/200), loss = 0.242988 (0.051 sec/batch), lr: 0.162108\n",
      "2018-03-22 20:00:56.158954: step 57600/300200 (epoch 39/200), loss = 0.262946 (0.053 sec/batch), lr: 0.162108\n",
      "2018-03-22 20:01:17.475660: step 58000/300200 (epoch 39/200), loss = 0.286997 (0.049 sec/batch), lr: 0.162108\n",
      "2018-03-22 20:01:38.514627: step 58400/300200 (epoch 39/200), loss = 0.090266 (0.048 sec/batch), lr: 0.162108\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4907.0 / guessed_by_relation= 13326.0\n",
      "Calculating Recall: correct_by_relation= 4907.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 36.823%\n",
      "   Recall (micro): 75.972%\n",
      "       F1 (micro): 49.603%\n",
      "epoch 39: train_loss = 0.145660, dev_loss = 1.570840, dev_f1 = 0.4960\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_39.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:02:08.213373: step 58800/300200 (epoch 40/200), loss = 0.197023 (0.048 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:02:28.614644: step 59200/300200 (epoch 40/200), loss = 0.088002 (0.042 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:02:49.567179: step 59600/300200 (epoch 40/200), loss = 0.172442 (0.043 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:03:10.793645: step 60000/300200 (epoch 40/200), loss = 0.240792 (0.053 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4942.0 / guessed_by_relation= 12812.0\n",
      "Calculating Recall: correct_by_relation= 4942.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.573%\n",
      "   Recall (micro): 76.513%\n",
      "       F1 (micro): 51.290%\n",
      "epoch 40: train_loss = 0.141797, dev_loss = 1.486255, dev_f1 = 0.5129\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_40.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:03:40.879736: step 60400/300200 (epoch 41/200), loss = 0.086881 (0.052 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:04:01.921053: step 60800/300200 (epoch 41/200), loss = 0.117518 (0.051 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:04:23.013162: step 61200/300200 (epoch 41/200), loss = 0.168871 (0.053 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4856.0 / guessed_by_relation= 12419.0\n",
      "Calculating Recall: correct_by_relation= 4856.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 39.101%\n",
      "   Recall (micro): 75.182%\n",
      "       F1 (micro): 51.446%\n",
      "epoch 41: train_loss = 0.139060, dev_loss = 1.393859, dev_f1 = 0.5145\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_41.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:04:52.770321: step 61600/300200 (epoch 42/200), loss = 0.106650 (0.050 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:05:13.595442: step 62000/300200 (epoch 42/200), loss = 0.348297 (0.045 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:05:34.767764: step 62400/300200 (epoch 42/200), loss = 0.088083 (0.048 sec/batch), lr: 0.154003\n",
      "2018-03-22 20:05:55.525604: step 62800/300200 (epoch 42/200), loss = 0.077262 (0.050 sec/batch), lr: 0.154003\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4842.0 / guessed_by_relation= 12539.0\n",
      "Calculating Recall: correct_by_relation= 4842.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.616%\n",
      "   Recall (micro): 74.965%\n",
      "       F1 (micro): 50.974%\n",
      "epoch 42: train_loss = 0.135273, dev_loss = 1.463196, dev_f1 = 0.5097\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_42.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:06:25.000904: step 63200/300200 (epoch 43/200), loss = 0.192587 (0.050 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:06:45.700103: step 63600/300200 (epoch 43/200), loss = 0.080337 (0.054 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:07:06.729293: step 64000/300200 (epoch 43/200), loss = 0.320852 (0.059 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:07:27.819833: step 64400/300200 (epoch 43/200), loss = 0.038235 (0.060 sec/batch), lr: 0.146302\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4848.0 / guessed_by_relation= 12530.0\n",
      "Calculating Recall: correct_by_relation= 4848.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.691%\n",
      "   Recall (micro): 75.058%\n",
      "       F1 (micro): 51.061%\n",
      "epoch 43: train_loss = 0.131829, dev_loss = 1.487363, dev_f1 = 0.5106\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_43.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:07:57.362057: step 64800/300200 (epoch 44/200), loss = 0.092398 (0.050 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:08:17.720213: step 65200/300200 (epoch 44/200), loss = 0.105589 (0.051 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:08:38.659166: step 65600/300200 (epoch 44/200), loss = 0.169308 (0.050 sec/batch), lr: 0.146302\n",
      "2018-03-22 20:08:59.322462: step 66000/300200 (epoch 44/200), loss = 0.158261 (0.043 sec/batch), lr: 0.146302\n",
      "Evaluating on dev set...\n",
      "Calculating Precision: correct_by_relation= 4864.0 / guessed_by_relation= 12740.0\n",
      "Calculating Recall: correct_by_relation= 4864.0 / gold_by_relation= 6459.0\n",
      "only_non_relation: 6459.0\n",
      "Precision (micro): 38.179%\n",
      "   Recall (micro): 75.306%\n",
      "       F1 (micro): 50.669%\n",
      "epoch 44: train_loss = 0.127943, dev_loss = 1.573862, dev_f1 = 0.5067\n",
      "model saved to ./saved_models/37_self_attention_dropout/checkpoint_epoch_44.pt\n",
      "\n",
      "Current params:  heads-1 enc_layers-3  drop-0.1 scaled_drop-0.1 lr-0.3  lr_decay-0.95 grad_norm-1.0\n",
      " weight_no_rel-2.0 weight_rest-1.0 attn_dim-200\n",
      "2018-03-22 20:09:28.690806: step 66400/300200 (epoch 45/200), loss = 0.089726 (0.054 sec/batch), lr: 0.138987\n",
      "2018-03-22 20:09:49.583921: step 66800/300200 (epoch 45/200), loss = 0.130682 (0.051 sec/batch), lr: 0.138987\n",
      "2018-03-22 20:10:10.528331: step 67200/300200 (epoch 45/200), loss = 0.187955 (0.041 sec/batch), lr: 0.138987\n",
      "Evaluating on dev set...\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(1, opt['num_epoch']+1):\n",
    "    \n",
    "    print(\n",
    "        \"Current params: \"+ \" heads-\"+ str(opt[\"n_head\"]) + \" enc_layers-\" + str(opt[\"num_layers_encoder\"]),\n",
    "        \" drop-\"+ str(opt[\"dropout\"]) + \" scaled_drop-\" + str(opt[\"scaled_dropout\"]) + \" lr-\"+ str(opt[\"lr\"]),\n",
    "        \" lr_decay-\"+ str(opt[\"lr_decay\"]) + \" grad_norm-\"+ str(opt[\"max_grad_norm\"])\n",
    "    )\n",
    "    print(\n",
    "        \" weight_no_rel-\"+ str(opt[\"weight_no_rel\"]) +\n",
    "        \" weight_rest-\"+ str(opt[\"weight_rest\"]) + \" attn-\"+ str(opt[\"attn\"]) +\" attn_dim-\"+ str(opt[\"attn_dim\"]),\n",
    "        \" obj_sub_pos-\"+ str(opt[\"obj_sub_pos\"])\n",
    "    )\n",
    "    \n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_batch):\n",
    "        start_time = time.time()\n",
    "        global_step += 1\n",
    "        loss = model.update(batch)\n",
    "        train_loss += loss\n",
    "        if global_step % opt['log_step'] == 0:\n",
    "            duration = time.time() - start_time\n",
    "            print(\n",
    "                format_str.format(datetime.now(), global_step, max_steps, epoch,\n",
    "                opt['num_epoch'], loss, duration, current_lr)\n",
    "            )\n",
    "        # do garbage collection, as per https://discuss.pytorch.org/t/best-practices-for-maximum-gpu-utilization/13863/6\n",
    "        del loss\n",
    "\n",
    "    # eval on dev\n",
    "    print(\"Evaluating on dev set...\")\n",
    "    predictions = []\n",
    "    dev_loss = 0\n",
    "    for i, batch in enumerate(dev_batch):\n",
    "        preds, _, loss = model.predict(batch)\n",
    "        predictions += preds\n",
    "        dev_loss += loss\n",
    "        \n",
    "    predictions = [id2label[p] for p in predictions]\n",
    "    dev_p, dev_r, dev_f1 = scorer.score(dev_batch.gold(), predictions)\n",
    "    \n",
    "    train_loss = train_loss / train_batch.num_examples * opt['batch_size'] # avg loss per batch\n",
    "    dev_loss = dev_loss / dev_batch.num_examples * opt['batch_size']\n",
    "    print(\n",
    "        \"epoch {}: train_loss = {:.6f}, dev_loss = {:.6f}, dev_f1 = {:.4f}\".format(epoch,\\\n",
    "            train_loss, dev_loss, dev_f1)\n",
    "        )\n",
    "    \n",
    "    file_logger.log(\"{}\\t{:.6f}\\t{:.6f}\\t{:.4f}\".format(epoch, train_loss, dev_loss, dev_f1))\n",
    "\n",
    "    # save the model during every epoch, if fscore is the best, move it to best_model.pkl\n",
    "    model_file = model_save_dir + '/checkpoint_epoch_{}.pt'.format(epoch)\n",
    "    model.save(model_file, epoch)\n",
    "    if epoch == 1 or dev_f1 > max(dev_f1_history):\n",
    "        copyfile(model_file, model_save_dir + '/best_model.pt')\n",
    "        print(\"new best model saved.\")\n",
    "    # delete single checkpoints based on the save_epoch int\n",
    "    if epoch % opt['save_epoch'] != 0:\n",
    "        os.remove(model_file)\n",
    "\n",
    "    # lr schedule\n",
    "    if len(dev_f1_history) > 10 and dev_f1 <= dev_f1_history[-1] and opt['optim'] in ['sgd', 'adagrad', 'adam']:\n",
    "        # don't go lower than 0.01 lr\n",
    "        if current_lr >= 0.01:\n",
    "            current_lr *= opt['lr_decay']\n",
    "            model.update_lr(current_lr)\n",
    "\n",
    "    dev_f1_history += [dev_f1]\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Training ended with {} epochs.\".format(epoch))\n",
    "\n",
    "# !!!!!!!! change the model output folder !!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without residual and with ulinear\n",
    "# other weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
